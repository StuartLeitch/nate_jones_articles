---
title: "Managers Are Breaking AI Budgets (And It’s Going to Cost You Your Best People)"
author: "Nate Jones"
published: 2025-10-11
url: https://natesnewsletter.substack.com/p/dear-manager-your-ai-budget-is-costing
audience: everyone
scraped_at: 2026-01-05 19:15:16
---

“Nate, I can’t use these fancy models. The company won’t pay for it.”

I hear that a lot.

And I’m really tired of it.

So I made this post for you and your manager and the IT department.

Here’s exactly why the best companies on the planet are blowing out traditional software budgets to equip their teams with AI costing hundreds a month.

Here’s the ROI case for doing that at work, and here’s my personal plea to management to invest in the best models out there.

And most of all, here’s how to do it right:

- Case studies from companies leaning in on AI investments
- A guide for building your own business case
- Sector-by-sector productivity gains from AI tooling
- Detailed implementation guides for getting better tooling operational

It’s everything you need to make the case for better AI tooling, whether you’re a professional or a manager or a leader.

The bottom line is simple: it makes zero sense to wait on tackling AI tooling upgrades. Yes, waiting will cost you technically. But the real reason not to wait is people: your best people will absolutely walk away if they can’t get the tooling that helps them accelerate.

I know, because they talk to me about it.

> *PS. This would normally be an executive circle post, but I’m circulating it to all paid members since I think AI professionals at all levels should be armed with this info to make the case for better tooling ([you can learn more about my plans here](https://youtu.be/KC3GkEnHR-8)). Good luck getting better AI models!*


# Managers Are Breaking AI Budgets (And It’s Going to Cost You Your Best People)

I need to be honest about something uncomfortable: managers are systematically losing their best people because they can’t adapt budget processes fast enough for AI tools. I say this as someone who has managed teams, with full sympathy for the challenges we’re all facing. But the data is overwhelming, and the consequences are already here.

Enterprise AI spending jumped 75% year-over-year in 2024, reaching $400,000 annually per organization for AI-native applications alone. Yet I’ve talked to hundreds of individual contributors over the past year, and the complaint is nearly universal. They want access to AI tools that meaningfully improve their work—tools that can double productivity in content creation and data analysis. And they can’t get them, blocked not by technical limitations, but by budget processes designed for a different era.

The frustration plays out identically every time. Managers tell me they can’t go to IT because security will raise objections. They can’t go to their bosses because department heads balk at $200-500 per employee per month for software—numbers unprecedented in traditional software budgeting. So managers kick the can down the road, thinking they’ll figure it out next fiscal year.

Here’s the problem: it’s not getting cheaper. By 2026-2027, expect comprehensive AI tools to cost $1,000-2,000 per employee monthly as models become more sophisticated and computational requirements increase. And here’s why that matters more than you think.


## Why Traditional Software Budgeting Is Dead

Traditional software budgeting assumes marginal improvements at marginal costs. A typical enterprise tool costs $10-50 per user monthly. But AI tools operate in a completely different economic paradigm—comprehensive AI suites now range from $200-500 per employee monthly, with advanced enterprise implementations reaching $2,000-2,500 monthly.

This isn’t software inflation. It’s a category shift. Microsoft Copilot at $30 per month pays for itself if it saves just 2-3 hours monthly for a $100,000 employee. That’s the calculation that changes everything: **AI software pricing now competes against human compensation, not other software tools**. When a $400 monthly AI subscription can replace 10-20 hours of manual work, the value proposition remains compelling even as prices rise.

Software costs have already grown from 13% of total IT budgets in 2019 to 21% in 2024, with AI features driving much of this increase. Nearly half of organizations experienced licensing cost increases exceeding the industry average of 10%, as vendors systematically add AI capabilities to justify price increases.

The enterprise AI market exploded from $24 billion in 2024 to a projected $150 billion by 2030—a 35% compound annual growth rate. This isn’t gradual software evolution. It’s comparable to the transition from mainframes to personal computers, and your budget process wasn’t designed for it.


## The Talent Retention Crisis You Can’t Afford

Here’s the math that should terrify every manager operating under traditional budget constraints: **the cost of employee turnover ranges from 50-200% of annual salary**. For a $100,000 employee, turnover costs between $50,000-200,000. Comprehensive AI tools cost $4,800 annually. A single retained employee pays for 10-40 colleagues’ AI subscriptions.

The competition for AI-skilled talent has reached unprecedented intensity. Demand for AI skills grew 292% in 2024. Elite AI companies like Anthropic maintain 80% retention rates, compared to 67% at OpenAI and 64% at Meta—largely because they provide cutting-edge tools and autonomy.

The brain drain from traditional companies is accelerating. Meta ranked as the second-most targeted company for AI talent poaching in 2024, with 4.3% of new AI lab hires coming from the organization. At least nine research scientists moved directly from Meta to the AI startup Mistral since April 2023. Microsoft compiled lists of Meta’s most sought-after engineers to match competitive offers.

Notice the pattern: **top AI talent gravitates toward organizations that provide modern tooling and technical autonomy**. Companies restricting AI tool access because of budget constraints are systematically losing their best performers to competitors who understand the new economic reality.


## The Shadow AI Problem Nobody’s Tracking

Here’s what’s already happening in your organization, and the numbers are more alarming than you think. 50% of employees now use Shadow AI—unauthorized AI tools they’ve purchased themselves or access through free accounts. But here’s the kicker: **46% of employees say they would refuse to give up their personal AI tools even if banned by their organization**.

The governance gap is massive. Only 14% of organizations have clear AI security policies, yet 84% of AI tools have been breached. Meanwhile, 57% of employees actively hide their AI use at work, and 25% of employees input sensitive data into AI tools without oversight. Most telling: **58% of IT managers themselves use unapproved AI tools**—even the people responsible for governance are going around the system.

The micro-transaction nature of AI subscriptions bypasses traditional procurement oversight. Many AI tools cost $20-30 monthly, falling below the $5,000 threshold that triggers formal IT review. 65% of enterprise SaaS apps are unsanctioned, with 65% of remote workers using non-approved tools. Directors can procure AI tools for entire teams without appearing on procurement radar, creating accountability gaps that traditional budget processes weren’t designed to handle.

Security concerns, while legitimate, are often used to justify inaction rather than drive appropriate governance. 67% of organizations plan to increase AI security spending, but many use security reviews as delaying tactics. The result: employees either work without necessary tools or use unauthorized alternatives that create greater security risks than managed deployment would.


## What Success Actually Looks Like

The organizations getting this right aren’t waiting for perfect processes. They’re moving fast, measuring outcomes, and demonstrating ROI with real numbers:

**Contraktor** (legal tech) reduced contract analysis time by 75%, achieving 4-5X faster processing and cutting legal review costs by three-quarters.

**Sojern** (travel marketing) compressed audience generation from 2 weeks to 2 days—an 86% reduction—while improving cost-per-acquisition by 20-50%.

**Galaxies** (marketing analytics) cut campaign testing from months to 48 hours, achieving 85% savings in direct research costs.

**Hiscox** (insurance) reduced underwriting time from 3 days to minutes—a 95% reduction that transformed their competitive position.

These aren’t marginal improvements. They’re fundamental transformations in how work gets done, and they’re happening right now at organizations that figured out how to say “yes” instead of “we’ll review this next quarter.”


## The Industry Reality: One Size Doesn’t Fit All

The productivity gains and ROI timelines vary dramatically by industry, and understanding this helps set realistic expectations:

**Technology:** 45% productivity gains, 2-month ROI timeline ($500/employee/month)

**Professional Services:** 40% gains, 3-month ROI ($250/employee/month)

**Manufacturing:** 35% gains, 4-month ROI ($350/employee/month)

**Financial Services:** 30% gains, 6-month ROI ($400/employee/month)

**Healthcare:** 25% gains, 8-month ROI ($300/employee/month)

Healthcare’s longer timeline reflects HIPAA compliance requirements and patient privacy protocols. Financial services faces regulatory compliance challenges. Manufacturing must integrate with legacy systems. But every industry shows positive ROI—the question is just how quickly you’ll realize it, not whether you will.

If your organization operates in a highly regulated industry, don’t let that become an excuse for inaction. It means you need stronger governance, not indefinite delay.

Wondering where I got the numbers? These benchmarks are supported by (1) McKinsey, PwC, and Stanford HAI data showing sectoral differences in AI adoption, (2) industry surveys and pilot outcomes published in sources like the Stanford 2025 AI Index Report, Wharton’s projected impact analysis, and Mordor Intelligence’s enterprise AI market size and case study reviews, and (3) pricing models reported across major sectors.


## What Your Employees Actually Need

Look at this from the perspective of an individual contributor trying to build a career right now. It has never been more challenging to figure out a path forward. They need to build AI skills. They need to deliver in their current role using rapidly evolving tools. They need to set up their resume for a world where job descriptions change every quarter.

They need these tools to show their current employer they can deliver value. They need these tools to demonstrate capability when they look for promotion or their next role. They need these tools to stay competitive in a market where the best companies already require AI fluency. And by and large, we’re not setting them up for success.

The really good ones vote with their feet. People who are exceptional at using AI move to roles and companies that understand this shift. They won’t stay at organizations that treat AI like a nice-to-have. This isn’t about compensation alone—people left Meta after $100 million packages because they didn’t like the culture. It’s about whether your business generates the kind of environment where your best people feel they can actually thrive.


## The Financial Reality Leadership Needs to Understand

Let’s make this concrete with numbers you can take to your next budget meeting. For a typical $100,000 employee using $400/month AI tools:

- **AI tools cost:** $4,800 annually
- **Productivity gain:** 25% improvement (10 hours saved monthly)
- **Value of time saved:** $7,500 annually
- **Net benefit:** $2,700 positive annual return
- **ROI:** 56% return on investment
- **Payback period:** 7.7 months

That’s using conservative assumptions. Technology teams often see 45% productivity gains with 2-month payback. Professional services teams frequently hit 40% gains with 3-month payback. Even healthcare, with its complex compliance requirements, achieves 25% gains within 8 months.

Organizations that successfully adapt their procurement and budgeting processes gain enormous competitive advantages. Companies implementing AI tools effectively see 30-40% productivity gains in knowledge work, while those stuck in traditional processes watch their best talent leave for more forward-thinking competitors.

The financial impact of getting this wrong is severe. Organizations experience 30% higher turnover costs when they fail to provide modern tooling, while replacement costs for AI-skilled employees can reach $200,000 or more per departure. The math is unforgiving: a $50,000 annual AI tool budget per employee costs less than losing that employee once every four years.

Early adopters are already seeing results. Salesforce closed 5,000 Agentforce deals since October 2024, including 3,000 paid customers, demonstrating real market demand for comprehensive AI platforms. Organizations using outcome-based pricing models report better alignment between costs and value delivered.


## What Managers Need to Do Now

Senior managers and directors need to carry the flag on this. You need to explain clearly to your bosses and to IT why traditional software budgeting doesn’t work anymore. Here’s how:

**1. Reframe the comparison.** Stop comparing AI tools to software subscriptions. Compare them to human compensation and turnover costs. A $6,000 annual AI tool investment that prevents one $150,000 turnover is a 25x ROI.

**2. Use specific numbers.** Don’t say “AI will make us more productive.” Say “Microsoft Copilot saves 2-3 hours per week for knowledge workers. For our team of 20 at $100,000 average compensation, that’s $96,000-144,000 in productivity value annually for a $7,200 total investment.”

**3. Address security proactively.** Create a fast-track approval process for tools below defined risk thresholds while maintaining appropriate governance. Don’t let security reviews become indefinite delays. The Shadow AI statistics show that delayed approval doesn’t prevent usage—it just prevents oversight.

**4. Prepare for the price trajectory.** Build 2026-2027 budgets assuming $1,000-2,000 per employee monthly for comprehensive AI tools. If you wait until prices rise to adjust budgets, you’ll be two years behind.

**5. Measure retention by tool access.** Track whether employees with modern AI tool access have different retention rates than those without. The data will justify the investment.

Your teams need to tell you what tools they actually need. You need to explain to leadership what those tools enable—not in vague terms about innovation, but with specific use cases and concrete productivity metrics.

---


## The Implementation Sequence That Works

Successful organizations follow a clear sequence, though the timeline varies based on organization size and complexity. Here’s what actually works, phase by phase, with the context you need to avoid the traps that kill 88% of AI pilots before they reach production.


### Foundation Phase: Know What You’re Actually Dealing With

Most organizations start by debating which AI tool to buy. That’s backwards. You need to understand what’s already happening before you make any decisions.

**Conduct a Shadow AI audit** - Your employees are already using AI tools. You just don’t know which ones or how. Start by discovering the reality on the ground:

- Survey your organization: “What AI tools are you currently using for work, whether approved or not?” Make it anonymous. Make it clear this isn’t a witch hunt - you’re trying to help, not punish.
- Check your expense reports for $20-50 monthly subscriptions to ChatGPT Plus, Claude Pro, Midjourney, Jasper, Copy.ai, or similar tools.
- Talk to your IT team about which AI domains are being accessed from corporate networks.
- Interview your top performers individually. They’re the ones most likely to have found tools that work.

What you’re looking for: Which tools are people actually finding valuable enough to pay for themselves? What use cases are they solving? Where are the biggest productivity gains happening?

This audit typically reveals that 50-65% of your employees are already using unauthorized AI tools, and the most productive people are the ones using them most. That’s your roadmap - you’re not starting from zero, you’re catching up to what your best people already figured out.

**Secure executive sponsorship and budget allocation** - You cannot do this bottom-up. AI tools require budget categories that don’t exist in traditional software planning, and middle managers don’t have the authority to create them.

You need a specific executive sponsor who will:

- Commit to budget allocation in the $200-500 per employee per month range for initial deployment
- Override the “we’ll evaluate this next quarter” instinct when security raises concerns
- Defend the investment when other department heads ask why your software budget is 10x theirs
- Make clear this is strategic priority, not a skunkworks experiment

The pitch to executives: “We’re already spending this money through Shadow AI. We’re just spending it without governance, without security oversight, and without capturing the organizational learning. And we’re about to lose our best people to competitors who figured this out.”

Bring three numbers: current turnover cost for key roles, estimated Shadow AI spend (if 50% of employees spend $30/month, that’s already $15/employee/month with zero oversight), and productivity gains from the audit interviews.

**Form a cross-functional team** - This cannot be an IT-only project, and it cannot be a business-unit-only project. You need:

- **Business unit representative** who deeply understands the work and can identify high-value use cases
- **IT representative** who can evaluate technical integration and infrastructure requirements
- **Security representative** who can define risk thresholds and governance requirements (not veto everything, but establish guardrails)
- **Finance representative** who can track costs and measure ROI in ways your CFO will accept
- **HR representative** (for larger implementations) who can address training, change management, and retention implications

The team should be small (5-7 people maximum) and have explicit decision-making authority. If this becomes a 15-person committee that needs three approval layers, you’ve already failed.

Define the team’s mandate clearly: “Implement AI tools that demonstrably improve productivity within [your timeline], with appropriate security and governance, and prove ROI sufficient to justify broader deployment.”


### Assessment & Selection Phase: Stop Evaluating Generic Features

The failure mode here is spending three months creating elaborate capability matrices comparing 47 AI tools across 200 features. Nobody cares if a tool has a 4.7 vs 4.8 rating on G2. You care if it solves your specific problems.

**Identify priority use cases based on impact and feasibility** - Start with the problems that are:

- High-impact (hours of work per week, or quality issues that are expensive)
- High-frequency (happening daily or weekly, not quarterly)
- Already being solved by Shadow AI (your audit revealed this)
- Measurable (you can tell if it’s working or not)

Good use cases:

- “Our sales team spends 6 hours per week writing proposal responses that are 80% similar to previous proposals” (High impact, high frequency, measurable)
- “Customer support takes 45 minutes per ticket because they have to search through 15 different knowledge bases” (High impact, high frequency, measurable)
- “Engineers spend 4 hours per week writing documentation that’s often incomplete or out of date” (High impact, high frequency, measurable)

Bad use cases:

- “We want to be more innovative” (Not measurable, not specific)
- “AI will help us make better strategic decisions” (Not high-frequency, not measurable)
- “We should automate everything” (Not focused, not realistic)

Prioritize 2-3 use cases maximum for the pilot. You’re proving the model works, not solving every problem at once.

**Define security requirements and compliance constraints** - This is where most implementations die. Security becomes a veto, not a framework.

Instead of asking “Is this tool secure?” (answer is always “not secure enough”), ask:

- “What data classification levels are we working with?” (Public, internal, confidential, regulated)
- “What’s our acceptable risk threshold for each classification?” (Some data can go to cloud AI, some can’t)
- “What governance controls do we need?” (Audit logging, data retention policies, access controls)
- “What’s our compliance boundary?” (HIPAA, SOX, GDPR, etc.)

Create a decision matrix:

- **Green zone:** Public data, general business content → Can use cloud AI tools with standard terms
- **Yellow zone:** Internal data, no PII/regulated data → Can use cloud AI with specific contractual protections (data not used for training, encryption, access controls)
- **Red zone:** Regulated data, PII, trade secrets → Cannot use cloud AI, needs on-premise or specifically compliant solutions

Most use cases fall into green or yellow zones. If your organization treats everything as red zone, you’re creating security theater, not security.

The right answer is usually: “We can use ChatGPT Team/Claude Team for 80% of use cases with appropriate policies. We need specialized solutions for the 20% that involves regulated data.”

**Evaluate tools against specific business needs** - Now that you know your use cases and security requirements, evaluate tools by actually using them for real work.

Don’t evaluate based on:

- Feature lists in marketing materials
- Analyst reports ranking “AI capabilities”
- Demos with perfect example data

Do evaluate based on:

- Three real examples from your priority use cases. Does the tool solve them?
- How long does it take to get value? (Good tools work immediately, bad tools need weeks of configuration)
- What’s the learning curve for your actual users? (Have real users try them, not just the evaluation team)
- What’s the total cost including integration, training, and ongoing management?

Run this evaluation in parallel with 2-3 tools maximum. It should take 1-2 weeks, not months. You’re looking for “clearly solves our problem” vs “doesn’t work for us,” not optimizing for 5% better performance.


### Security & Planning Phase: Governance That Enables, Not Blocks

**Complete IT security review and compliance checks** - Security review should take days to weeks, not months to quarters. If your security review has been running for 90+ days, security is being used as a blocking mechanism, not a risk management mechanism.

The security review should answer:

- Where is data stored and who has access? (Specific controls, not general concerns)
- What data processing agreements are in place? (Can you get appropriate contractual protections?)
- What audit logging and monitoring exists? (Can you detect misuse?)
- What’s the incident response process if there’s a breach? (Specific plan, not “we’ll figure it out”)
- Does this meet our compliance requirements? (Specific regulations, specific controls)

For most cloud AI tools (ChatGPT Team, Claude Team, Microsoft Copilot), the answers are:

- Data stored in vendor cloud with encryption at rest and in transit
- Business terms explicitly state data is not used for training
- Audit logging available for enterprise plans
- Vendor incident response with SLAs
- Meets most compliance frameworks with appropriate configuration

If security identifies risks, the question is not “can we eliminate all risk?” (impossible) but “does the productivity benefit justify the residual risk with appropriate controls?”

Document the decision clearly: “We’re accepting X level of risk for Y business benefit with Z controls in place. Approved by [executive sponsor].”

**Establish governance frameworks and usage policies** - Create a simple, clear acceptable use policy. One page, not twenty.

Good policy:

- “Use AI tools for work tasks that don’t involve customer PII, regulated data, or trade secrets”
- “Don’t input: customer names/emails, financial data, health information, unreleased product plans, source code containing proprietary algorithms”
- “Do use for: drafting communications, summarizing public documents, brainstorming, general research, coding assistance with non-proprietary code”
- “When in doubt, ask [specific person/team]”

Bad policy:

- Twenty pages of legalese nobody reads
- Vague prohibitions like “don’t put sensitive information” (what’s sensitive?)
- Blanket restrictions like “don’t use AI for code” (nobody will follow this)

Make it easy to do the right thing and clear what the wrong thing is. Most users want to follow policy - they just need to know what it is.

**Select pilot users who will champion adoption** - This is critically important and commonly screwed up.

Don’t select pilot users who:

- Are “representative of average employees” (you need champions, not average users)
- Are skeptical of AI and need to be convinced (they’ll find reasons it doesn’t work)
- Are too busy to engage (they won’t use it, then will say it doesn’t work)

Do select pilot users who:

- Are already using Shadow AI or are enthusiastic about trying it
- Are respected by their peers (their testimonials matter)
- Have genuine productivity pain points the tool should solve
- Will give honest feedback, not just positive feedback
- Have time to learn and iterate

Aim for 10-30 pilot users for a small organization, 50-100 for a large organization. Big enough to get meaningful data, small enough to support well.

Tell them explicitly: “You’re helping us figure this out. We want honest feedback about what works and what doesn’t. Your job is to use this for real work and tell us what happens.”


### Pilot Phase: Learn Fast, Measure Everything

**Deploy tools to pilot group with clear success metrics** - Don’t just give people access and hope for the best. Set up the pilot for success.

Define success metrics before you start:

- Specific time savings: “Reduce proposal writing time from 6 hours to 2 hours per proposal”
- Quality improvements: “Reduce documentation errors from 15% to under 5%”
- User satisfaction: “80% of pilot users rate the tool as valuable or very valuable”
- Usage metrics: “Pilot users actively use tools at least 3x per week”

Make the tools as easy to use as possible:

- Pre-configure accounts for pilot users (don’t make them figure out procurement)
- Provide clear documentation on how to access and start using tools
- Create templates or examples for common use cases
- Set up a quick-response support channel (Slack channel, email, whatever works)

Track both quantitative metrics (time saved, usage frequency) and qualitative feedback (what’s working, what’s not, what’s surprising).

**Provide training focused on specific use cases** - Don’t do generic “AI 101” training. Do specific “here’s how to solve your actual problems” training.

Bad training:

- 2-hour presentation on “What AI is and how it works”
- Generic examples that aren’t relevant to real work
- Focus on features and capabilities rather than outcomes
- One-and-done training session with no follow-up

Good training:

- 30-minute sessions focused on specific use cases: “How to use AI for proposal writing,” “How to use AI for customer support,” “How to use AI for code documentation”
- Live demonstrations using real work examples from your organization
- Hands-on practice with immediate feedback
- Follow-up sessions after 1-2 weeks to address questions and share learnings

The best training is often peer-to-peer: have early successful users show others what they’re doing. People trust their colleagues more than training materials.

**Track usage patterns and collect feedback continuously** - Set up weekly check-ins with pilot users for the first month, then bi-weekly.

Ask specific questions:

- “What have you used the tool for this week?”
- “What worked well?”
- “What didn’t work or was frustrating?”
- “What would you try if you knew how?”
- “Are you saving time? How much?”

Watch for patterns:

- Are specific use cases working better than others?
- Are certain users getting much more value than others? (Why?)
- Are there common obstacles or frustrations?
- Are people finding use cases you didn’t anticipate?

Don’t wait until the end of the pilot to discover it’s not working. You should know within 2-3 weeks if people are getting value, and if not, you need to understand why and adjust.


### Measurement Phase: Prove It With Numbers

**Measure productivity gains with concrete metrics** - This is where most organizations fail. They deploy tools, people use them, but nobody can prove ROI because they didn’t measure properly.

Measure before the pilot:

- Baseline time for key tasks (how long does proposal writing take now?)
- Baseline quality metrics (what’s our error rate in documentation now?)
- Current costs (what are we spending on this activity now?)

Measure during and after the pilot:

- Time per task with AI tools
- Quality metrics with AI tools
- User-reported time savings (survey weekly)
- Usage frequency (how often are they actually using it?)

Get specific user stories with numbers:

- “Sarah used to spend 5 hours writing proposals. Now she spends 2 hours and uses AI to generate first drafts. She’s completing 50% more proposals per week.”
- “The support team’s average ticket resolution time dropped from 45 minutes to 28 minutes, a 38% improvement.”
- “Documentation error rate dropped from 18% to 7% after engineers started using AI to check their work.”

These stories are more valuable than aggregate statistics for making your case.

**Track costs against budget and compare to expected ROI** - Be honest about total costs:

- Tool subscription costs (obvious)
- Implementation time (how many hours did IT spend setting this up?)
- Training time (hours spent training pilot users)
- Management overhead (ongoing support and governance)

Compare total costs to demonstrated value:

- Time savings × employee cost per hour = productivity value
- Error reduction × cost of errors = quality value
- User satisfaction → retention impact (harder to quantify but real)

Example:

- Pilot: 30 users at $400/month = $12,000/month = $144,000/year
- Time savings: 8 hours/month average = 240 hours/month across 30 users
- Value at $50/hour fully loaded = $12,000/month = $144,000/year
- Net: Break-even in year 1, positive ROI as you scale

**Document success stories with specific numbers and user testimonials** - Create a simple case study document for internal use:

“Proposal Writing Pilot Results:

- Pilot size: 12 sales team members, 6 weeks
- Tool cost: $360/month total
- Time savings: Average 3.2 hours per proposal (was 6 hours, now 2.8 hours)
- Proposals completed: 47% increase (team completed 88 proposals vs. 60 in prior 6-week period)
- Quality: Client feedback scores unchanged at 4.2/5 (maintained quality while increasing speed)
- User satisfaction: 11 of 12 users rate tool as ‘valuable’ or ‘very valuable’
- ROI: $18,000 value for $2,160 cost = 733% ROI
- Quote from user: ‘I can now focus on customizing proposals instead of starting from scratch every time. I’m closing more deals because I can respond faster.’ - Sarah Chen, Senior Sales Rep”

Create 2-3 of these case studies from your pilot. They’re worth more than any generic AI presentation.


### Scaling Phase: Expand What Works, Not What Doesn’t

**Develop scaling plan based on pilot results** - Don’t just say “let’s roll this out to everyone.” Scale methodically based on what you learned.

Review pilot results honestly:

- Which use cases showed clear ROI? (Scale these first)
- Which use cases didn’t work? (Understand why before scaling - maybe wrong tool, maybe wrong use case, maybe need different approach)
- Which user groups got the most value? (Similar roles should be next to scale)
- What obstacles did users face? (Fix these before scaling)

Create a phased scaling plan:

- Phase 1: Expand to similar roles/use cases where pilot was successful (next 90 days)
- Phase 2: Expand to adjacent use cases with similar characteristics (next 180 days)
- Phase 3: Expand to remaining organization with appropriate tools per use case (next 360 days)

Don’t try to scale everything at once. Scale what’s working, learn from it, then scale more.

**Identify additional use cases and user groups** - Your pilot revealed new use cases you didn’t anticipate. Some will be valuable, some won’t.

Evaluate new use cases with same criteria:

- High impact × high frequency × measurable = good candidate
- Already happening via Shadow AI = proven demand
- Similar to successful pilot use cases = likely to work

Prioritize user groups for scaling:

- Groups most similar to successful pilot users (same roles, same problems)
- Groups with highest demonstrated need (they’re asking for access)
- Groups where productivity gains have biggest business impact

**Get approval and funding for broader deployment** - Go back to your executive sponsor with pilot results.

Present:

- Clear ROI from pilot with specific numbers
- User testimonials from respected employees
- Scaling plan with expected costs and expected returns
- Risk assessment based on actual pilot experience (not theoretical concerns)

Ask for specific budget allocation:

- “We need $X per month to scale to Y additional users over Z timeline”
- “Expected ROI based on pilot is A%, with B-month payback”
- “This prevents C employees from leaving for competitors with modern tools”

Make it easy to say yes. You’ve proven the model works. Now you’re asking to scale what works.


### Optimization Phase: Make It Sustainable

**Calculate and communicate ROI to stakeholders** - Create a regular reporting cadence (monthly or quarterly) showing:

- Current usage: X% of target users actively using tools Y times per week
- Measured productivity gains: Z hours saved per user per month = $A value
- Cost tracking: Spending $B per month as planned, or adjusted based on actuality
- User satisfaction: C% of users rate tools as valuable
- Example success stories: Specific users/teams achieving specific results

Keep reports concise - one page with key metrics and 2-3 success stories. Executives don’t need 20 slides, they need clear evidence it’s working.

**Establish regular measurement and reporting cadence** - Make measurement ongoing, not one-time:

- Automate usage tracking where possible (tool analytics, license usage)
- Survey users quarterly about value and satisfaction
- Collect new success stories continuously
- Track retention metrics for users with AI tools vs. without

Create a feedback loop: measurement → learning → improvement → measurement.

**Plan next phase investments based on demonstrated value** - Use your success to justify continued investment:

- If productivity gains exceed expectations, accelerate scaling
- If certain use cases show exceptional ROI, prioritize similar use cases
- If new tools become available that solve current limitations, pilot them using same process
- If costs are increasing as predicted, get ahead of budget planning cycle

The goal is to move from “we’re experimenting with AI” to “we have systematic AI enablement as part of how we work.”

---

The key principle underlying all of this: **move as fast as your organization’s risk tolerance allows, but don’t skip the measurement steps**. The failure mode isn’t moving too fast - it’s getting stuck in permanent pilot mode because you can’t demonstrate value without measuring it, and you can’t measure without clear metrics from the start.

Organizations that execute this sequence typically move from kickoff to scaled deployment in 3-9 months, depending on size and complexity. Organizations that skip steps or let any phase drag on indefinitely usually fail to scale at all, and their best people leave for companies that figured it out.


## The Stakes

This is not about keeping up with trends. It’s about whether your organization can compete. Companies that figure out their legacy procurement processes are actively hurting their ability to enable great work will kick those processes aside and find ways to get their teams what they need. They’ll attract and retain the best AI talent. Their productivity will reflect what’s possible with modern tools.

Everyone else will keep using 2023 budgets for 2025 work, wondering why their best people are leaving and why their AI productivity gains are disappointing. The answer will be obvious in retrospect: you can’t deliver extraordinary results with tools you won’t pay for.

---


## Take Action

**If you’re an individual contributor:** Send this to your manager with specific examples of tools you need and how they’d improve your work. Be concrete about the productivity gains, not abstract about “staying current.” If you can estimate hours saved monthly, do the ROI math yourself.

**If you’re a manager:** Schedule a meeting with your boss this week. Bring three things: (1) specific tools your team needs, (2) productivity calculations showing ROI using the framework above, and (3) turnover cost analysis. Ask for a dedicated AI tools budget separate from traditional software line items.

**If you’re in leadership:** Audit your current AI tool spending and shadow AI usage. You’ll find your organization is already spending more than you think—just without governance or strategic direction. The question isn’t whether to invest, but whether to invest strategically or let it happen chaotically.

The companies that adapt will have an enormous advantage. The ones that don’t will lose their best people to competitors who understand what’s actually at stake.

---

**Sources:** This analysis draws on data from Zylo’s AI cost research, Stack-AI enterprise market studies, BCG’s software cost analysis, SignalFire’s 2025 talent report, Forrester’s shadow AI research, Google Cloud’s case study compilation, and multiple industry retention cost studies.

[![](https://substackcdn.com/image/fetch/$s_!K74H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b263800-2a83-4320-9913-33ba0488d3d7_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!K74H!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0b263800-2a83-4320-9913-33ba0488d3d7_1024x1024.png)
