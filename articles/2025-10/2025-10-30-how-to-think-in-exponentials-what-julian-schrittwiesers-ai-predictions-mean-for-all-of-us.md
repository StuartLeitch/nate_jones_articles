---
title: "How to Think in Exponentials: What Julian Schrittwieser’s AI Predictions Mean for All of Us"
author: "Nate Jones"
published: 2025-10-30
url: https://natesnewsletter.substack.com/p/new-proof-ai-is-exploding-exponentiallybut
audience: everyone
scraped_at: 2026-01-05 19:13:11
---

I keep getting asked the same question: “Is there an AI bubble, Nate?”

We’re not. But it’s been hard to yell loud enough to be heard over the tide of noise when everyone wants to believe we’re in an AI bubble.

[![](https://substackcdn.com/image/fetch/$s_!33HR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba761e9f-e335-4af4-9530-7d5fa6d151bd_2242x1342.png)](https://substackcdn.com/image/fetch/$s_!33HR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba761e9f-e335-4af4-9530-7d5fa6d151bd_2242x1342.png)

The bubble narrative is becoming a bubble.

But seriously, the thing that really bothers me is that none of this speculation is free.

There is real risk here. People who believe AI is a bubble are going to spend the next 12-18 months thinking nothing will really change and by then it will be too late.

If you’re familiar with [Pascal’s Wager](https://en.wikipedia.org/wiki/Pascal%27s_wager), this is taking the wrong end of the bet.

So this post is all about helping you understand where we REALLY are in the AI world, what that ACTUALLY means for jobs, and how to think about AI skills as a result.

Think of it as a companion to my Executive Briefing on [AI fluency](https://natesnewsletter.substack.com/p/executive-briefing-ai-usage-is-not?r=1z4sm5) earlier this month. That piece focused on how to measure and build skills for your team.

This piece focuses on you as an individual—what skills do you need to survive and thrive in the new AI exponential world? How can we measure where you’re at on AI exponential readiness (yes we can use AI to do this) and how can we make sure that you have a thoughtful plan for how to move forward?

The heart of this piece came from a close read of Julian Schrittwieser’s excellent writing on AI exponentials. I’ve taken time in this piece to dive into what Julian thinks, why it bursts the bubble narrative, and then I’ve extended his work into a novel thesis on why the exponential gains we’re seeing WON’T disrupt jobs the way many in the media assume.

Then I dig into what you DO need for the AI exponential future, and give you a prompt pack to give yourself an AI exponential readiness test and identify specific action steps you can take to get ready for the next few months.

Here’s what’s in the box:

**A complete breakdown of Julian’s exponential thesis**

- The AI tests that show us exponential gain and can’t be gamed
- The mechanism driving the exponential
- Julian’s specific timeline predictions for AI

**Why the “jobs are doomed” thesis is wrong**

- What exponential gains DON’T mean
- Breakdown of a new study on task automation
- What this reveals about AI-Human working models
- Bonus note for solo founders

**My own deep dive on the skills that compound as AI capabilities grow**

- AI Direction (give the robot a job)
- AI Evaluation (did it do a good job)
- Task Decomposition (can the robot handle it)
- Learning Velocity (are you fast at learning?)
- Why these skills get MORE valuable as the exponential continues
- Yes, they’re learnable

**Your AI Exponential Fluency level-up:**

- Five scored assessments: curve reading, compound skill identification, cognitive failure recognition, leverage positioning, signal reading
- Takes 20-25 minutes with any LLM
- Delivers positioning category, gap analysis, timing assessment, ranked 90-day action plan
- Re-run quarterly to track improvement

**Who needs this?**

- Anyone asking “is there an AI bubble?” who wants the actual data
- Founders trying to figure out what they can attempt solo now
- Teams wondering whether small + fluent beats large + casual
- Engineers, PMs, consultants, researchers trying to read capability curves themselves
- People who want to stop debating and start building the skills that compound
- Anyone who doesn’t want to miss the exponential while everyone else is still arguing about whether it exists

Why am I writing this? Because there’s a peculiar information asymmetry happening right now. People inside the labs—the researchers building these systems—see explosive progress on metrics that matter economically. People outside see chatbots making mistakes and conclude we’ve plateaued.

That gap is dangerous. The preparation window is closing fast. If capabilities are doubling every few months, waiting 6-12 months to get fluent means you’re always catching up while the gap widens.

I don’t know if Julian’s exact timeline is right. Could be 18 months, could be 36. But I know how to read the curve myself now, and I want you to learn that too.

Because exponentials are hard to time but easy to miss entirely if you’re not paying attention.

Let’s learn to think in exponentials, and skill up!


## [Grab the Exponential Fluency Assessment](https://www.notion.so/product-templates/AI-Future-Exponentials-Assessment-29b5a2ccb5268004a501f2afd39583a6?source=copy_link)

YES, I specifically built this to go with my Executive Briefing [AI fluency assessment](https://natesnewsletter.substack.com/p/executive-briefing-ai-usage-is-not?r=1z4sm5). This one is designed to measure your ability to adapt as AI continues to gain capabilities rapidly—as the exponential takes hold. As before, it’s a series of prompts you can use to talk to an LLM like ChatGPT and assess your readiness, plus a score overall. Have fun and good luck riding the curve!

---


# How to Think in Exponentials: What Julian Schrittwieser’s AI Predictions Mean for All of Us

I keep seeing the same pattern. People argue about whether AI is overhyped while the actual capability curves just keep climbing. And most people miss it completely—the same way so many of us missed exponential COVID growth until it was too late.

Here’s what I mean: while the industry as a whole was debating the AI bubble (especially in the last month), autonomous task length has been on a doubling pace for over 18 months now: from 15 minutes to 2+ hours. That’s not hype. That’s measured capability growth from METR, an independent AI safety organization that tracks how long models can work without human intervention. Specifically METR measures how long a *human-equivalent* task a model can complete.

So in this case models can now autonomously complete tasks that take humans two hours, with (of course) anecdotes of much much longer tasks getting done in certain situations. I know a Principal PM who used a nicely formatted Deep Research prompt to save himself ~3 months of human work, for instance.

I’ve mentioned METR before. I like them because they are hard to game. Julian Schrittwieser likes them too. He’s the researcher who built AlphaGo, MuZero, and now works at Anthropic, and he is someone to listen to when he has an opinion.

In this case, he’s gone on the record saying we’re missing the exponential again, and says we’ve all got maybe 18 months before the AI exponential scales past human expertise in many areas. And here’s the thing: he’s not guessing. He’s reading data most people can’t see, pointing at data (like METR) we CAN see, and he’s already been proven right once when he called out the original doubling law for autonomous agent execution.

The question I want you to wrestle with isn’t whether to trust his specific timeline. It’s whether you know how to read exponential curves yourself, and whether you’re building the skills that compound as AI capabilities accelerate. Because if Schrittwieser is even directionally correct, the gap between people who figured out AI fluency in 2025 and people who figure it out in 2027 could be two years of compounding advantage.

That’s not a gap you make up.


## Why You Should Listen to Julian Schrittwieser

So who is this Julian guy?

Julian Schrittwieser isn’t making predictions from the sidelines. He spent the last decade inside the two labs—Google DeepMind and Anthropic—that have arguably made the most significant breakthroughs in AI. And his track record isn’t just impressive, it’s a highlight reel of “impossible” achievements that became reality.

He built AlphaGo, the system that beat world Go champion Lee Sedol in 2016 when most experts said it wouldn’t happen for another decade. Then he made AlphaGo Zero, which learned from scratch without any human games. Then MuZero, which learned to play games without even knowing the rules. Then AlphaCode, which writes competitive programming solutions. Then AlphaTensor, which discovered new algorithms for matrix multiplication. And AlphaProof, which proves mathematical theorems.

When he talks about exponential progress, he’s describing the systems he’s building, not speculating about someone else’s work. He sees internal evaluations showing capability improvements months before they become public. He watches competing labs—OpenAI, Anthropic, DeepMind—hit the same exponential slopes independently, which suggests they’re all discovering the same fundamental scaling laws rather than gaming benchmarks.

Here’s why his credibility matters: he’s been proven right before when everyone else was wrong. When AlphaGo beat Lee Sedol in 2016, it was his system that made the remarkable Move 37. It was a move that violated basic Go strategy, that experts called a mistake, that later proved optimal. That moment demonstrated AI discovering solutions humans hadn’t even considered. Schrittwieser was core to making that happen.

So when he says we’re in an exponential regime and people are missing it, I pay attention. And that’s exactly what he’s [saying in his blog](https://www.julian.ac/blog/2025/09/27/failing-to-understand-the-exponential-again/), and on a 70 minute podcast that released this week.


## Why I’m Writing This

Look, I need to be direct about something. If you’re in San Francisco, if you visit the frontier labs, if you talk to people who are on the cutting edge of AI research, none of them are saying we’ve hit a wall. And it’s not just the people with gigantic piles of equity like Sam Altman who have the incentive to pump things up. It’s researchers like Julian who are just trying to understand how to make systems work better.

And yet I keep getting asked from OUTSIDE SF: “Is there an AI bubble, Nate?”

I gotta say, I see the same thing Julian sees. I don’t see evidence of a bubble. And I’m really glad he did this podcast and blog post because he articulates it better than I can. There’s a peculiar information asymmetry happening right now—people building these systems see explosive progress in metrics that matter economically, while the outside world debates whether we’ve plateaued.

That gap matters. And I want to close it.


## So what’s in Julian’s blog post?

On September 27, 2024, Schrittwieser published “Failing to Understand the Exponential, Again” on his personal blog. The title alone tells you everything: he’s calling out the same cognitive failure that happened with COVID-19, where politicians and commentators dismissed exponential case growth even when the data was screaming at them.

His argument cuts through the noise. People see models making mistakes and conclude AI will never work, completely ignoring that these same tasks were impossible science fiction three years ago. They compare two consecutive model releases, don’t notice much difference in casual ChatGPT conversations, and declare scaling is over. All while missing the exponential trendline that’s visible in rigorous evaluations.

I’ve been after Wall Street and journalists both for missing this for awhile now, and both camps have been instrumental in peddling the AI bubble narrative in the last month. And don’t get me wrong: the narrative is real (even if the facts are not).

[![](https://substackcdn.com/image/fetch/$s_!4Eb6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63b480f4-04b0-4dc0-95aa-33abce0cb3b5_2232x1268.png)](https://substackcdn.com/image/fetch/$s_!4Eb6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F63b480f4-04b0-4dc0-95aa-33abce0cb3b5_2232x1268.png)

This graph is wrong, and Julian and I are both right.

Here’s Julian’s argument: we can see evidence of a reliably predictable exponential on metrics that matter across multiple measures that cannot be gamed.

Let’s dive into what he means:

First, METR measured how long AI can work autonomously on software engineering tasks. The result: capability doubles every 7 months, growing from roughly 15-minute tasks to 2+ hour tasks in seven months. That’s not a projection. That’s what actually happened.

Second, OpenAI’s GDPval evaluated AI performance across 1,320 real work tasks spanning 44 professions—lawyers, doctors, consultants, engineers—graded by experienced professionals who couldn’t tell if they were rating human or AI work. The result: the same exponential slope, with Claude Opus 4.1 already matching human expert performance across industries.

Now here’s what separates this from typical AI hype: METR published their finding that autonomous task duration doubles every seven months. Seven months later, new models from competing labs—Anthropic’s Opus 4.1, xAI’s Grok 4, OpenAI’s GPT-5—were released and landed exactly where the math predicted.

This wasn’t curve-fitting after the fact. It was a forecast that held up when tested by independent companies racing against each other. That’s the difference between hype and signal.

And by the way, Julian specifically praised OpenAI for publishing GDPval even though Anthropic’s model performed best. That’s intellectual integrity—they’re not trying to game their own benchmarks, they’re trying to measure real capability.

One more thing, GDPVal was published for the first time AFTER the models it measured were released. Again, the models couldn’t be trained to game it. And Julian is correct that OpenAI admitting Claude was better reinforces the integrity of the measure.


## How Tech Drives an Exponential

So what about it? What is making this exponential believable and not just correlation? Julian gets into that as well.

Understanding the mechanism matters because it helps you evaluate whether the exponential can continue or will hit fundamental limits. And the mechanism is actually elegant once you see it.

Pre-training—reading massive amounts of human-written text, code, and data—accomplishes two things simultaneously. First, it builds an internal model of how the world works by predicting what comes next, which requires understanding cause and effect, physical constraints, and human behavior. Second, it instills baseline alignment by observing what humans value and care about.

Here’s something most people don’t know: Reddit has mostly been purged from frontier model training data. It wasn’t high enough quality. The training corpus now focuses much more heavily on very curated sources, like scientific papers, high-quality books, and curated text. That matters because starting with a clean, excellent corpus of human knowledge gives you both efficiency and safety benefits—you’re not trying to filter out garbage after the fact.

This is why starting from scratch isn’t necessary. Human data provides both the knowledge base and the value system.

After pre-training, reinforcement learning enables trial-and-error improvement. Here’s the crucial breakthrough that makes this exponential rather than linear: better AI generates better training data for itself.

An intermediate-level coding AI writes intermediate-level code that can be automatically checked for correctness. That becomes training data for reaching advanced-level capability. Then advanced AI generates harder problems, creating a flywheel where improvement accelerates because you’re not constrained by “how much human-generated data exists?”

This data generation mechanism is why concerns about training data exhaustion miss the point. The bottleneck shifts from “how much human data is available?” to “how efficiently can models explore and evaluate solution spaces?” As long as you can automatically verify correctness—which you can for code, math, science, and increasingly other domains—the model generates its own curriculum.

Schrittwieser spent significant podcast time explaining why this took years to scale. Not theoretical barriers, but engineering challenges around stability (RL training can be unstable), feedback loop complexity (how do you provide useful learning signal?), and infrastructure (running millions of evaluations to generate training data requires enormous computation). The labs that figured out these engineering problems first are now seeing exponential returns.

The Move 37 concept helps understand what’s possible when you combine search with learning. AlphaGo’s famous move violated basic Go strategy, looked like a mistake to expert players, then proved optimal. This demonstrated AI finding solutions by exploring possibilities rather than mimicking human play.

Modern language models show similar behavior in programming and mathematics—generating approaches that surprise domain experts but prove correct upon examination. This matters because it means AI can make genuine discoveries rather than just remix existing knowledge.

The prediction of Nobel-level breakthroughs by 2027-28 isn’t about AI reading all the papers faster than humans. It’s about AI exploring solution spaces—potential drug compounds, material structures, mathematical approaches—more thoroughly than human intuition can guide. AlphaGo didn’t just play like the best humans. It found moves the best humans hadn’t considered. The same dynamic could apply to scientific discovery.


## So What About Humans?

This is where most analysis goes wrong. The question isn’t “what skills survive AI?”—it’s “what skills compound as AI capabilities grow?”

Julian is very clear that he views AI skills as critical, but that fundamentally he sees a future of AI-human collaboration as the future of work. So do I.

I went a bit farther than Julian and named some skills I think pop out as useful:

1. AI Direction (the ability to direct AI usefully and efficiently) is the meta-skill that matters most. Can you take a fuzzy problem and decompose it into clear, executable instructions? This isn’t about knowing magic prompts. It’s about understanding what you want well enough to specify it precisely, recognizing what context AI needs that you haven’t provided, and iterating efficiently when the first attempt misses. This skill gets better with deliberate practice and increases in leverage as AI capabilities grow. The person who can nail the specification in one conversation versus ten has a 10x time advantage that compounds across every task.
2. AI Evaluation means catching when outputs are brilliant versus plausible-sounding nonsense. This isn’t about knowing everything—it’s pattern recognition for “does this make sense given the constraints?” and “what assumptions did I forget to specify?” As AI gets better, the errors get subtler, making evaluation more rather than less valuable. The people who develop strong evaluation instincts become quality gates for high-stakes decisions. They’re the difference between “we can use AI for this” and “we can’t trust AI for this.”
3. Task Decomposition is knowing what to delegate to AI versus what to do yourself. Not based on fixed rules like “AI can’t do creative work” (wrong—AI can generate hundreds of creative options), but situational judgment: “AI can explore this solution space in ten minutes, I’ll pick the right direction in thirty seconds.” The skill is recognizing which tasks benefit from AI’s breadth (generation, exploration, iteration) versus human strengths (taste, strategic direction, final decisions under ambiguity).
4. Learning Velocity is a must. AI capabilities are changing every month, literally. Waiting for “best practices” to emerge means you’re always 6-12 months behind. The skill is adopting new capabilities fast, learning what works through experimentation, and moving to the next capability before others have mastered the last one. This is classic builder mentality, just operating at higher speed.

These are large skill clusters in reality. They need to be operable across multiple toolsets and evolve themselves as AI evolves. These skills compound because each improvement in AI capability multiplies their value. Getting 10% better at AI direction when models can handle 30-minute tasks means you save 3 minutes per task. Getting 10% better when models can handle 3-hour tasks means you save 18 minutes. The same skill improvement has 6x more impact.

This is why early investment in these skills has asymmetric returns—you’re building capabilities that become more valuable as the exponential continues.


## No, AI Will Not Take the Jobs

But here’s what almost everyone gets wrong when they look at these curves. They see METR showing exponential growth in autonomous work duration—15 minutes to 2+ hours—and they immediately jump to “AI is going to automate all the jobs.”

That’s not what’s happening. And the data proves it. Julian did NOT get into this as much, so I’m bringing in my own research and data here and making what I believe is a novel articulation of a jobs thesis in the age of AI:


#### Exponential gains in AI agent execution time will not translate into full automation, because jobs are much more complex than a sum of tasks.

In other words, you can get a LOT better at task delegation on even big tasks and make almost no progress on automation. This gets back to [Polanyi’s Paradox](https://en.wikipedia.org/wiki/Polanyi%27s_paradox), which I’ve written about before.

And the data is starting to show exactly that.

The Remote Labor Index from Scale AI and the Center for AI Safety tested AI’s ability to fully automate hundreds of real-world projects from remote work platforms. These are actual tasks that matter economically—software development, architecture, game development, scientific document preparation. The kind of work that METR measures in terms of hours of autonomous activity.

Current automation rate: less than 3%.

[![First image is a bar chart titled Current AI Agents Fully Automate Few Tasks with y-axis from 0 to 100 percent and x-axis showing AI models: Gemini 2.5 Pro, ChatGPT, GPT-5o, Sonnet 4.5, Grok 4, Manus. Bars indicate low automation rates under 10 percent for each model, with icons representing each AI. Second image shows a research paper abstract on Remote Labor Index measuring AI automation of remote work, listing authors including Dan Hendrycks from Center for AI Safety and Scale AI, describing progress on benchmarks and index design for evaluating AI agents. Third image is a 3x2 grid of panels depicting AI project tasks: top-left interactive dashboard with world map and data; top-middle 3D product render of device with battery and case; top-right animated video with human figure and deliverables; bottom-left architecture plan with building design; bottom-middle game development with character and physics interaction; bottom-right scientific document with figures and equations for IEEE.](https://substackcdn.com/image/fetch/$s_!uIgb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F374d9064-849b-416b-84db-968ed4469501_1200x747.jpeg "First image is a bar chart titled Current AI Agents Fully Automate Few Tasks with y-axis from 0 to 100 percent and x-axis showing AI models: Gemini 2.5 Pro, ChatGPT, GPT-5o, Sonnet 4.5, Grok 4, Manus. Bars indicate low automation rates under 10 percent for each model, with icons representing each AI. Second image shows a research paper abstract on Remote Labor Index measuring AI automation of remote work, listing authors including Dan Hendrycks from Center for AI Safety and Scale AI, describing progress on benchmarks and index design for evaluating AI agents. Third image is a 3x2 grid of panels depicting AI project tasks: top-left interactive dashboard with world map and data; top-middle 3D product render of device with battery and case; top-right animated video with human figure and deliverables; bottom-left architecture plan with building design; bottom-middle game development with character and physics interaction; bottom-right scientific document with figures and equations for IEEE.")](https://substackcdn.com/image/fetch/$s_!uIgb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F374d9064-849b-416b-84db-968ed4469501_1200x747.jpeg)

Think about what this means. Autonomous work duration is growing exponentially. But full task automation remains near zero. These aren’t contradictory findings—they’re revealing something crucial about how this exponential actually manifests.

The 15 minutes to 2+ hours curve means AI can handle longer stretches of work within a human-directed workflow. The sub-3% automation rate means it’s not remotely on track to replace entire projects or jobs independently. AI is becoming dramatically better at executing within context, but it still needs humans to set direction, provide judgment, and handle the ambiguous parts.

In other words, there is a low gear ratio between having an agent work for awhile and actually seeing full role automation. The former doesn’t touch enough of the complexity of a real job to really replace a human.

This is the most data-backed version of Julian’s complementarity thesis. The exponential is real. The capabilities are real. But what we’re seeing isn’t wholesale replacement—it’s collaboration leverage. Humans + AI doing 10x more work, not AI doing 100% of the work.

I’m already seeing this with solo founders. They’re able to pick up their weak spots—the marketing they’ve always struggled with, the financial modeling they’ve avoided, the code they never had time to write properly—in ways that solo founders have traditionally just had to suffer through. They’re not getting replaced. They’re getting superpowers in their weak areas while staying focused on their strengths.

This is what 10x productivity actually looks like in practice. Not one person replaced by AI, but one person empowered to do what used to require a team. The exponential shows up as capability expansion, not capability replacement.

> Sidebar: for those saying I need to wait 6 months and then things will be more doom-filled, I say—I just did. I’ve heard that argument since early last year in fact. And I am not seeing evidence that rapidly scaling intelligence gains are leading to huge chunks of jobs getting automated reliably.
>
> As an example: no software engineer will tell you their job is in danger if they are worth any salt. And yet code is where automation is supposed to hit first! Gergely Orosz has laid out exactly why engineering is more than coding over the last year in a series of posts, and I encourage you to read him if you’re wondering.


## How to Measure Your Own Fluency

The trap is treating AI fluency as binary—you either have it or you don’t. The reality is it’s a spectrum that you can measure and improve deliberately.

It’s much more complex than that, and I built a whole diagnostic to help you figure out how well positioned you are for an exponential world. I love it so much I am putting it in this post twice, just in case you missed it at the top.


#### [You grab your AI exponential diagnostic here](https://www.notion.so/product-templates/AI-Future-Exponentials-Assessment-29b5a2ccb5268004a501f2afd39583a6?source=copy_link)

---


## A Few Closing Thoughts for Builders

If Schrittwieser is directionally correct about exponential capability growth, three dynamics reshape how you should think about building.

First, leverage becomes everything. One person with strong AI fluency can output what used to require a team. This has profound implications for what you can attempt solo. The ambitious product that seemed to require raising money for a team of 5-8 people might now be tractable if you’re excellent at AI direction. Capital requirements for software businesses are collapsing, but fluency requirements are rising. Small teams with deep AI capabilities beat large teams with casual AI usage, and the gap is widening.

Second, speed compounds differently. You can iterate 10x faster on implementation, which means you can try 10x more ideas, which means you find product-market fit faster. But the advantage isn’t just “build faster”—it’s “learn faster.” Running 100 experiments in the time competitors run 10 creates an information advantage that accumulates. Early experimentation when others are still cautious produces disproportionate learning.

Third, first-mover advantage intensifies. If capabilities are doubling every 7 months, getting fluent now versus waiting 6-12 months could mean the difference between leading and catching up. People who figure out AI leverage in 2025 have time to establish themselves, build proof of what’s possible, and develop advanced skills before it becomes obvious to everyone. The window where being early provides meaningful advantage might be shorter than in previous technology transitions because the capability growth is faster.

Look, I say this all the time but it bears repeating: the preparation window is closing fast. If you can draw straight lines on a chart and you see them going up exponentially, we have to get ready for this now. There’s not another moment. It will not get easier if you wait six months. The skills that compound—direction, evaluation, decomposition, learning velocity—they build on each other over time. Starting later means you’re always playing catch-up while the gap widens.

The smart move is learning to read the exponential yourself. Watch the metrics that matter: autonomous task duration, evaluation scores on real work, adoption patterns in leading companies. Track your own fluency improvements. Build the skills that compound regardless of whether the timeline is 18 months or 36 months.

Exponentials are hard to time but easy to miss entirely if you’re not paying attention.


## The Optimistic Frame

I want to be clear about something. I do think the evidence is starting to point us toward abundance, not replacement. One person can do much more than before BUT we don’t see real evidence that indicates dispensing with the human touch is going to be good, profitable, or useful anytime soon. And as this post argues, we’re starting to see evidence that AI is in fact best positioned as a helper for humans.

All this means that small companies can compete with large ones. Ambitious projects become tractable. The barriers to building are collapsing for people who invest in fluency.

If you’re scrappy, learn fast, and willing to iterate, the next 12-24 months represent the highest-leverage skill-building period in decades. The question isn’t “will AI replace me?”—it’s “how fast can I learn to leverage AI to build things that were previously impossible?”

Schrittwieser’s thesis isn’t doomer. It’s not even primarily about jobs or displacement. It’s about recognizing that we’re in an exponential growth phase for AI capabilities, and that exponentials create massive opportunities for people who understand them early.

The people who win aren’t the ones who predict the exact timeline. They’re the ones who develop the skills that compound as capabilities grow, who build when others are still debating, who learn in public while others wait for certainty. They’re the ones who read the curve themselves rather than waiting for experts to tell them what to think.

Learn to think in exponentials. Measure your fluency. Build the skills that compound.

Everything else is noise.

[![](https://substackcdn.com/image/fetch/$s_!EoSl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76606232-59d0-4136-9ef5-2fa7a33f9216_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!EoSl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F76606232-59d0-4136-9ef5-2fa7a33f9216_1024x1024.png)
