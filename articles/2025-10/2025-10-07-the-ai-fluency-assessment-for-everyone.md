---
title: "The AI Fluency Assessment for Everyone"
author: "Nate Jones"
published: 2025-10-07
url: https://natesnewsletter.substack.com/p/the-ai-fluency-assessment-for-everyone
audience: everyone
scraped_at: 2026-01-05 19:15:33
---

I get really tired of AI tests online. Everyone uses ChatGPT. Everyone does their best.

Everyone says they have the skills (or hopes they do and hopes you won’t check.)

Measuring AI skills is become near-impossible on job interviews, and at the same time showing those skills is brutally hard for the rest of us.

How can the most important skill in the new world of AI be this hard to measure??!

I’ve talked to hundreds of job seekers and hiring managers in the last year, and there are three big reasons why AI fluency is impossible to measure right now:

1. The skill sets are changing so fast (like every new ChatGPT release)
2. AI is changing the jobs themselves really fast—so what’s needed?

We lack a common vocabulary for AI skills—so who knows what’s good?

Put simply, I’m aiming to tackle all three of those with this post.

- If you are interested in leveling up your AI skills, this tells you where you are and what you need to work on

  - This is true for engineers as much as for non-technical roles
  - It also holds true across levels
  - Why? I designed it to measure foundational AI fluency levels we all need
- If you’re hiring for AI, this is the first foundational level, job-independent AI assessment out there

  - And yes, it’s hard to game in interviews! (I made it that way on purpose)
- This is also the first intentionally future-proofed AI fluency assessment

  - I built it anticipating future AI developments
  - It’s designed to flex and grow with future releases

How does all this work?

Well first, I’ll explain and lay out the framework for my competency assessment. I’m measuring:

- Prompt mastery
- Technical understanding (NOT code)
- Practical application ability
- Critical evaluation ability
- Workflow design ability

I explain how I weight those and why, and then I give you a guided, accessible assessment that walks you through the test step by step. All you have to do is have a quiet place to think and a willingness to be honest (it’s ok it gets better!)

Then I give you a comprehensive scoring prompt PLUS a custom 90 day level up prompt that will take your scores and work with you to develop a level up prompt to get you where you want to go.

In addition, I have my own notes as a seasoned AI watcher—how different fluency levels look and present in different contexts, what skills are involved, how they will evolve in the future. Super useful stuff!

I’ve even got notes for coaches and teachers here—stuff you can read as you implement this test in a classroom setting or as HR with a team.

I hope you enjoy, and best of luck leveling up on AI in this wild new world we all are living in together!


# The AI Fluency Assessment for Everyone

There’s a specific moment I see repeatedly when working with smart people on their AI workflows. They’ll show me how they use ChatGPT or Claude, walking me through their process with confidence. Then I’ll ask them to solve a specific problem—something practical, like drafting a technical proposal or analyzing a dataset. And within thirty seconds, I can tell exactly where they sit on the skill spectrum.

Usually, it’s lower than they think.

This isn’t about intelligence. The executives, founders, and professionals I work with are brilliant. But AI fluency is a distinct skill set, and most people have no reliable way to gauge their actual competence. You can’t see the gap between your current approach and what’s possible until someone shows you what advanced usage actually looks like.

Here’s what I’ve learned: The difference between a competent AI user (call it a 5 or 6 out of 10) and an advanced user (7 or 8) is worth somewhere between 10 and 15 hours of productive output per week. That’s not hype—that’s the difference between having documented workflows with specific prompt patterns versus winging it every time. And by 2026, that gap will likely determine who advances in their career and who plateaus.

The problem is measurement. You can feel when you’re getting value from AI, but you don’t know if you’re at 40% of what’s possible or 80%. You don’t know which skills you’re missing or what would unlock the next level of capability. You’re driving without a speedometer, assuming you’re doing fine while others who’ve cracked the code are lapping you.

So here’s what I’ve built: a self-assessment framework that lets you accurately score your AI fluency in under 20 minutes. More importantly, it shows you exactly what separates each level, so you can see where you actually stand and what specific skills move you up the ladder.

This isn’t an academic exercise. Your score tells you whether you’re in the danger zone, coasting, or building genuine competitive advantage. And it gives you a clear roadmap for what to learn next.


## The Five Components That Actually Matter

AI fluency isn’t one skill—it’s a cluster of related capabilities. After working with hundreds of people at different skill levels, I’ve identified five components that predict actual performance. Here’s how they break down and how much each one matters to your overall competence.


### 1. Prompt Mastery (40% of your score)

This is the foundation, which is why it carries the most weight. But prompt mastery isn’t just about writing clear instructions—it’s about architecting prompts that consistently produce the results you need.

There’s a progression here that most people don’t see. At the beginning, you’re writing prompts the way you’d ask a junior assistant for help: “Write a blog post about AI.” That’s a Level 2. You’ll get something back, but it won’t be what you actually need.

By Level 5, you’ve learned to add specificity and context: “Write a 1,000-word blog post for technical founders about prompt engineering. Focus on ROI rather than theory. Use a direct, practical tone.” This is competent. You’ll get usable output.

But Level 8 looks completely different. You’re not just adding details—you’re designing the entire interaction: “You’re writing for technical founders who’ve tried AI tools and been underwhelmed. They’re skeptical about reliability and unclear on ROI. Address those specific objections directly. Use the three-stage framework I outlined earlier for content generation, and include one concrete example of time savings. Format as scannable sections with clear headers. Target 1,200 words.”

See the difference? You’re not just being specific—you’re anticipating objections, referencing previous context, specifying format, and building in validation criteria. This is prompt architecture, not just prompt writing.

The progression continues to Level 10, where you’re designing multi-turn workflows, building custom GPTs or Claude Projects with specific instructions, and creating prompt libraries for repeatable tasks. But the key insight is that each level requires learning new patterns, not just practicing what you already know.


### 2. Technical Understanding (15% of your score)

You don’t need to understand how transformers work or what attention mechanisms are. But you do need to know which tool to use when, what their limitations are, and how to work around those constraints.

A Level 2 user knows that ChatGPT exists. A Level 5 user knows that Claude is better for long-form analysis, GPT-4 is faster for quick tasks, and Perplexity is the right choice when you need current information with citations. A Level 8 user understands token limits, knows how to structure information to work within context windows, and can explain why a task failed and how to fix it.

This isn’t about technical credentials—it’s about practical knowledge that makes you effective. You need to understand enough to make good decisions about tool selection and troubleshoot when things go wrong.


### 3. Practical Application (20% of your score)

This measures whether you’ve actually integrated AI into your real workflows or if you’re just experimenting with it occasionally.

A Level 3 user might use ChatGPT for everything, regardless of whether it’s the right tool. A Level 6 user has specific use cases where AI provides consistent value—maybe content drafting, data analysis, or research synthesis. A Level 8 user has built multi-tool workflows with clear handoffs: web search for current information, Claude for analysis, a specialized tool for formatting or visualization.

The key here is systematization. Can you describe your process for common tasks, or do you start from scratch every time? Have you documented what works, or are you relying on memory and intuition?


### 4. Critical Evaluation (15% of your score)

Advanced AI users know that the output is a starting point, not a finished product. They’ve developed validation habits that catch errors, bias, and hallucinations before they cause problems.

This isn’t about paranoia—it’s about professional standards. A Level 3 user takes output at face value. A Level 6 user spot-checks for accuracy and rewrites anything that sounds off. A Level 8 user has systematic verification processes: checking citations, validating logic, comparing outputs across models, and maintaining healthy skepticism about anything that seems too convenient.

The ability to evaluate output critically is what separates people who get burned by AI mistakes from people who use AI reliably in high-stakes situations.


### 5. Workflow Design (10% of your score)

This is the advanced skill that most people never develop. It’s the difference between “I use AI for writing” and “I have a three-stage content pipeline that produces consistently high-quality output.”

A Level 6 user has informal processes—they know what generally works. A Level 8 user has documented workflows they can teach to others or replicate across different contexts. A Level 10 user is designing systems that other people can use, building custom tools, and solving problems that seem impossible to people at lower skill levels.

This component carries less weight because it’s only relevant at higher skill levels. But if you want to move from good to exceptional, workflow design is what gets you there.


## The Self-Test: Where Are You Actually?

Enough theory. Let’s figure out where you stand. I’ve designed three tests you can run right now—no scoring rubric needed. The quality of your approach will make your level self-evident.

*NOTE: This is the loose “get the concept” version of the test. If you want a more formal version of this test, [it’s right here](https://www.notion.so/product-templates/AI-Fluency-Assessment-Evidence-Based-Self-EvaluationPurpose-2835a2ccb52680b79918d10650657b33?source=copy_link) (and I discuss it more below).*


### Test 1: The Specificity Challenge

Take this vague request and turn it into a prompt that would produce excellent results:

**Starting prompt:** “Help me write a proposal for a potential client.”

Now stop and actually write your improved version before reading further.

Here’s how to interpret your result:

**If you added basic details** (who the client is, what you’re proposing, rough word count): You’re probably a **3-4**. You understand that specificity matters, but you’re still thinking about AI as a simple request-response tool.

**If you included persona, context, format, and constraints** (the client’s industry and pain points, your specific solution, the tone and structure you need, objections to address): You’re likely a **5-6**. You’re thinking architecturally about the prompt, not just adding information.

**If you designed a multi-step process** (first research the client’s industry and competitors, then draft key sections addressing specific criteria, finally refine for tone and format—with validation checkpoints built in): You’re operating at a **7-8** level. You’re not just prompting; you’re designing a workflow.

The gap between these approaches isn’t subtle. A Level 4 prompt might save you 30 minutes. A Level 7 workflow might save you four hours and produce better output than you could create manually.


### Test 2: The Iteration Test

This test reveals whether you know how to work *with* AI or just *use* AI.

Pick any piece of AI-generated content—something you’ve created recently or something you generate right now. Your task: improve it through prompting alone. No manual editing allowed.

**One revision attempt, general feedback** (”Make this better” or “This needs to be more professional”): **Level 3**. You’re treating AI like a black box and hoping for improvement.

**Multiple targeted revisions** (”Strengthen the opening hook,” “Add more specific examples in the second section,” “Reduce jargon and make this accessible to non-technical readers”): **Level 6**. You’re directing the improvement process with clear, specific guidance.

**Systematic multi-stage refinement** (”First, analyze this draft and identify the three weakest sections. Then rewrite each section using [specific framework]. Finally, check the entire piece for [specific criteria] and revise accordingly”): **Level 8**. You’re creating a structured improvement process with built-in quality checks.

Most people never progress past “make it better.” Advanced users treat iteration as a designed process with clear improvement criteria at each stage.


### Test 3: The Workflow Test

Describe your process for a common task you do regularly—writing reports, analyzing data, creating presentations, conducting research, whatever applies to your work.

Be honest. Write it down as if you’re explaining it to a colleague.

**“I ask ChatGPT/Claude”**: **Level 2-3**. You’re using AI, but you don’t have a repeatable process.

**“I use this specific prompt pattern”** (and you can articulate what that pattern is): **Level 5-6**. You’ve identified what works and you’re consistent about using it.

**“I have a documented workflow across multiple tools”** (you can describe specific steps, tool handoffs, and quality checks): **Level 7-8**. You’ve systematized your approach to the point where someone else could replicate your results.

The workflow test is the most revealing because it shows whether you’ve moved from experimentation to systematization. If you can’t describe a repeatable process for your most common tasks, you’re leaving enormous value on the table.


## [Your Personal Assessment Prompt (here)](https://www.notion.so/product-templates/AI-Fluency-Assessment-Evidence-Based-Self-EvaluationPurpose-2835a2ccb52680b79918d10650657b33?source=copy_link)

Now that you’ve completed the three tests, let’s score!

To keep you honest I’ve placed the prompt to help you score plus a detailed explanation of each scoring stage [at this link](https://www.notion.so/product-templates/AI-Fluency-Assessment-Detailed-Grading-Rubric-2835a2ccb52680b79918d10650657b33?source=copy_link).

Calculating your score will help give you clarity on where you actually stand and what to work on next. The AI will spot patterns in your approach that you might miss, so be honest!


## What Your Score Actually Means

Now that you’ve tested yourself, let’s talk about what each level means for your productivity and career. But remember: these interpretations assume average fluency velocity. If you’re learning fast, you’ll move through these levels quickly. If you’re stuck, even a decent score becomes a problem.


### 1-3: You’re in the Danger Zone

You’re using AI, but inefficiently. You might be spending more time fighting with it than it saves you. The frustration you feel is real—AI probably feels unreliable and unpredictable because you haven’t learned the patterns that make it consistent.

The gap between where you are and where a Level 6 user operates is substantial. We’re talking about 10-15 hours per week in productive output, plus the qualitative difference in what you can accomplish. But here’s the good news: the climb from 3 to 5 is very achievable. It’s mostly about building mental models and understanding how LLMs actually work.

**Velocity check:** If you’re at this level, your velocity is everything. Can you move to Level 5 in the next 8-12 weeks? If yes, you’re fine. If you’ve been stuck at 2-3 for six months, that’s a serious problem.


### 4-5: You’re Competent But Plateaued

You get genuine value from AI. You probably use it daily and can point to specific time savings. But you’ve hit a wall, and you might not realize it.

The problem at this level is that you don’t know what you don’t know. You need new patterns, not just more practice. The jump from 5 to 7 requires professionalizing your practice—building auditable patterns, measuring prompt yield, creating processes teammates can use.

Most people plateau here because they assume competence means they’ve learned everything important. They haven’t. The productivity gap between a 5 and a 7 is almost as large as the gap between a 2 and a 5.

**Velocity check:** How long have you been at this level? If you’ve been a solid 5 for six months, you’re coasting when you should be climbing. The baseline is shifting—by mid-2025, a 5 will be table stakes, not competitive advantage.


### 6-7: You’re Building Competitive Advantage

You’re systematically faster than your peers. You have repeatable processes for common tasks. You’re starting to see the strategic possibilities—not just “AI can help me write faster” but “AI can let me tackle projects I couldn’t attempt before.”

At this level, you’re probably teaching others or at least fielding questions from colleagues about how you’re so productive. The challenge now is systematization. Can you document your workflows well enough that others could replicate them? Can you design processes that work reliably across different contexts?

**Velocity check:** If you’re still climbing (moving from 6 toward 8), you’re in great shape. If you’ve been at 6-7 for a year, you’ve stopped learning. That’s dangerous because the people behind you are catching up while AI capabilities race ahead.


### 8-10: You’re Creating Leverage

You’re using AI to accomplish work that seems impossible to others. You’re not just writing better prompts—you’re designing systems. You might be building custom GPTs, creating prompt libraries, or developing workflows that combine multiple tools in sophisticated ways.

At Level 9-10, you’re probably teaching others systematically, building tools, or solving novel problems that define new use cases. This level is less about personal productivity and more about multiplying impact through systems and teaching.

**Velocity check:** At this level, velocity means staying current with rapidly evolving capabilities and finding novel applications. If you’re teaching and innovating, you’re maintaining velocity. If you’re just executing established patterns, you’re starting to plateau.


## The Second Dimension: Fluency Velocity

Before we talk about improvement paths, you need to understand something critical: your current score is less important than your learning rate.

A person who’s a 4 out of 10 but gains a level every two months is in a fundamentally better position than someone who’s been stuck at 6 for a year. AI capabilities are advancing so rapidly that standing still means falling behind. Your fluency velocity—how quickly you can pick up new concepts and integrate new techniques—matters more than your snapshot score.

Here’s how to assess your velocity:

**Can you learn new AI concepts using AI itself?** When a new technique or tool emerges, do you use Claude or ChatGPT to understand it, experiment with it, and integrate it into your workflows? This meta-skill—using AI to accelerate your AI learning—is the clearest predictor of future competence.

**How long does it take you to master a new prompting pattern?** When you encounter a new framework or technique, can you internalize it and apply it reliably within days? Or does it take weeks of inconsistent practice?

**Do you have a systematic approach to learning?** When you discover a better way to do something, do you document it, practice it deliberately, and replace your old method? Or do you keep doing things the way you’ve always done them?

Your fluency velocity determines your trajectory. Someone with high velocity at Level 4 will blow past someone with low velocity at Level 6 within months. And as AI capabilities accelerate, velocity matters more each quarter.

Now let’s talk about how to move up—and more importantly, how to accelerate your learning rate while you do it.


## How to Actually Improve (yes there’s a prompt)

I built a whole prompt to help with this, plus I have some specific pointers per level below to give you a general idea of how to improve.

The prompt below is designed to do what most people can’t do for themselves: build a personalized 90-day development plan. But it only works if you’re honest about your actual performance, not your aspirational version.


#### *[Here’s your custom AI development plan prompt](https://www.notion.so/product-templates/AI-Development-Prompt-2845a2ccb526807697e8dea376ae90db?source=copy_link)*

And for extra color below, I’m including a set of instructor notes on what it looks like to level up at each stage, so you get a sense of what’s in the box.


### From 3 to 5: Build Mental Models

The jump from 3 to 5 isn’t about memorizing frameworks. It’s about understanding how LLMs actually work so you can reason about what will work and what won’t.

**Understand inference and prediction.** LLMs don’t “know” things—they predict the most likely next token based on patterns in training data. This changes everything about how you prompt. When you ask “What is the capital of France?” you’re not querying a database. You’re triggering a pattern completion. When you understand this, you start thinking about prompts differently: you’re setting up contexts that make the desired output the most likely completion.

**Grasp context windows and retrieval.** Most people think AI “remembers” your conversation. It doesn’t. Every response is generated from a fixed context window—the previous messages in your conversation up to a token limit. When you understand this, you stop assuming AI knows what you talked about twenty messages ago. You start explicitly referencing important context. You understand why breaking complex tasks into stages with clear context handoffs produces better results than one giant prompt.

**Learn to work backwards from outcomes.** Stop starting with “what should I tell the AI?” Start with “what output do I need?” Then work backwards: what would make that output the most likely completion? What context, examples, constraints, and format specifications would steer the model toward that outcome? This is outcome-oriented prompting, and it’s the difference between random success and predictable results.

**Understand search versus generation.** When should AI search your documents (RAG, retrieval augmented generation) versus generate from its training? When do you need current information that requires real-time web search versus knowledge the model already has? Understanding this distinction means you stop asking Claude to “search the web” and start using tools designed for retrieval when that’s what you need.

These aren’t frameworks to memorize. They’re mental models that change how you think about prompting. Once you understand how LLMs generate text, you can reason about what will work instead of guessing.

**The meta-skill: Use AI to learn this.** Ask Claude to explain inference with examples. Have it show you how context windows affect output. Request demonstrations of outcome-oriented prompting. The fastest way to build these mental models is to use AI to teach you about AI—which also accelerates your fluency velocity.


### From 5 to 7: Professionalize Your Practice

You understand how AI works. Now you need to work like a professional, not an enthusiast. The gap between 5 and 7 is systematization.

**Think in auditable patterns.** A professional process is one you can explain, document, and repeat. You need to move from “I usually do something like this” to “Here is the exact sequence I follow.” This means: documented prompts, clear stages, explicit validation criteria, predictable outcomes. Build processes you could hand to a colleague and have them replicate your results.

**Measure prompt yield.** Prompt yield is quality output per unit of prompting effort. A low-yield approach might take 10 iterations and 30 minutes to get usable output. A high-yield approach gets you 90% of the way there in 2 prompts and 5 minutes. Start tracking this. Which prompts consistently produce good first-draft output? Which require extensive iteration? Optimize for yield by studying what works and eliminating what doesn’t.

**Build for your teammates, not just yourself.** The shift from 5 to 7 is moving from personal productivity to team capability. Can you create prompts others can use? Can you document workflows a colleague could follow? When you think about uplifting teammates, you’re forced to make your implicit knowledge explicit. You build better systems because you have to explain them.

**Create feedback loops.** Professional practice requires measurement. How do you know if your prompt improvements are working? How do you validate that your documented workflow produces consistent results? Build quality checks into every stage. Test edge cases. Document failure modes and how to handle them. This is how you move from “it usually works” to “it reliably works.”

**Develop prompt libraries with versioning.** Stop recreating prompts from memory. Build a library of tested, proven prompts for common tasks. Version them as you improve them. Document what each version changed and why. Track success rates. This is professional craft—you’re building tools you can rely on.

The jump from 5 to 7 is about treating AI work like professional work: documented, auditable, measurable, improvable, and transferable to others.


### From 7 to 9: Teach and Innovate

At this level, improvement comes from two activities: teaching others and solving novel problems.

**Teaching forces clarity and reveals gaps.** When you document a workflow for someone else to use, you discover all the implicit knowledge you’ve been relying on. When you train a teammate, their questions expose holes in your process. Teaching makes you better because it forces you to articulate what you know and refine what doesn’t quite work.

**Design systems for scale.** Move beyond personal workflows to team systems. Build custom GPTs or Claude Projects with instructions others can use. Create prompt libraries your organization can adopt. Develop quality standards and evaluation criteria that work across people and contexts. This level is about leverage through systems.

**Solve problems that seem impossible.** Take on tasks where the obvious AI approach doesn’t work. This forces innovation—combining techniques in new ways, orchestrating multi-tool workflows, developing novel applications. You learn more from one hard problem than from ten routine tasks.

**Use AI to accelerate others’ learning.** Design onboarding processes that use AI to teach AI skills. Create self-service resources that let people level up without you. Your impact shifts from personal productivity to organizational capability.

The move from 7 to 9 isn’t about frameworks. It’s about developing deep expertise through teaching, building scalable systems, and pushing boundaries.


#### *[Coaching Students or Implementing as HR? Grab these notes](https://www.notion.so/product-templates/AI-Fluency-Notes-2855a2ccb526807aa831cec483a67440?source=copy_link)*

These implementation notes dive into how the prompts were put together as a pair, what’s being measured, and the principles I used to construct the assessment. You can backwards engineer here to modify or tweak the assessment for what you need.


## The Competitive Reality

Here’s what matters most: The baseline is shifting, and it’s shifting fast. Being a 5 out of 10 at AI fluency won’t be enough by late 2025 or early 2026. As these skills become table stakes in knowledge work, the competitive threshold will rise to 6 or 7.

But here’s the more important insight: your current score matters less than your fluency velocity. A person at Level 4 who’s climbing fast will surpass a stagnant Level 6 within months. And with AI capabilities advancing this rapidly, standing still means falling behind.

The people who win aren’t necessarily the ones with the highest scores today. They’re the ones who are learning fastest—who can integrate new techniques within days, who use AI to accelerate their AI learning, who systematically improve their workflows instead of repeating the same patterns.

They’re developing mental models that let them reason about new capabilities. They’re building auditable processes that compound. They’re teaching others, which forces clarity and reveals gaps. They’re solving novel problems that push boundaries.

This isn’t about staying ahead of AI. It’s about staying ahead of other people who are learning to use AI effectively. And those people are moving faster than you think.

You now know where you stand, what separates each level, and how to assess whether you’re climbing or coasting. You have the mental models for building understanding, the frameworks for professionalizing your practice, and the path to systematic improvement.

The only questions that matter now are:

How quickly can you move up? And will you start before the baseline shifts past you?

Because it will shift. The only variable is whether you’re driving that shift or getting left behind by it.

[![](https://substackcdn.com/image/fetch/$s_!Ilom!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf07f69-c8c7-4782-a0e6-1d6e689fe1d5_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!Ilom!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf07f69-c8c7-4782-a0e6-1d6e689fe1d5_1024x1024.png)
