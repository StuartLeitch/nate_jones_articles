---
title: "Why Ten People With AI Fluency Beat Five Hundred With AI Training"
author: "Nate Jones"
published: 2025-10-19
url: https://natesnewsletter.substack.com/p/executive-briefing-ai-usage-is-not
audience: everyone
scraped_at: 2026-01-05 19:13:56
---

AI use is not the same as AI fluency, and that fact keeps mattering more and more.

I’ve spent about 2,000 hours watching people use AI over the last year.

Here’s what I can tell you: just using AI is not the bar.

AI fluency is 10x more valuable than AI usage. AI fluent teams are running absolute circles around teams that have taken the corporate training on ChatGPT (or CoPilot).

You’re using AI. Maybe your whole company is. Dashboards show adoption climbing. People report time savings.

And yet you might not be getting any actual value from it.

The difference between people who merely use AI and people with AI fluency is the difference between 30 percent productivity gains and 300 percent productivity gains. Between incremental improvement and explosive capability expansion.

Why is that? I’ve spent the last six months trying to understand why some people and teams get explosive results while others get theater. Why 10 people with AI fluency can outperform 500 people with AI training. It’s not just that small startups win! I’ve seen teams at big companies level up massively too.

What I’ve found across teams big and small is surprisingly consistent. After sitting with my notes and findings over the last few months, I’ve come to the conclusion that the real drivers of AI fluency are hidden.

You can’t teach true fluency by teaching a tool, like ChatGPT.

You have to dig deeper, and very few people do.

After looking through my notes across dozens of team engagements, I’ve derived three root principles that determine org-level AI fluency:

1. Constraints over process
2. AI-shaped problem solving skills first (there are five pieces to this!)
3. No infrastructure, for awhile

None of these are about prompting techniques or tool training.

Together, they’re about building judgment that compounds.

Whether you’re an IC trying to 10x your own output, a manager trying to help your team get better, or a leader trying to figure out why AI isn’t showing up in the business metrics—the question is the same.

Are you building capability that lets you tackle harder problems, or are you just getting faster at the easy ones?

This piece walks through the three principles that create AI fluency, breaks down the five transferable problem-solving skills I’ve observed in high-performing people and teams, and gives you three specific questions to assess where you stand.

And yes, there’s a prompt to help you make sense of it all for your unique situation.

My goal is to help you figure out whether you’re building capability that compounds or dependency that will be obsolete in months, and then to give you the tools to take action so you (and your team) can level up.

Let’s get into it.

> ***This is an Executive Circle briefing**, a Sunday newsletter exclusively for Founding Tier Members. You can learn more via this [60 second video](https://youtu.be/KC3GkEnHR-8) explaining what’s in each tier, and you can change your plan [here](https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack). Enjoy, and back to regular programming Monday!*


## [Grab the prompt to analyze this article](https://www.notion.so/product-templates/Executive-Briefing-AI-Fluency-Prompt-2915a2ccb52680a68056eeb134441c60?source=copy_link)

This prompt jumps you into a diagnostic conversation about AI fluency. It helps you figure out if your organization has activity or fluency, and works in a conversational way to pin down what’s really going down at work and what steps you need to take to cut the AI theater.

The goal is simple: help you combine your unique context with these principles to figure out how to build explosively productive AI teams.

---


# Why Ten People With AI Fluency Beat Five Hundred With AI Training

I’ve spent more than 2,000 hours watching teams use AI. High performers and low performers, both equipped with the same tools. The gap between them isn’t 2x or 5x. It’s 10x. The difference between teams that merely use AI and teams with AI fluency is the difference between 30 percent productivity gains and 300 percent productivity gains. Between incremental improvement and explosive capability expansion.

Here’s what I’ve observed. You can have 500 people trained on AI tools, using them daily, reporting time savings on surveys, and still not capture a fraction of the value that 10 people with true AI fluency deliver. The nonlinear unlock you get from small teams that operate with fluency is enormous. And most organizations don’t understand why.


## What’s At Stake

Let me be direct about what’s at stake. The gap between organizations with AI fluency and organizations with AI activity is going to widen dramatically over the next 12 months.

The activity organizations will report impressive adoption numbers. They’ll show surveys where 80 percent of employees use AI weekly. They’ll demonstrate time savings on routine tasks. They’ll have hundreds or thousands of custom GPTs or Skills or whatever the next thing is. And they’ll wonder why it’s not showing up in their business metrics. Why revenue per employee isn’t climbing. Why they’re not shipping faster. Why competitors are outmaneuvering them.

The fluency organizations will have smaller adoption numbers but explosive results. They’ll have pockets of teams—maybe 5 percent of the organization—delivering outcomes that seem impossible. Launching products in weeks instead of quarters. Serving customers at scale with teams that are a fraction of the size that used to be necessary. Making strategic decisions faster because they can analyze options that used to be too complex.

The difference isn’t the AI tools. Both organizations have access to the same technology. The difference is that one organization is building people who can solve increasingly complex problems with AI, and the other is building people who are dependent on increasingly complex AI systems.

And here’s what keeps me up at night. The activity path feels productive. It feels like progress. Teams are busy. Things are happening. Dashboards show usage. But you’re not building the flywheel that compounds. You’re building dependency on specific tools and workflows that will be obsolete in months.

The trigger for me this week was Skills—Claude’s new release that lets you package instructions and workflows into reusable modules. Skills has all the hallmarks of brilliant technology that people who don’t understand it can turn into complete spaghetti code in your organization. Fast forward three months and you have 5,000 Skills for a team of 300 people. No one’s maintaining them. No one can track where they all are. Complete mess. We’ve seen this pattern before with custom GPTs, with Zapier integrations, with N8N agents. Excitement bursts onto the scene, teams build a bunch of stuff, and six months later the team is busy but you never see value come through anywhere.

If you’ve rolled out AI and it’s been enthusiastically received by at least a corner of the company, your first question should not be about talent and it should not be about tools. You should be asking yourself about fluency.


## Three Principles That Create AI Fluency


### Enabling Constraints Rather Than Processes

The goal is not to control what people build. It’s to set structure and boundaries that make good work with AI feel natural, and that make bad work with AI feel hard. We’re setting incentives that raise the floor.

What does that look like? Every Skill you build includes a test case. Skills have a named maintainer. AI cannot access regulated data outside our virtual sandbox—it’s not even possible. These constraints are building blocks that enable creativity within safe boundaries. You can do whatever you want inside the sandbox.

Claire Vo, a three-time CPO now building AI tools at ChatPRD, describes what breaks when you over-process this. AI cracks traditional lanes—PMs prototyping code, engineers vibing features without specs. But clinging to “stay in your lane” codified manuals creates anxiety and slows velocity. Her fix? Cultural shifts around cheap experiments with constraints like vibe-coded demos married to production code, turning constraints into organic quality boosters rather than bureaucratic gates.

The opposite kills value. AI Skills require IT approval—that’s gatekeeping. Submit your Skills to the review board—that’s bureaucracy. Use only approved patterns for prompts—that’s killing creativity. These process constraints lower the ceiling. They make it hard for teams to excel.

Enabling constraints raise the floor. Process lowers the ceiling. The next time you think about adding a constraint, ask yourself which one you’re building.


### Learn Problem-Solving Skills That Are AI-Fungible

This is where the 300 percent teams separate from the 30 percent teams. The people getting 10x better aren’t learning tools or memorizing prompts. They’re learning judgment that transfers across AI systems. And that’s not easy to learn on the open web because most videos are made for clicks and entertainment, not teaching hard skills like problem decomposition.

Let me break down the five specific skills I’ve observed in high-performing teams. These are the skills that compound. These are the skills that let a team of 10 outperform a team of 500.

**1. How to decompose complex problems into AI-sized pieces.** This is the foundational skill. When you give someone a complex business problem—optimize our customer onboarding flow—most people either hand the entire problem to AI and get a generic answer, or they solve it themselves without AI. Neither works well.

The fluent AI user breaks the problem into pieces where AI excels and pieces where human judgment matters. Ask AI to analyze patterns in onboarding data and identify drop-off points. Apply human judgment about which patterns align with business constraints. Back to AI to draft solutions for promising approaches. Human judgment on feasibility and trade-offs. AI to build the implementation plan.

This back-and-forth, this ability to chunk work into the right size pieces and hand them to the right solver—that’s the skill. You develop an intuition for what an AI-sized piece of work looks like. Too big and you get generic garbage. Too small and you’re micromanaging. Just right and you get solutions you couldn’t have reached alone.

**2. When to iterate on a solution versus when to start over.** Most people don’t develop good judgment about this. They’ll spend hours refining mediocre AI output when starting fresh would get better results in minutes. Or they’ll abandon promising work too early because the first draft wasn’t perfect.

High performers can look at an AI output and assess whether the foundation is sound. Is the core approach right but execution needs refinement? Iterate. Is the fundamental framing wrong? Start over. Did AI misunderstand the constraints? Start over with better context. Is it 80 percent there and just needs polish? Iterate.

This judgment comes from pattern recognition. You have to see enough AI outputs to know what good foundations look like versus irredeemable ones. I’ve watched teams waste entire afternoons trying to fix AI-generated code that had the wrong architecture from the start. I’ve also watched high performers recognize that in 30 seconds, start fresh with better framing, and ship working solutions before lunch. The time difference is 10x. The quality difference is even larger.

**3. How to recognize when AI is confidently wrong.** AI will confidently tell you things that are completely wrong. It will make up research papers, cite nonexistent APIs, generate code that looks perfect and doesn’t work. And it will do all of this with the same confident tone it uses when it’s right.

High performers develop a sixth sense for AI wrongness. They know the patterns. When AI gives extremely specific numbers without sources. When it describes a process that sounds too clean, too perfect. When technical explanations skip over the hard parts. When it gives you exactly what you asked for without noting any trade-offs or edge cases.

This isn’t about being skeptical of everything AI produces. That would be paralyzingly slow. It’s about developing pattern recognition for the specific ways AI goes wrong, then building habits around verification for high-stakes outputs. The teams getting 300 percent gains have built this into their culture. They’ve developed judgment about what needs checking and what doesn’t.

**4. What kinds of context actually matter to include.** Most people either give AI no context and get generic results, or they dump everything into the prompt and overwhelm the model or waste tokens on irrelevant details.

High performers develop a sense for what context actually matters. For a coding task, is it more important to include the full codebase structure or a focused example of the pattern you want? For a writing task, does AI need your brand guidelines or just three examples of your voice? For an analysis task, is the full dataset necessary or can you provide summary statistics and a representative sample?

This skill comes from understanding how AI actually uses context. It doesn’t reason the way humans do. It pattern-matches. So the question isn’t what context would help a human understand this problem. The question is what context will help the AI recognize the right patterns.

Rohan Paul’s MIT research shows this clearly. Theory-grounded prompts—like applying behavioral economics frameworks to social agents—generalize 3.41 times better across different datasets than task-specific prompts. The transferable skill is learning what predicts performance across contexts, not memorizing what worked once.

**5. When to use AI versus when to just do it yourself.** This might be the most important skill and it’s almost never discussed. The best AI users know when not to use AI.

There’s a class of task where the overhead of explaining what you want to AI, checking the output, and fixing the inevitable mistakes takes longer than just doing it yourself. High performers recognize these tasks instantly and don’t waste time on AI.

There’s another class where AI is tempting—it will give you an answer—but human judgment is critical and AI will lead you astray. Strategic decisions, nuanced interpersonal situations, trade-offs that require deep business context. High performers don’t outsource these to AI even though they could.

And there’s a class where AI is transformative. Where it can do in minutes what would take you hours or days. Where it can explore solution spaces you wouldn’t have considered. Where it can handle tedious work while you focus on judgment. High performers route these tasks to AI immediately.

The difference between 30 percent gains and 300 percent gains often comes down to this routing judgment. Gergely Orosz interviewed seven AI engineers for his Pragmatic Engineer series. The pattern he found? Fungible wins come from decomposition. Chunking vague requests like optimize system into AI-assisted log analysis plus human root-cause determination. One engineer’s test: replicate results in Claude versus GPT. Judgment transfers. Workflows don’t.

**Why these five skills create the 300 percent difference.** The teams getting incremental 30 percent gains are using AI as a better search engine or a faster autocomplete. They’re saving time on individual tasks. That’s valuable but it’s not transformative.

The teams getting 300 percent gains are using AI to tackle fundamentally harder problems than they could before. They’re not just doing the same work faster. They’re doing work that was previously impossible or impractical. They’re solving problems that would have required hiring three specialists by coordinating AI across domains. They’re building products in weeks that would have taken quarters. They’re analyzing patterns in data that no human could spot.

That capability expansion comes from these five skills. Decomposition lets them break impossible problems into solvable pieces. Iteration judgment keeps them from getting stuck. Recognition of AI wrongness keeps them from catastrophic mistakes. Context awareness makes their AI interactions efficient. Routing judgment keeps them focused on high-value work. And these skills compound. The flywheel spins faster.

Can your people get similar results in a different AI tool? If not, they learned a workflow, not a capability. Can someone tell you when I decompose my problems, I find that this particular context window works better for this type of problem, and here is why? If you have 10 or 20 people in an organization of 500 that have that skill set, you will probably make more of a difference to your business than if you have all 500 trained on ChatGPT.


### Don’t Over-Infrastructure Until Workflows Break

There’s a real pattern right now. Teams see the power of AI and immediately start building infrastructure to contain it. I’ve seen teams with barely any users building custom harnesses for agent orchestration, complicated RAG systems for knowledge management of a dirty Wiki, elaborate frameworks for prompt management when nobody’s prompting at work. Oftentimes this is premature.

Peter Steinberger is building a 300,000-line TypeScript app solo, running 3-8 AI coding agents in parallel. His take on premature infrastructure? Don’t waste your time on stuff like RAG, subagents, or other things that are mostly just charade. Just talk to it. Play with it. Develop intuition. He ditched elaborate harnesses because GPT-5 searches codebases natively—the RAG system he almost built would already be obsolete.

McKinsey studied 50-plus agent deployments and found the pattern. Teams overbuilt for low-variance tasks, adding 30 to 50 percent redundancy with multi-agent systems where simple workflows worked fine. Their lesson? Deploy modular components only after breakage.

Start simple. Add infrastructure when the simple approach breaks. If you’re thinking about build versus buy and vendor solutions, ask yourself this. Are your people in a place where their workflows are breaking despite current use of AI and they need this tool to unbreak the workflow? There are absolutely cases like that. But don’t start by building infrastructure. Start by building value, and then build the infra when workflows actually break.

The instinct to complicate as soon as you see something like AI is strong. I’ve sometimes wondered if it’s because it gives teams an illusion of certainty. Regardless of the reason, just start simple.


## How High-Performing Teams Pull It All Together

These three principles work together in a way that compounds. Enabling constraints let people experiment safely without permission. That means people can actually get better as they work within structures that push them toward healthy work habits. The constraints create the safety to develop the five problem-solving skills because people aren’t afraid of breaking things or creating compliance issues. They can iterate rapidly and learn from failures.

AI-fungible skills mean people get better, not just busier. Every hour spent building these transferable skills pays dividends across every future AI interaction. Every hour spent memorizing tool-specific workflows becomes obsolete the next time the tool updates. High-performing teams optimize for building capability that compounds.

Minimal infrastructure keeps the focus on developing judgment and not managing systems. If you start simple, you’re encouraging people to spend more time learning the art of problem-solving, learning the art of tackling increasingly complex problems with AI. You’re not asking them to learn your internal AI orchestration platform. You’re asking them to solve real problems and build judgment from that experience.

If you look at these teams that are getting 200, 300x improvements and you ask yourself where is this massive multiple coming from, it is coming from the ability to tackle much harder problems with AI. Order of magnitude 10x harder problems than people who don’t know these skills can tackle. That is a new class of problem-solving understanding. It is a big deal and we are not teaching it well today. Most organizations are outsourcing this to vendors who have every incentive to keep you dependent on their specific tools rather than building transferable capability.

The result you want is people who can solve increasingly complex problems with AI, not people who are dependent on increasingly complex AI systems. That distinction determines whether you’re in the 30 percent bucket or the 300 percent bucket.


## What This Means For Skills Specifically

Skills will proliferate. Your choice isn’t whether to control that—it’s whether to create conditions for people to get better.

Create enabling constraints. Skills need maintainers. Skills that access sensitive data get security review. Skills include documentation. Humans vet AI suggestions for high-stakes decisions.

Develop AI-fungible judgment. When to create a new Skill versus use existing ones. How to iterate on Skills that aren’t quite working. When a Skill is the wrong approach entirely. How to decompose problems that Skills can actually handle.

Stay minimal. Don’t build elaborate Skills marketplace immediately. Don’t create Skill review boards. Don’t require certification to create Skills.

Add infrastructure only when you need it. When people can’t find existing Skills, build search. When duplicate Skills proliferate, add recommendation. When quality varies wildly, add peer review. When costs spike from inefficiency, add routing.


## The Three Questions

This week, if you’re a team leader, if you’re a director, if you’re an executive, anyone who has people responsibilities, there are three questions I think would be productive for you to engage with your team on.

First, what are our enabling constraints? What boundaries here at work make good AI work feel natural? And if the answer is we don’t know what good AI work feels like, there’s your answer. That’s where you need to dig in and start to figure out what good AI workflows look like. Someone on your team has a good example. Go find them.

Second, how do we develop good judgment in problem-solving? Are we just training to the tools or are we truly building the capability to problem-solve with AI? Are we multiplying our problem-solving muscles because people understand how to structure pieces of work in ways that AI can use and assist them on? It’s learning the skill of working side by side with a robot, of passing the work over, letting the robot take a turn and then coming back. That is a new skill versus passing it to a human. People have to learn the difference. Ask your team, how are we doing at developing judgment? Ask your learning and development team if you have one. Are we just training to the tools? The vendors will encourage that. They want you to train to the tools and buy more tools. Or are we training to the skill, to the capability?

Third, where are we over-building? What infrastructure have we been tempted to add on before we know we need it? Is it a vanity project? Is it something for the board? Seriously, what infrastructure don’t we really need for AI? Can’t we just focus on building the value and find out what breaks?

You will have things break and you can add necessary complexity and infrastructure at that point, but then you’re not wasting your effort.

This is especially important in an era when we’re going to get release after release that feels a lot like Skills from Claude—democratizing, empowering, everyone loves it, soon it will proliferate across your business. It may not deliver actual value. This piece is all about how we move from activity to true fluency and multiplicative value for teams and for the business.

---

[![](https://substackcdn.com/image/fetch/$s_!u1qM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e467858-1016-4826-aed3-bbd0cd7df503_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!u1qM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5e467858-1016-4826-aed3-bbd0cd7df503_1024x1024.png)
