---
title: "Claude Sonnet 4.5 Pulls You Closer to Your Work"
author: "Nate Jones"
published: 2025-09-29
url: https://natesnewsletter.substack.com/p/claudes-new-model-just-launchedi
audience: everyone
scraped_at: 2026-01-05 19:16:54
---

I’ve been lucky enough to try the new Claude model (Sonnet 4.5) for a few days now.

After running it through its paces, I am beginning to think everyone’s getting the AI story backwards.

Here’s why: this model is the first model I’ve seen that invites humans to collaboate MORE, not LESS. Let me explain how:

Instead of going real fast and producing a mix of insights and AI slop, this model thinks carefully, checks its work, and produces clear outputs that I find it really easy to dig into and offer perspective on. This model invites expertise from people!

And that feels important.

It’s the difference between yelling at the model to slow down and do something else and working with a colleague.

I know the dominant narrative says AI wins by automating us out of the loop. Every demo, every breathless LinkedIn post, every VC deck—they’re all selling the same fantasy: AI that does the work so you don’t have to. But after three days testing this model against everything else out there, I am even more convinced that narrative is wrong.

The breakthrough isn’t AI that works without you—all of them are trying to do that these days. It’s AI that produces clarity instead of slop.

Here’s an example: I fed Claude Sonnet 4.5 66 pages of unstructured customer quotes—the kind of mess that used to take me days to synthesize at Amazon. What came back wasn’t just organized. It was clear. Clear enough that I could immediately see where my domain knowledge mattered. Clear enough that I knew exactly which insights to push on, which narratives to reshape, which recommendations aligned with strategy only I knew.

That’s the shift no one’s talking about. When AI produces clarity instead of slop, something counterintuitive happens: you get pulled closer to your work, not further from it. You’re not spending hours cleaning up garbage outputs or decoding what the model meant. You’re spending time on the decisions that actually require your expertise—the judgment calls about strategic emphasis, audience framing, competitive positioning.

During my early access, I watched this model obsessively check its own work. It validated spreadsheet formulas, measured pixel overlap in presentations, verified that code actually runs before claiming it works. Not because it’s trying to be autonomous, but because it’s trying to give you outputs clear enough that you can focus on what matters instead of fixing basic errors.

This isn’t the AI that Silicon Valley is selling you. After a few days of testing, my first impression is clear: this model isn’t trying to prove you’re unnecessary. It’s trying to make your expertise more visible, more applicable, more valuable. And that changes everything about how we should think about AI and meaningful work.

Read on to dig into how I work with the model and why I think it’s the first model that feels like a mature AI colleague—easier to steer than ChatGPT-5, thoughtful, clarifying, appropriately opinionated, and even easier to prompt!

Grab the full details on my testing below, plus sample work by Sonnet 4.5, plus my notes for implementers—everything you need to leverage it in your workflows across docs, decks, sheets, and code!

This is a really fun model, and it’s the first one in awhile where the transition has felt like a sigh of relief. We deserve that don’t we?! Enjoy…


#### *First, see it in action—[grab some sample model outputs](https://drive.google.com/drive/folders/1s19hPIeW-v-uzElMAys3w-oPeSFt_sjs?usp=sharing)*

I put the model through its paces, and I’m including some of the sample outputs to give you a sense of how the model handles various artifacts. Look at this as a quick tour through what this model can build with fairly minimal input. With a little bit of nudging I think we’ll be able to get even higher quality work out of the new Claude. Exciting stuff!

---


# Claude Sonnet 4.5 Pulls You Closer to Your Work

I spent the last few days testing Sonnet 4.5, which released today. I fed it 66 pages of unstructured customer quotes, asked it to build spreadsheets with working formulas, had it create executive presentations, and watched it validate running services before claiming they worked. What I learned surprised me. This isn’t a story about AI doing more work for you. It’s about AI that makes it easier to see where you—with your expertise and judgment—need to touch the work to make it right.

The most valuable AI isn’t the one that automates you out of the loop. It’s the one that helps you see clearly when your domain knowledge matters most.


## Why Clarity Matters

When you work with AI models, you’re usually dealing with one of two problems. Either the output is so messy you spend hours cleaning it up, or it’s so confident and polished that you can’t tell where it might be wrong. Neither helps you do better work. The first wastes your time. The second hides problems until they matter.

Sonnet 4.5 does something different. It checks its work obsessively before showing you anything. I watched it create a PowerPoint slide, measure the pixel overlap between a title and visual element, recognize the problem, and fix it without asking me. I saw it build spreadsheets and validate every formula. When I asked it to demonstrate a Next.js project, it started the dev server and confirmed it could run before telling me it worked.

That verification creates clarity. Not perfection—nothing is perfect—but clarity about what’s there and what works. When outputs are clear, you can immediately see where your expertise needs to intervene. You’re not decoding what the model meant. You’re not fixing basic mistakes. You’re applying the judgment that only you can provide because you know your domain, your audience, and your goals.

The model pulls you closer to the meaningful parts of your work instead of burying you in cleanup.


## In Practice…

The Voice of Customer analysis shows this pattern clearly. I gave the model 66 pages of raw customer testimonials—unstructured quotes with no organization. When I worked in Voice of Customer at Amazon, this was the hardest part of the job. Not reading feedback, but extracting signal from noise. Customer quotes blur together. It takes real cognitive effort to spot themes and map individual utterances to larger insights.

What came back was a complete narrative: executive summary identifying the core value proposition, audience segmentation with percentages, value drivers ranked by frequency, emotional impact assessment, competitive differentiation, and strategic recommendations. The model didn’t just summarize quotes. It found patterns, quantified them, and built a decision-ready document.

But here’s what matters: the narrative was clear enough that I could immediately evaluate whether it was right. I could see the reasoning. I could trace how conclusions followed from evidence. That meant I could apply my judgment about whether the patterns the model found were the ones that mattered strategically.

The model did the exhausting synthesis work (a real step forward for the Sonnet lineage, for those watching closely). Meanwhile, I did the work only a human with context can do—deciding whether the story was the right story to tell.


## Human Judgement

The AI Strategy brief demonstrates the same pattern. Sonnet 4.5 produced strategic intent, five key initiatives with metrics, investment breakdown with ROI, risk assessment with mitigation strategies, and a go/no-go recommendation with supporting criteria. It committed to a point of view and backed it with reasoning.

Whether that recommendation is right depends entirely on context the model doesn’t have. Your company’s risk tolerance. Your competitive position. Your organizational capability to execute. The brief can’t make those calls. But it can give you a clear structure that lets you apply your judgment efficiently.

That’s the shift. Previous models gave you productivity—they helped you get work done faster. This model gives you decisioning capability—it sets you up to make better choices about work that matters. You spend less time on structure and more time on the judgment calls that actually require your expertise.

When I say this model pulls you closer to your work, this is what I mean. Your expertise becomes more visible, more applicable, more valuable because you’re not drowning in the mechanics of synthesis and presentation. You’re focused on the decisions that only you can make.


## Iteration to Win

Clear outputs let you iterate in ways messy outputs don’t. When a presentation takes five minutes to generate and comes back 90% ready, you can run it three times with different framings. You can test alternative structures. You can refine the narrative until it says what you actually mean.

That’s how expertise compounds. You’re not stuck with the first draft because revising means starting over. You can try approaches, evaluate them quickly with your domain knowledge, and iterate toward the right answer. The model does the synthesis work fast enough that your time goes into thinking about what you want to say, not into the mechanics of saying it.

The examples bear this out. The Voice of Customer analysis needed human judgment on strategic priorities. Which insights matter most? Which audience segments deserve focus? What recommendations align with company direction? The AI Strategy brief needed context-specific risk assessment. How risk-averse is leadership? What’s the organization’s track record on technology adoption? What political realities affect implementation?

Those are judgment calls. The model can’t make them because it doesn’t have your context. But it can give you clear enough outputs that you can focus your time on exactly those questions instead of on building the structure that lets you ask them.


## Why This Matters for Professional Work

Most professional work happens in established formats with existing decision-making processes. Presentations. Briefs. Spreadsheets. Meetings where people make choices based on synthesized information. AI that tries to replace those processes faces an uphill battle. AI that makes those processes faster and clearer by pulling human expertise closer to the work—that fits into how organizations actually operate.

Sonnet 4.5 makes a bet on that integration. It assumes we’ll still need presentations and documents. We’ll still make decisions in meetings. Human expertise will still matter because context and judgment can’t be fully automated. The model’s job is to make your expertise more effective, not to make it less necessary.

I saw this throughout the testing. The model’s obsession with checking its work serves this goal. When it verifies formulas and validates services and measures pixel overlap, it’s not trying to be autonomous. It’s trying to give you outputs clear enough that you can focus on applying domain knowledge instead of fixing basic errors.

When it produces briefs and presentations together that share a coherent narrative, it’s not trying to eliminate the need for human judgment about framing and emphasis. It’s making it easier to see the story clearly so you can decide if it’s the right story.

When it responds to both formal prompts and casual instructions, it’s not trying to be magical. It’s trying to lower the friction between what you know and what you need to communicate so more of your time goes into the substance and less into fighting with the tool.


## The Test for Your Work

The question isn’t whether Claude Sonnet 4.5 is better than alternatives in some abstract sense. The question is whether it helps you do your actual work in a way that lets your expertise matter more.

Start with the messiest input problem you have. Voice of customer feedback. Research notes that need synthesis. Multiple data sources that need coherent narrative. Feed it to the model with a clear target format—a brief, a presentation, a structured analysis. See if what comes back is clear enough that you can immediately evaluate whether it’s directionally right.

If you can look at the output and quickly identify what’s good and what needs your expert touch, you’ve found a workflow worth exploring. If you’re spending more time decoding what the model meant than you would have spent creating from scratch, it’s not the right fit for that task.

The pattern I saw across all testing was this: messy inputs to structured outputs, then human refinement based on domain expertise. That workflow works when you have the expertise to evaluate outputs and when the model produces clarity instead of slop. It doesn’t work when you can’t tell if the output is good or when your inputs are already well-structured.

Watch for where you spend your time. If you’re fixing basic errors and restructuring narrative, the model isn’t helping. If you’re making judgment calls about strategic emphasis and audience framing while the model handles synthesis and presentation, you’re in the right zone.


## What Makes This Different

Other models focus on doing more autonomously. Sonnet 4.5 seems to focus on making human expertise more effective. That’s a meaningful difference in professional contexts where judgment matters.

When the model checks its work obsessively and produces clear outputs, it’s not trying to prove it doesn’t need humans. It’s trying to make it obvious where humans add value. When verification catches errors before you see them, you spend less time on quality control and more time on the choices that actually require your experience and context.

Put another way: the model that pulls you closer to your work might matter less for what it can do autonomously and more for how it changes what you can accomplish when clarity replaces slop and human expertise has room to operate.

If that describes what you need—if your bottleneck is synthesis and presentation rather than domain knowledge or data—then this model is worth testing on real work. Start with medium-stakes deliverables where 90% ready is genuinely useful and where you can evaluate correctness because you know the domain.

Your expertise matters. Sonnet 4.5’s job is to make it easier for that expertise to shape the work instead of getting lost in the mechanics of producing it. If it succeeds at that, you’ll know within one real test. You’ll see your judgment improving the output instead of fighting to understand it.

That’s the shift worth watching for. Not AI that does more without you, but AI that makes it clearer where you need to be involved.

---


## Notes for Implementers

If you’re reading this and wondering where to start, the answer depends on what kind of messy inputs you’re drowning in.

Start with your Voice of Customer problem if you have one. The clearest signal in all this testing was how the model handled unstructured customer feedback. If you’re sitting on NPS comments, survey responses, support tickets, or sales call notes that need to become insight, that’s your entry point. The model’s ability to extract patterns, quantify themes, and build coherent narrative from scattered utterances is genuinely strong. You’ll know within one test whether it’s working—either the narrative makes sense and maps to what you know about your customers, or it doesn’t.

The second-best starting point is any executive briefing you’re dreading. If you have a strategy document due, a board presentation to build, or a decision memo that needs to synthesize multiple sources, try the model there. Feed it your research, your data, your messy notes. See if it can build the structure you’d build yourself if you had six uninterrupted hours. The 90% ready output means you can iterate fast. Run it twice with different framings. See which narrative structure serves your audience better.

Don’t start with mission-critical work that has zero room for error. Start with medium-stakes deliverables where 90% ready is genuinely useful and where you have the expertise to evaluate whether the output is directionally correct.

Watch for these patterns as signals you’ve found a good use case. First, you have messy inputs that need structure—customer quotes, research notes, multiple data sources that need synthesis. Second, the output format is something standard—a presentation, a brief, a spreadsheet—not something exotic or highly specialized. Third, you can evaluate correctness quickly because you know the domain. Fourth, iteration matters more than getting it perfect on the first try.

The organizations that will benefit most are ones where knowledge work is bottlenecked by synthesis and presentation rather than by data collection or domain expertise. If your team has plenty of smart people who understand the business but are constantly behind on turning their knowledge into executive-ready deliverables, this model helps. If your problem is that you don’t have expertise or data, the model can’t fix that.

The warning signs that it’s not working are straightforward. If you’re spending more time fixing outputs than you would have spent creating them from scratch, stop. If the narrative structure is consistently wrong and you’re doing major reorganization every time, the model isn’t finding the right patterns in your inputs. If you can’t tell whether the output is good or bad without extensive research, you’re not in a position to evaluate it properly.

The highest-value pattern across all the testing was this: give Sonnet 4.5 messy inputs and a clear target format, then use your expertise to evaluate and refine the structured output it produces. That workflow—messy to structured, then human refinement—is where the time savings actually materialize and where your expertise becomes most valuable.

One more thing worth noting: the model’s ability to produce briefs and presentations together in one coherent package means you can think about work products differently. Instead of building a presentation and then writing the brief that explains it, you can produce both together and refine them in parallel. That’s faster, and it keeps the narrative consistent across formats.

Start small, with one real deliverable that has actual stakes but isn’t career-defining. Give it your messiest input problem and your clearest output requirement. See if the 90% ready pattern holds for your work. If it does, you’ve found a workflow worth scaling. If it doesn’t, at least you know quickly and haven’t committed to a major process change.

Claude Sonnet 4.5 isn’t magic. It’s a tool that happens to be very good at turning messy inputs into clear outputs that let your expertise shine through. If that’s what your work needs, try it. If it’s not, don’t force it.

[![](https://substackcdn.com/image/fetch/$s_!kx39!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99c6d072-5c52-4472-8822-fb72e2ed149f_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!kx39!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99c6d072-5c52-4472-8822-fb72e2ed149f_1024x1024.png)
