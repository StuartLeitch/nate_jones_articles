---
title: "Executive Briefing: The Fork That Determines Whether AI Compounds or Stalls"
author: "Nate Jones"
published: 2026-01-04
url: https://natesnewsletter.substack.com/p/executive-briefing-the-leveling-crisis
subtitle: "Watch now | Why the skill trees for tech and non-tech roles are merging—and what leaders need to define so teams can actually level up."
audience: everyone
scraped_at: 2026-01-05 19:08:26
---

Andrej Karpathy—early OpenAI researcher, former head of AI at Tesla—recently said he’s never felt this behind as a programmer. The programming profession, he wrote, is being “dramatically refactored.” The contributions from the programmer are becoming “increasingly sparse.”

Read that again. Someone who literally helped build these systems feels the ground shifting underneath decades of accumulated expertise. If that’s true for him, what does it mean for everyone else?

Here’s the part that matters for organizational leaders: Karpathy also said he senses he could be “10x more powerful” if he figured out how to properly work with what’s emerging. The gap isn’t between practitioners and obsolescence. The gap is between where people are now and where they could be if they learned to work differently. But “differently” isn’t a training module. It’s a new skill tree—and right now, most organizations haven’t defined what that tree looks like.


## **The decision at the heart of this briefing**

We’re stuck in a world where technical teams have their skill tree and non-technical teams have theirs. Engineering levels up through code review, system design, debugging. Legal levels up through contract drafting, negotiation, regulatory knowledge. Finance levels up through modeling, analysis, forecasting. All of these skill trees assume manual problem solving: humans do the cognitive work, tools make it faster or more accurate.

That assumption is breaking. The skill trees are going to merge—not because AI makes everyone “technical,” but because the skills that matter when AI handles generation are the same across functions: specifying intent clearly, maintaining authority over outputs you didn’t fully create, building workflows that don’t depend on individual heroics, and creating systems that improve over time instead of resetting with every project.

The lawyer building a contract review workflow and the engineer building a debugging assistant are now climbing the same tree. Different artifacts, different tools, same hierarchy of capabilities. Leaders who define that tree give their teams something to climb toward. Leaders who don’t leave everyone stuck—leveling up on skill trees built for a world that’s disappearing.

**The decision this briefing asks you to make:** Are you going to keep separate skill trees for technical and non-technical roles, or define a unified AI problem-solving tree and re-level your organization around it? That’s not a philosophical question. It’s a budget question, a hiring question, a “what does competence mean here” question. Everything that follows is designed to make that decision concrete.


#### **This briefing covers:**

- **Why the old skill trees are breaking**—the shift from manual problem solving to orchestrating AI-assisted work, and why it affects every function
- **The new skill tree**—four levels of capability that apply across legal, finance, engineering, operations, and every other knowledge function
- **The fork at the base**—tool-mode versus infrastructure-mode as the first decision on the new tree
- **What leaders need to define**—how to lay out the skill tree so teams can actually level up
- **How to get started**—building proof that the new tree works in your organization


## **Why this matters more than operational hygiene**

I want to be direct about the strategic stakes before we get into mechanics, because I think they’re easy to miss.

The obvious framing is risk mitigation. AI-generated work often ships without proper review, without records of how decisions were made, without clear accountability. That exposure grows every week. It’s true, and it matters. But if risk avoidance were the whole story, we’d be talking about compliance, not strategy. The deeper question is about value capture.

AI is creating enormous productivity gains across every knowledge function. That much is obvious to anyone paying attention. What’s less obvious is that these gains don’t automatically flow to the organizations deploying the tools. They flow to whoever maintains authority over the outputs—whoever can stand behind the work and say, with confidence, “this is correct, and here’s how we know.”

That authority used to come naturally. When a person wrote something, they could explain it, defend it, trace every decision. But when AI generates the first draft—or the first five drafts—that natural chain of custody breaks. You can ship something correct without fully understanding why. You can also ship something wrong that looks entirely professional. The question becomes: how do you capture AI’s speed without losing your ability to stand behind what you produce?

There are two ways organizations tend to answer that question, and they lead to very different outcomes.

The first treats AI as a personal productivity tool. People use it however they see fit. Quality depends on individual skill. Review happens after the fact, if it happens at all. I call this **tool-mode**. It feels fast. It often is fast. But it’s also unpredictable—sometimes you get brilliant acceleration, sometimes you get confident errors that slip through because they look polished. There’s no systematic way to improve the odds over time.

The second approach treats AI as part of the organization’s operating infrastructure. There are clear checkpoints between “AI generated this” and “we’re sending this to a customer or board.” There are records that let you reconstruct how any decision was made. There are defined boundaries around what AI can and cannot do without human approval. I call this **infrastructure-mode**. It requires more upfront design. But it means you can systematically capture the upside of AI while containing the downside.

That difference in approach tends to create a difference in capability, not just efficiency. Organizations in infrastructure-mode can often do things their competitors cannot. They can handle more complex AI-assisted work because they have the governance to support it. They can adopt new AI models with more confidence because they have ways to test before deploying. They can bring new people up to speed more quickly because knowledge lives in systems rather than individual heads. They compound improvements week over week instead of starting from scratch with every project.

There’s also a talent dimension worth noting. The best people are watching to see which organizations will help them build durable skills in this new era, and which will just have them chasing the latest tool. A clear path for growth—what I’ll describe as a skill tree—becomes a signal that you’re serious. That signal matters now, while the market is still sorting itself out.

So when I talk about defining a unified skill tree for AI-assisted work, I’m not talking about training programs or HR housekeeping. I’m talking about whether your organization is positioned to capture the economic value AI creates—or merely to accelerate its risks.


## **Why the old skill trees are breaking**

Every organization has skill trees, whether they’re written down or not. They’re the implicit ladders people climb: what you need to know at each level, what competence looks like, how you prove you’re ready for more responsibility. For decades, these trees have been function-specific and built on the same assumption—humans do the cognitive work.

In engineering, the tree runs from writing working code to designing systems to leading technical strategy. In legal, it runs from drafting contracts to structuring deals to advising on enterprise risk. In finance, from building models to owning forecasts to shaping capital allocation. Each tree has its own vocabulary, its own proof points, its own senior practitioners who exemplify mastery.

The trees have remained separate because the work itself was different. A senior engineer’s expertise doesn’t transfer to legal. A CFO’s judgment doesn’t make them a better coder. The skills compound within domains, not across them.

AI changes this. Not because it makes everyone “technical,” but because it changes what the scarce skills actually are. When models can draft code, contracts, analyses, and forecasts, the value shifts from production to orchestration. And orchestration skills look surprisingly similar across functions:

- **Specifying intent clearly**—so the model has what it needs to produce useful output, not plausible nonsense.
- **Maintaining authority over outputs you didn’t fully create**—verification, knowing where claims come from, recognizing when to trust and when to check.
- **Building workflows that don’t depend on individual heroics**—repeatable processes with checkpoints, records, and clear ownership.
- **Creating systems that improve over time**—evaluation, feedback loops, governance that gets better week over week.

These skills aren’t domain-specific. They’re the new technical skills, and they apply equally to the lawyer reviewing AI-drafted contracts, the analyst validating AI-generated forecasts, and the engineer debugging AI-written code. The skill trees are merging because the underlying capability stack is converging.

---


## **What the merger looks like in practice**

In the old world, authorship and authority were tightly linked. If you wrote the code, you could justify it line by line. If you drafted the contract, you could trace every clause. If you built the model, you could explain every assumption. You owned the work because you made the work.

That coupling is breaking. Now you can ship something correct without fully understanding why it’s correct—and you can also ship something wrong that looks completely correct to everyone involved. The new competence isn’t authorship. It’s orchestrating uncertainty without surrendering authority.

Let me share a composite example drawn from multiple organizations I’ve worked with. A regional bank’s support team used AI to respond to a customer complaint about overdraft fees. The model drafted a response offering a fee reversal and a rate adjustment. It looked reasonable. It shipped. The commitment contradicted internal policy, and now legal is trying to unwind it while the customer threatens to post the exchange publicly.

The person who sent that response wasn’t incompetent. They were operating with old-tree skills in a new-tree world. They knew how to draft good responses. They didn’t know how to maintain authority over responses they didn’t draft—how to verify against policy, how to recognize when a plausible output wasn’t actually a correct one.

Another composite: a finance team used AI to analyze quarterly results and prepare a board presentation. The model used a different baseline period than the company’s standard methodology. The year-over-year comparison showed improvement where the correct calculation would have shown decline. The reviewer focused on the prose quality—which was excellent—and didn’t re-verify the underlying calculations. A strategic decision was made using the wrong number.

Same pattern. Old-tree skills (review the narrative, check the formatting) applied to new-tree work (validate computational assumptions, maintain clear records of methodology). The failure wasn’t “AI hallucinated.” The failure was “we haven’t defined what competence looks like when AI handles generation.”

These failures look different across functions, but they’re the same missing skill: authority without authorship. That’s the capability gap the new tree is built to close.

---


## **The first thing that breaks is leveling**

There’s another reason you can’t treat this as simply “training people on tools”: your leveling system will fight you.

I’ve seen this pattern at multiple companies over the past few years. HR becomes the de facto gatekeeper for skills and promotions because they own the rubrics. The rubrics are tied tightly to job families: engineering is “technical,” everyone else isn’t. The requirements get frozen into job descriptions and interview processes.

So you get outcomes that feel policy-correct but are operationally wrong. A team needs someone who can reason about data, but the job description says “SQL proficiency required,” so HR filters out candidates who can get to correct SQL reliably using AI—because it doesn’t count as “real SQL skill.” Or someone is otherwise qualified for a role but gets blocked because they can’t perform a task manually that the organization has already delegated to tools in practice.

That’s not bureaucratic friction. It’s a mismatch between what the organization thinks it’s measuring and what the work now requires.

AI is turning skills into a continuum across job families. More people will need to be comfortable with semi-technical behaviors—designing checks, thinking in test cases, noticing failure modes, setting constraints, diagnosing why a system drifted. We probably won’t call them “unit tests.” We may not even call them “evaluations.” But the underlying skill is the same: designing systems around correctness instead of hoping the output looks right.

This is becoming the new equivalent of “everyone at work knows Excel.” Not because everyone becomes an engineer, but because everyone is now operating systems where the cost of being vaguely right is shipping the wrong thing with confidence.

If leaders don’t define the unified skill tree, HR will keep enforcing the old one. And you’ll have the worst of both worlds: AI is already in the workflow, but your hiring and promotion gates still reward manual authorship over reliable orchestration.

**The implication is worth stating plainly:** your first infrastructure investment isn’t a tool. It’s a rubric. The job families and leveling ladders need to be rewritten to reward clarity of intent, good judgment, systematic thinking, and the ability to design for correctness—across functions.

What that looks like in practice: if your rubrics require manual execution of skills that the organization now delegates to AI, you’re selecting for the wrong thing. Consider replacing “can do X manually” with “can get correct outcomes reliably, with records showing how.” Update promotion criteria so proof artifacts matter more than self-reported skill. And audit your job descriptions—any hard requirement that tests old-tree competence may be filtering out people who could actually do the new-tree work.

---


## **Level 0: The fork at the base**

Before teams can climb the new skill tree, organizations face an entry test: tool-mode or infrastructure-mode. This is Level 0—the fork that determines whether anything else compounds.

I defined these briefly earlier, but let me be more specific about what each looks like in practice.

**Tool-mode** treats AI as a personal productivity aid. Prompts live in individual heads. Quality depends on who’s using the tool. Review happens after the fact, if it happens at all. There’s often no systematic record trail, no reliable way to tell at scale if what shipped was correct or just plausible. Research suggests that a large majority of AI users are bringing their own tools to work without organizational oversight—which makes it easy to drift into tool-mode by default, especially in workflows no one explicitly owns.

**Infrastructure-mode** treats AI as a production environment. Permissions are explicit. Checkpoints exist between generation and decision. Record trails are automatic. The model generates; the workflow decides; authority stays with humans who are accountable for outcomes.

Tool-mode feels faster. Infrastructure-mode compounds. Organizations that build infrastructure-mode improve week over week—they can adopt new models without gambling, reuse workflows across teams, catch errors before shipment, and learn from failures systematically. Organizations stuck in tool-mode stay trapped in one-off improvisation, where every project starts from scratch and success depends on having the right person in the room.

The fork matters for skill development because tool-mode doesn’t have a skill tree. There’s nothing to climb. People get better at prompting through trial and error, but their knowledge doesn’t transfer, doesn’t compound, and walks out the door when they leave. Infrastructure-mode creates the scaffolding for an actual tree: defined levels, clear proofs, capabilities that build on each other.

---


## **How to tell which mode you’re in**

You don’t need a consultant to diagnose your mode. These questions will do it:

**Where are the records?** If “records” means “check the chat history,” you’re in tool-mode. If it means “pull from the system of record,” you’re building infrastructure.

**How did the last model upgrade go?** If it was hope and spot-checking, tool-mode. If you tested systematically and knew the results before deploying to production, infrastructure-mode.

**Who owns the output?** If accountability diffuses to “whoever used the tool,” tool-mode. If there’s a named decision owner in a system of record, infrastructure-mode.

**What happens when something goes wrong?** If it’s a scavenger hunt through chat logs followed by a policy memo, tool-mode. If reconstruction takes under an hour and the workflow gets updated, infrastructure-mode.

**What do new hires learn?** If they inherit tribal knowledge about “prompts that work,” tool-mode. If they get onboarded into documented workflows, infrastructure-mode.

Most organizations will recognize themselves in more than one of these patterns. That’s diagnostic: you probably have tool-mode in most places and infrastructure-mode in few or none—or you’re mid-transition with some workflows properly structured and most still running on hope.

---


## **The new skill tree: ur-skills for an agentic era**

Once you’ve chosen infrastructure-mode, capability stacks in four levels. But I want to be clear about what these levels actually represent, because they’re easy to mistake for a governance checklist or a maturity model. They’re neither.

These are **ur-skills**—foundational capabilities that used to be specialized but are now becoming universal. Each one was historically the province of a particular profession or function. Requirements engineers knew how to specify intent precisely. Quality managers knew how to define what good looks like. Process engineers knew how to design repeatable systems. Continuous improvement specialists knew how to build feedback loops. Most knowledge workers could get by without these skills because they were executing their own intent, checking their own work, and relying on personal expertise to deliver quality.

Agentic work changes that. When you’re delegating cognitive work to AI systems—when the first draft, the first analysis, the first code, the first recommendation comes from a model rather than from you—these specialized skills become table stakes for everyone. The lawyer needs specification skills that used to belong to requirements engineers. The analyst needs evaluation skills that used to belong to quality managers. The engineer needs systematization skills that used to be the domain of ops. The skill trees are merging because the underlying work is converging: everyone is now orchestrating systems that generate on their behalf.

Let me walk through each level in that light.

---


## **Level 1: Specification—the discipline of saying what you actually mean**

Specifying intent precisely used to be a specialized skill. Requirements engineers did it when translating business needs into technical specifications. Creative directors did it when briefing agencies. Lawyers did it when drafting contracts where ambiguity creates liability. Product managers did it when writing specs that would be implemented by people who weren’t in the room for the original conversation.

But most professionals could afford to be vague. If you were writing your own analysis, drafting your own memo, building your own model, the clarity lived in your head. You could iterate toward what you wanted because you were executing your own intent. Ambiguity wasn’t costly because you were there to resolve it.

That changes when you delegate to AI. Vague intent doesn’t produce “roughly what I wanted.” It produces plausible nonsense—outputs that look professional but miss the point, that confidently answer questions you didn’t ask, that fill gaps in your specification with hallucinated details. The model doesn’t know what you meant. It only knows what you said.

So specification becomes an ur-skill. Everyone who works with AI needs to get better at articulating purpose (what is this for?), audience (who will use it?), constraints (what must be true, what must not be true?), and success criteria (how will I know if this is good?). Everyone needs to learn to provide examples of what good looks like and examples of what failure looks like. Everyone needs to develop the habit of writing specifications that someone else—or something else—could execute without reading your mind.

This isn’t prompt engineering in the narrow sense. It’s the discipline of externalized intent. And it’s the same discipline whether you’re a lawyer specifying what a contract review should flag, a financial analyst specifying how a quarterly analysis should be structured, or an engineer specifying how a code refactor should behave.

**The proof that someone has developed this skill:** they can write specifications that another person could execute and get similar results. Their requests are legible to others. When AI outputs miss the mark, they can diagnose whether the problem was in their specification or somewhere else in the process.

**The failure mode when this skill is missing:** quality varies wildly based on who’s using the tool. Some people get good results and others don’t, and nobody knows why. Outputs require extensive rework because they don’t match what was actually needed. The organization can’t systematize AI assistance because it’s locked in individual heads.

---


## **Level 2: Evaluation—the discipline of knowing good without having made it**

Defining what good looks like used to be implicit. When you authored something yourself, you could feel when it was right. You had internalized standards from years of practice. You could look at your own draft and know whether it met the bar—not because you had written criteria, but because judgment was embedded in your hands.

That coupling between authorship and evaluation is breaking. When AI generates the first draft—or the first five drafts—you’re no longer checking your own work. You’re evaluating work you didn’t produce, using judgment that may or may not apply to outputs structured differently than you would have structured them.

This is genuinely disorienting, and I think we underestimate how much it matters. The instincts you developed over a career were trained on your own work product. They may or may not transfer to AI-generated outputs that are stylistically different, organized differently, confident in places where you would have hedged. A plausible-looking paragraph can hide a factual error. A well-formatted analysis can use the wrong methodology. A professional-sounding email can make commitments that violate policy. The surface quality is high; the underlying correctness is unknown.

So evaluation becomes an ur-skill. Everyone who works with AI needs explicit criteria for what good looks like—not just intuition, but articulable standards. Everyone needs methods for verification: How do I check whether this claim is true? How do I confirm this calculation is correct? How do I know this recommendation follows from the evidence? Everyone needs to understand provenance: Where did this claim come from? What sources did the model draw on? What’s the chain of custody from input to output?

This is the skill I’ve been calling “authority without authorship.” It’s the ability to stand behind work you didn’t fully produce—to maintain accountability for outputs even when you didn’t generate them. It requires making implicit standards explicit, building verification into the workflow rather than hoping you’ll catch problems in review, and knowing when to trust and when to check.

**The proof that someone has developed this skill:** they can explain how they would know if an output was wrong. They have verification methods that don’t depend on heroic attention. When challenged on AI-assisted work, they can show the trail—inputs, sources, checks performed—rather than saying “the model said so.”

**The failure mode when this skill is missing:** confident wrongness ships. Errors survive review because the reviewer was checking for surface quality rather than underlying correctness. Claims can’t be traced to sources. Incidents are hard to reconstruct because no one logged what actually happened.

---


## **Level 3: Systematization—the discipline of making it work without you**

Designing repeatable systems used to be an operations specialty. Process engineers did it. Workflow designers did it. The DevOps movement taught software teams to treat their deployment pipelines as first-class artifacts. But most knowledge workers could rely on ad hoc expertise. They didn’t need repeatable processes because they were the process. Their judgment was the system.

That works until you want to scale, or until you want to take a vacation, or until you want to improve. Ad hoc expertise means starting from scratch every time. It means success depends on having the right person in the room. It means knowledge walks out the door when people leave. It means you can’t measure whether you’re getting better because there’s nothing consistent to measure.

Agentic work makes this problem acute. If AI assistance lives in individual prompts and personal techniques, you get tool-mode: quality varies by user, nothing compounds, every project reinvents the wheel. If you want infrastructure-mode—if you want the organization to get better at AI-assisted work over time—someone has to design the system.

So systematization becomes an ur-skill. Everyone who works with AI needs to think about their work as a pipeline with stages, not as a conversation with a chatbot. They need to identify where checkpoints belong—the moments where “draft” becomes “decision” and human authority must be explicitly exercised. They need a taxonomy of what can go wrong, so that when something fails, they can diagnose the failure class and apply the right intervention rather than randomly tweaking prompts.

This is the shift from “I use AI” to “I operate an AI-assisted workflow.” It means documenting what you do well enough that someone else could do it. It means building the observability to see what’s happening in your process—what inputs were used, what intermediate steps occurred, what checks passed or failed. It means treating your workflow as a system that can be improved, not as a series of one-off interactions.

**The proof that someone has developed this skill:** they have a documented workflow that someone else could run. When something fails, they can name the failure class and point to the intervention. Their knowledge doesn’t live only in their head.

**The failure mode when this skill is missing:** every project starts from scratch. Debugging happens by guess-and-check. Success depends on individual heroics. The organization can’t learn because there’s nothing stable to learn from.

---


## **Level 4: Compounding—the discipline of building systems that improve**

Building for continuous improvement used to be a quality specialty. Continuous improvement methodologies, feedback loops, regression testing, drift detection—these were the concerns of quality managers and platform engineers. Most knowledge workers shipped their work and moved on. Improvement was something that happened to processes, not something individuals built into their daily practice.

But when you’re operating AI-assisted workflows, “ship and move on” means you’re not compounding. You’re just prompting forever. Every time the underlying model changes, you don’t know if your workflows still work. Every time you discover a better approach, it doesn’t propagate. Every time something fails, you fix it locally without improving the system.

The highest-leverage skill in agentic work is building systems that get better over time. This means evaluation harnesses—sets of test cases that let you know whether changes improved things or broke things before you deploy to production. It means feedback loops that catch errors inside the system rather than in the field. It means governance that keeps the system reliable over months as models change, as data changes, as team members turn over.

This is where leverage becomes durable rather than episodic. Organizations that build this capability can upgrade models with confidence. They can onboard new people quickly because knowledge lives in systems. They can improve week over week because they have the infrastructure to measure improvement. Organizations that don’t have this capability are stuck in perpetual improvisation, hoping each time that it works.

So compounding becomes an ur-skill. Everyone who operates AI-assisted workflows needs to think about how their work can be tested and validated. They need to build feedback loops, even simple ones. They need to version their approaches so they can roll back when something breaks. They need to treat their workflows as production systems that require care and feeding, not as chat conversations that disappear when the window closes.

**The proof that someone has developed this skill:** they can upgrade a component of their workflow and know before deployment whether it improved things. They have feedback loops that actually run. When things change, they can roll back. Their work gets better over time in measurable ways.

**The failure mode when this skill is missing:** model upgrades are roulette. Improvements don’t stick. Drift accumulates without anyone noticing until something breaks. The organization runs in place instead of compounding.

---


## **The tree as a whole**

Notice what these four levels share. Each one takes a skill that used to be specialized—the domain of experts in a particular function—and makes it universal because agentic work requires it. Specification was for requirements engineers; now everyone needs it. Evaluation was for quality managers; now everyone needs it. Systematization was for process designers; now everyone needs it. Compounding was for CI/CD specialists; now everyone needs it.

This is why the lawyer and the engineer are climbing the same tree. They’re not doing the same work. They’re not producing the same artifacts. But they face the same structural challenge: they’re delegating cognitive work to AI systems, and doing that well requires the same ur-skills regardless of domain. The skill trees are merging because the underlying discipline of working with intelligent systems is the same whether you’re reviewing contracts or reviewing code.

That’s what makes this moment hard. It’s not that AI is difficult to use—in many ways it’s remarkably easy. It’s that using AI well requires skills most professionals never had to develop because they were executing their own intent, checking their own work, and relying on personal expertise. Those skills are now table stakes. And most organizations haven’t defined what developing them actually looks like.

---


## **What the tree looks like in practice: the mechanics of governance**

Abstract levels become real through specific mechanics. Here’s what Levels 2–4 actually require when you implement them.

**Decision classes.** Not all AI-assisted work carries the same risk. Low-stakes work (internal drafts, brainstorming) needs logging but not gates. Medium-stakes work (team deliverables, documentation) needs peer review plus automated checks. High-stakes work (customer-facing communications, legal documents, financial reports, code affecting production) needs a defined approval workflow with a named decision owner. Defining these classes explicitly is the first step toward proportionate governance.

**Ownership model.** For high-stakes workflows, you need clarity about roles: a decision owner who approves the final artifact and is accountable if it’s wrong, a reviewer who validates correctness before approval, a system owner who maintains the workflow itself, and an incident owner who runs post-mortems when things go wrong. These are roles, not extra meetings. Without them, accountability diffuses to “whoever used the tool,” which in practice means no one.

**Record schema.** Every high-stakes output needs a record: what inputs were used, what sources were retrieved, what model and version, what assumptions were made, what verification checks were passed, what approval chain was followed. This isn’t documentation overhead—it’s what lets you reconstruct incidents, reuse verified components, and demonstrate to auditors that you maintained authority over AI-generated work.

**Checkpoint placement.** Somewhere in the workflow, draft has to become decision. That’s the checkpoint. It needs a checklist of what must be true to proceed, an owner who can approve or reject or escalate, and a fallback path for when the checklist fails. If you can’t point to where this checkpoint is enforced—not suggested, enforced—you’re in tool-mode.

These mechanics map directly to the tree. Decision classes are Level 2 (Evaluation). Record schemas span Levels 2 and 3. Checkpoint placement is Level 3. Evaluation processes are Level 4. The tree tells you what capability looks like; the mechanics tell you how to build it.

---


## **Why leaders need to define this**

Skill trees don’t define themselves. Someone has to say: “Here’s what leveling up looks like. Here’s what you need to demonstrate at each stage. Here’s how we’ll know you’re ready for more responsibility.”

In the old world, this happened implicitly. Senior engineers mentored junior engineers on what good code looks like. Senior lawyers showed associates how to draft. The path was clear because it had been walked many times.

The new tree doesn’t have that history yet. If leaders don’t define it explicitly, teams default to the old trees—leveling up on skills that are being repriced, climbing toward mastery that’s becoming less scarce. Or worse, they stay stuck in tool-mode, getting better at prompting through random trial and error with no systematic path upward.

Defining the tree means several things:

**Making the fork explicit.** Tool-mode versus infrastructure-mode isn’t a philosophical debate. It’s a decision about how your organization operates. Name it. Choose it. For high-stakes work, choose infrastructure-mode.

**Mapping the levels to your functions.** What does Level 1 (Specification) look like for legal? For finance? For customer support? What’s the proof that someone’s operating at Level 2 (Evaluation) versus still developing at Level 1?

**Changing what you hire for.** The valuable capability is now specification clarity, verification design, and systems thinking—not “prompting skill.” Your interview processes should test for this. Your leveling rubrics should reward it.

**Building the infrastructure that makes the tree climbable.** People can’t level up on Evaluation if there’s no system for logging record trails. They can’t reach Systematization if every project starts from scratch. The tree needs scaffolding.

---


## **What to ask for: artifacts, not intentions**

“Define the tree” becomes actionable when you can request specific deliverables:

**A unified skill tree rubric.** One page: the four levels (Specification, Evaluation, Systematization, Compounding) with proof artifacts for each, mapped to at least three functions (one technical, one customer-facing, one back-office).

**Updated leveling criteria.** For at least three roles, what does “senior” mean when AI handles generation? What’s the proof that someone operates at Level 2 versus Level 1?

**A decision class policy.** Low, medium, high stakes—defined in writing. What approval path applies to each? Who owns high-stakes outputs?

**A record template.** What gets logged, by whom, where it lives. One template for low-stakes, one for high-stakes. System of record named.

**A pilot workflow charter.** One page: owner, checkpoint location, record schema, rollback procedure, evaluation plan. This is the proof that infrastructure-mode works in your organization.

---


## **What this changes for executives**

If you accept that the skill trees are merging, three executive levers need adjustment:

**Ownership.** In tool-mode, accountability is diffuse—”whoever used the tool” is responsible, which in practice means nobody. Infrastructure-mode requires named owners for checkpoints, permissions, and escalation paths. The question isn’t whether to assign ownership but whether that ownership has teeth. Does the owner have budget? Can they block shipment? Are they measured on outcomes, not just adoption metrics?

**Hiring and leveling.** Your interview process probably doesn’t test for the new tree yet. Your leveling rubrics probably don’t reward it. Your job descriptions probably still emphasize execution speed over system design. The gap between what you’re hiring for (old-tree skills) and what you need (new-tree skills) will widen until you adjust explicitly. This isn’t about adding “AI experience” to job postings. It’s about testing for specification clarity, verification thinking, and the ability to maintain authority over work you didn’t fully produce.

**Operating model.** Where are the checkpoints, and how do exceptions escalate? This might sound like process bureaucracy, but it’s the opposite—it’s what lets you move fast without accumulating hidden risk. Define the approval steps for high-stakes outputs. Define what triggers human review. Define how rollback works. If your operating model assumes humans catch errors through vigilance alone, you’re back in tool-mode by default. Vigilance doesn’t scale.

---


## **The audit: where are you on the tree?**

Eight questions. Answer honestly—if you’re not sure, the answer is probably the one that makes you uncomfortable.

1. Can any AI-generated draft be shipped or sent without an explicit human approval step?
2. For high-stakes workflows, do we have a defined decision owner—not just “whoever used the tool”?
3. Do we capture a record trail (inputs, sources, model used, rationale) by default?
4. Are permissions defined and enforced systematically—or are we relying on prompts and policies?
5. If a model output is wrong, can we reconstruct how it happened within one business day?
6. Do we have systematic evaluation for any AI-assisted workflow—or are we checking outputs by feel?
7. Do we have a list of “never delegate” decisions—things no model should touch without human judgment?
8. Do we know where tool-mode is already in production (shadow usage), or are we guessing?

If you answered “yes” to question one or “no” to most of the others, you’re in tool-mode. That’s not a moral failing. It’s a starting point.

Questions 1–2 test whether you’re past the fork (Level 0). Questions 3–5 point toward Evaluation (Level 2). Question 6 is evidence of Compounding (Level 4). Question 7 probes the boundary between Evaluation and Systematization—decision rights. Question 8 tests situational awareness: do you even know where you are?

**The escalation question.** Ask your CTO, COO, or General Counsel: “Where do we require a pause between draft and decision—and where do we log the records?” If they can’t answer in concrete terms—systems, permissions, audit trails—you know where you stand. If they describe policies and guidelines instead of enforcement mechanisms, you know.

---


## **Getting started**

You don’t define a skill tree in a strategy document. You define it by building one workflow that demonstrates what the levels actually look like in practice.

The approach I’d recommend: pick a single high-stakes workflow as a pilot. Look for something with high frequency (so you learn fast), medium stakes (high enough to matter, not catastrophic if the first attempt fails), an existing system of record (so you’re not building from scratch), and clear success criteria (so you know when you’ve won).

Good pilot candidates include: customer support response drafting, contract clause review, code review assistance, financial report commentary, marketing copy generation. These are high-frequency, have clear quality standards, and produce outputs that can be verified against existing criteria.

The work involves several phases: selecting and scoping the workflow, defining the infrastructure (record schema, checkpoint placement, permission boundaries), implementing and testing with real work, and then adding systematic evaluation so improvements stick.

**What success looks like after a few months:** one workflow running with checkpoints, records, and evaluation in place. Decision owner named and empowered. Incident response tested at least once. The team knows what infrastructure-mode feels like because they’ve built it.

**What success looks like after six months:** multiple workflows in infrastructure-mode. A pattern emerging for how to build checkpoints and records efficiently. Model upgrades completed without production incidents because evaluation caught regressions. New hires onboarded into documented systems instead of tribal knowledge. The skill tree is real because people are climbing it.

---


## **Real questions you’ll face**

**“Our legal team says they need to review every AI output—including internal brainstorm docs.”**

This is a classification problem. Legal should own high-stakes decisions, but reviewing everything doesn’t scale and isn’t necessary. The solution is defining decision classes clearly: legal owns high-stakes, medium-stakes gets peer review plus automated checks, low-stakes gets logging. The upfront work of classification is worth it.

**“We tried checkpoints and people routed around them—copying outputs to personal docs, skipping the approval step.”**

This usually means the checkpoints weren’t catching errors people cared about. If the checkpoint is just friction, people will bypass it. The solution is building evaluation first—showing that the checkpoint actually catches real problems. Make bypass visible through logging and reporting, and demonstrate value quickly.

**“This will slow us down. We have targets to hit.”**

It’s worth being honest that the first workflow will take real investment. You’re building scaffolding, not just shipping. But after the first workflow, you reuse that scaffolding. The second workflow takes a fraction of the time. The third is mostly configuration. The question is whether you want to invest upfront for compounding returns, or keep improvising indefinitely.

---


## **The decision**

Karpathy’s reflection isn’t a lament. It’s a signal. The skills that made people valuable last year are being repriced. The instincts they trained are less reliable than they were. And this isn’t just true for programmers—it’s true for lawyers, analysts, writers, strategists, and every other knowledge worker whose job involves producing drafts that become decisions.

The old skill trees were built for manual problem solving. Technical roles had their tree, non-technical roles had theirs, and they were separate because the work was different. That separation is collapsing. The new tree is unified—not because everyone becomes an engineer, but because the skills that matter when AI handles generation are the same across functions.

So: are you going to keep separate skill trees, or define a unified one and re-level your organization around it?

Keeping separate trees means continuing to train people on old-tree skills while the ground shifts underneath them. It means hiring for execution speed when the scarce capability is now verification design. It means running workshops that don’t change behavior because they’re teaching prompting, not orchestration.

Defining a unified tree means asking: what does “senior” mean when AI handles generation? What are the proof artifacts for each level? What do we hire for now? It means requesting the artifacts—the rubric, the leveling criteria, the decision class policy, the pilot charter—that turn “we should adapt” into “here’s what competence looks like here.”

The skill trees are merging whether you plan for it or not. The only question is whether you define what the new one looks like—or leave your teams climbing toward mastery that’s becoming less scarce, on ladders built for a world that’s disappearing.

---


## **Appendix: The Complete Skill Tree**

Reference for mapping the tree to your functions. Each level builds on the one before. The “proof” column describes artifacts, not claims—things you can point to, not things you believe about your organization.

[![](https://substackcdn.com/image/fetch/$s_!Sx0b!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30db96f2-90ce-41a5-abc7-ccc5d0108cdd_793x470.png)](https://substackcdn.com/image/fetch/$s_!Sx0b!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30db96f2-90ce-41a5-abc7-ccc5d0108cdd_793x470.png)

[![](https://substackcdn.com/image/fetch/$s_!3nvT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6563d958-3112-4885-b38a-80dc571db9ba_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!3nvT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6563d958-3112-4885-b38a-80dc571db9ba_1024x1024.png)
