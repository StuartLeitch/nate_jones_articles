---
title: "Grab the 4 prompts I use to make messy work legible--without killing what made it valuable + the visibility trap most companies fall into (and how to avoid it)"
author: "Nate Jones"
published: 2026-01-03
url: https://natesnewsletter.substack.com/p/grab-the-4-prompts-i-use-to-make
subtitle: "Watch now | Why uing AI for \"visibility\" isn't as valuable as you think--and why it may actually hurt the teams that drive progress."
audience: everyone
scraped_at: 2026-01-05 19:08:29
---

I’ve been watching companies buy a particular kind of AI tool lately, and I’m increasingly convinced it’s a trap.

The pitch goes like this: point a model at your PRs, your Slack threads, your docs and tickets and on-call logs, and leadership can finally see what’s happening inside their own company. A single pane of glass for the enterprise. Total visibility. The demos are beautiful—clean dashboards, insights that appear to surface themselves, the promise that you’ll finally understand your own organization.

And look, I get the appeal—I’ve wanted this kind of visibility too. Who wouldn’t? But here’s what I keep seeing when I talk to teams that actually deploy these systems: the dashboards get built, the reports get generated, leadership feels like they can see everything—and the parts of the organization that actually keep the company alive start quietly dying. Not because anyone intended that outcome. Because the tool was solving for the wrong constraint.

AI is making legibility cheap. That sounds like a win. It isn’t—not unless you spend it on the right thing. Most companies are spending it on visibility when they should be spending it on leverage for the small teams that actually produce value. And those two things are in tension more often than anyone wants to admit.

**Here’s what’s inside:**

- **The fork most leaders don’t realize they’re facing.** You’re building either a magnifying-glass company (AI for visibility, scoring, oversight) or a tiger-team company (AI for execution power in small groups). The first one feels like control. The second one is where value actually comes from.
- **Why cheap legibility produces fake visibility.** Before AI, making work legible was expensive enough that you couldn’t make everything visible. That friction was doing more work than anyone realized. Now AI generates the appearance of understanding faster than organizations can distinguish appearance from reality.
- **What happens when you focus on the wrong layer.** Surveillance creates concealment. When leadership uses AI primarily to supervise, the people doing real work respond rationally by hiding it. The company looks more organized while the root system dies.
- **What a tiger-team company actually looks like.** This isn’t “no legibility.” It’s legibility that follows work instead of dictating it. AI as translator and historian, not bureaucrat.
- **And a prompt kit** that reconstructs what actually happened (and what it means) from the messy work—so you get legibility without turning your company into a dashboard farm.

Let’s figure out which kind of company you’re building.


## **[Grab the Prompts](https://www.notion.so/product-templates/Prompt-Kit-Legibility-that-Follows-Work-2dc5a2ccb5268096b9dfd6990dbfb81c?source=copy_link)**

These prompts are designed for different roles and different moments, but they share a common assumption: the goal isn’t to eliminate messy work or make everything visible. It’s to make sure the *right* things become legible at the *right* time—after the work happens, not instead of it.

- **The Work Reconstructor** is the practical implementation of “AI as historian.” Legibility should follow work, not dictate it—and this prompt helps anyone do exactly that. Useful for the IC who just shipped something messy, the builder who spiked on a problem, anyone who needs to translate fast work into something leadership can understand.
- **The Visibility Gap Finder** addresses the personal stakes of the legibility trap. If you’re doing valuable illegible work, you need to know whether that’s hurting you—and if so, how to fix it without turning into a self-promotion machine. Works for anyone at any level who suspects their real contributions aren’t being captured.
- **The Tiger Team Identifier** is for leaders who want to understand where value comes from in their organization. It’s sharper than a generic “audit your culture” framework because it focuses on concrete emergency patterns—the moments when organizations tell the truth about themselves.
- **The Map Audit** directly addresses the magnifying-glass trap. If you have dashboards, metrics, or AI-generated reports that feel authoritative, this prompt forces you to ask whether they’re measuring reality or generating expensive noise. Works for leaders evaluating tools, and for ICs who want to push back on metrics that don’t capture what matters.


## **The two kinds of work inside any company**

There’s a distinction that’s been true for as long as organizations have existed, and it changes how you think about everything that follows.

There are two kinds of work in any functioning company.

**Legible work** is the stuff that shows up cleanly in your systems—the Jira tickets, the OKRs, the roadmaps and planning decks. It’s structured and trackable and explainable, the kind of work you can point to when a board member asks what engineering is doing. It fits in the boxes because it was designed to fit in the boxes.

**Illegible work** is everything else. It’s the favors, the backchannels, the shared intuition that never makes it into a ticket. It’s the engineer who texts a colleague at 10 PM because they know where the bodies are buried in that particular service. It’s the sales rep who knows which customer is actually about to churn based on vibes from the last call, before any churn score picks it up. It’s the PM who can broker a conversation between engineering and leadership because they’ve built up enough trust with both sides that people will actually tell them the truth.

And it’s the tiger team. You know the pattern: something is truly on fire—database hits its limit, major customer threatens to leave, production system fails in a way nobody anticipated—and the company suspends whatever formal process exists. Someone pulls together a small group of people everyone knows can actually fix things. Nobody checks the RACI matrix. Nobody opens a ticket first. They just get in a room, physical or virtual, and handle it.

Think about your own organization for a moment. When something urgent happens, who do people actually go to? The company tells the truth about itself in emergencies. The org chart is one story. The emergencies are another.

Here’s the thing: illegible work is what keeps companies running. Not as a nice-to-have supplement to the real work that shows up in dashboards—as the actual engine. The legible layer is scaffolding: necessary, useful, but not where the value gets created.

AI doesn’t erase illegible work. If anything, it amplifies it. A five-person team with AI tools can now do work that used to require a much larger org, because the leverage compounds across every dimension—coding, debugging, synthesis, option exploration, customer understanding. The illegible engine is getting more powerful.

And that’s exactly why the magnifying-glass approach is so dangerous. It focuses attention on the wrong layer of the organization.

---


## **The fork**

Here’s the actual strategic choice, and I think most leaders are drifting into one path without realizing there was a fork at all.

**Path one: the magnifying-glass company.** This is the organization that uses AI primarily for visibility, scoring, and oversight. It buys the single-pane-of-glass tools. It generates AI-drafted roadmaps and AI-scored OKRs. It builds dashboards that claim to show productivity and risk and progress across every team. Leadership looks at these dashboards and feels like they can finally see what’s going on.

That feeling is the problem. Confidence and accuracy are not the same thing.

**Path two: the tiger-team company.** This organization uses AI primarily to multiply execution power in small, high-trust groups. It treats pods of five or six people—with shared context, shared taste, and the ability to move without waiting for permission—as the primary production units of the business. AI helps these teams code faster, debug faster, explore options faster, synthesize customer feedback faster. And the job of making that work legible to leadership? That happens after the work, not before it. Not instead of it.

The magnifying-glass company selects for the feeling of control.

The tiger-team company selects for the reality of output.

Both sound reasonable when you describe them abstractly. But they lead to very different organizations over time, and the differences compound. Most organizations default to the magnifying glass—not because leaders are stupid, but because legibility is how large organizations make decisions. That instinct is wired deep. It takes active effort to resist.

---


## **How cheap legibility creates fake visibility**

“AI dashboards can be misleading” is obvious and unhelpful. What’s actually happening is more interesting and more dangerous.

Before LLMs, legibility had a cost. Engineers had to write tickets, PMs had to write summaries, managers had to build decks, and everyone had to sit in status meetings that existed primarily to make work visible to people who weren’t doing it. That friction wasn’t just annoying—it was a natural governor on how much visibility theater you could run. Making work legible required real human effort, which meant you couldn’t make everything legible, which meant leaders had to accept that some parts of the operation would remain opaque.

AI drops that cost toward zero. If you wire an LLM into the right sources, it can generate coherent summaries from code diffs, Slack threads, on-call logs, meeting transcripts, even badly written tickets. You can ask it what changed in a service last week, what shipped, what’s blocking the roadmap—and you can get a coherent-sounding answer without convening a single meeting.

That’s the upside, and it’s real: less ceremony, less pressure to stuff everything into rigid processes just to make it explainable, genuine gains in efficiency for the translation work that used to eat people’s time.

But here’s the part I keep seeing, and I don’t think enough people are talking about it: AI makes it trivially easy to generate the *appearance* of understanding without the substance behind it.

Let me make this concrete.

A director pulls up a dashboard that shows engineering velocity is up 40% quarter-over-quarter. The chart looks precise, the numbers are specific, and the whole thing was generated in minutes by a tool that ingested sprint data. It looks empirical.

But when someone asks what the metric actually measures, the answer is story points closed. And when you dig in, it turns out the team spent the last quarter splitting tickets into smaller pieces to hit sprint goals—not shipping faster, just recategorizing work so the numbers look better. The dashboard isn’t lying, exactly—but it’s measuring something that doesn’t map to the outcome anyone actually cares about. And because the dashboard looks authoritative, because it was generated by a system that ingested real data, nobody questions it until something breaks.

Or imagine a risk assessment that lands in leadership’s inbox every week—AI-generated, color-coded by severity level, flagging concerns across projects. It looks rigorous. But nobody can actually explain how the model decides what’s high-risk versus low-risk, because the system was spun up fast and the logic is opaque. So teams have learned to game the inputs. They know which phrases trigger warnings and which phrases get green lights, so they write their updates accordingly. The risk report becomes a Rorschach test for whatever leadership already wanted to believe.

I’ve started calling this the Potemkin organization. The company looks organized from the outside—dashboards, plans, metrics, reports—but the village is staged. The real work is happening off-map. And the map is getting more detailed and more confident even as it drifts further from the territory.

The danger isn’t that leadership becomes blind. That would be obvious. The danger is that leadership becomes overconfident in the wrong map—an AI-generated map that feels precise and creates a false sense of control. That’s worse than having no map at all. At least with no map, you know you’re navigating by feel.

---


## **Why surveillance creates concealment**

Here’s the second-order effect that should worry you most, and I want to be clear that this isn’t a moral argument. It’s incentive reality.

If leadership uses AI primarily to supervise—AI-drafted roadmaps, AI-scored OKRs, automated oversight rituals, dashboards-as-management—teams will respond rationally to the incentives they’re given.

Real work will go underground, because real work is messy and doesn’t map cleanly to the metrics. The tiger team that forms to fix a customer escalation doesn’t fit in a sprint. The engineer who spends a weekend hacking on something because they had a hunch doesn’t show up in the productivity dashboard. If the measurement system rewards clean legibility, people will hide the work that can’t be made clean.

Backchannels become more covert, more political, because that’s the only space where honest conversation feels safe. If everything visible gets scored, the invisible becomes more valuable.

People start performing for what the AI can measure instead of what customers actually feel, because that’s what gets rewarded. And when the measurement system is powered by AI that can generate increasingly detailed reports, the gap between the map and the territory can grow very wide before anyone notices.

And here’s the thing: this isn’t new. Organizations have always struggled with the fact that when a measure becomes a target, it stops being a good measure—people change their behavior to hit the number rather than achieve the outcome the number was supposed to represent. What’s new is that AI makes the measurement layer so cheap and so authoritative-looking that the failure mode accelerates. You can build the wrong map faster than ever before.

---


## **What a tiger-team company actually looks like**

I don’t want to leave this at “visibility tools bad.” That’s not the point. The point is that legibility should follow work, not dictate it. And there’s a real alternative to the magnifying-glass model that’s worth understanding.

In a tiger-team company, small teams are treated as the primary production units. They have clear scope, clear outcomes, and trusted autonomy to move without asking permission for every decision. AI’s job is to act as a translator and historian—to reconstruct what happened after the messy work is done, so leadership can understand it without having dictated it.

Let me make this concrete.

A company’s biggest customer calls with a problem that doesn’t fit neatly into anyone’s job description. It’s part product issue, part billing confusion, part “we told them something six months ago that turned out not to be true.” No single department owns it. The formal escalation process would take days that nobody has.

Within a few hours, a small group has formed organically—someone who knows this customer’s history, a technical person who can figure out what actually happened in the system, someone with authority to approve a credit or contract adjustment, and the person who’ll need to have the difficult conversation with the buyer. Nobody assigned them. They just know each other, trust each other’s judgment, and understand that this is the kind of thing that has to get handled before it festers.

They work through it together over a day and a half. The resolution isn’t elegant—it involves some apologies, a partial credit, and a commitment to fix something that should have been fixed months ago. Messy, but real. And the customer stays.

Leadership finds out about most of this afterward. Someone wrote up what happened—what broke, why, what was done, what needs follow-up—with AI helping organize the timeline and surface the key decisions. The director reads it and understands the situation without having been in the room while it was happening.

The work was illegible while it was in motion. That’s exactly what let it move fast enough to matter.

That’s the model: protect fast paths, let teams spike on problems without stuffing everything into a controlled pipeline, measure outcomes and impact rather than adherence to an AI-generated plan. Use AI as a cheap historian that reconstructs meaning after the work happens, not as a bureaucrat that dictates structure from above.

I should be honest about what this requires, because it’s not free. Tiger teams can curdle into hero culture if you’re not careful—a handful of people who are always on fire because the organization never builds the systems that would prevent fires.

And “legibility follows work” sounds elegant, but it demands discipline: after-action writeups, decision logs, clear handoffs. If you skip the reconstruction step, the work stays illegible forever, and leadership loses the ability to learn from it or allocate resources intelligently. The alternative to the magnifying glass isn’t chaos. It’s a different kind of rigor—one that respects how real work actually happens while still creating the organizational memory you need to improve.

---


## **What this means for different roles**

“Rethink your AI strategy” isn’t actionable. Here’s what is.

**If you’re a CEO or executive:** Treat visibility tools as dangerous by default—useful for reducing ceremony, risky when they become a management operating system. The question to keep asking: Is this helping me understand reality, or generating a confident-looking map my organization will bend itself toward? And: Do I know who my tiger teams are? Am I empowering them, or strangling them with oversight?

**If you’re an engineering or product leader:** Protect fast paths. The strange corners—spike mode, emergency response, the weekend hack—are often where the best work happens. Don’t accept AI metrics you can’t trace to concrete actions. Design for work first, legibility second.

**If you’re a PM or in a similar translation role:** PM historically existed partly as the translator layer between technical reality and business expectations. LLMs collapse that translation cost dramatically. Your value shifts toward judgment, sequencing, alignment, and risk—the illegible work that AI can’t do. Lean into that.

**If you’re an IC or operator:** Be skeptical of visibility systems that feel like surveillance. When you’re part of a tiger team—when you’re in the room that’s actually solving the problem—protect that space. Document what happened afterward, not during.

---


## **A note on why this is hard to resist**

Most organizations default to the magnifying glass because legibility is how leaders allocate resources and defend decisions. That instinct is wired into how organizations work.

What’s different now is that legibility has become cheap. The question isn’t “can we see this?” anymore—it’s “should we be looking here at all?” The magnifying-glass tools will keep getting better at looking authoritative. Choosing the tiger-team path requires active resistance. But I think it’s the right path.

---


## **The weekend-hack test**

I’ve seen something more than once in my career that captures what’s actually at stake here.

An engineer shows up on Monday morning with something they built over the weekend—wasn’t on the roadmap, no ticket, no sprint, no OKR. Just: “I made this thing—is it worth anything?”

Sometimes it was worth more than anything the organization had formally planned—because it solved a problem nobody had time to acknowledge, or addressed a need that didn’t fit in the planning framework, or was just *good* in a way that the process couldn’t have produced because the process was selecting for legibility rather than value.

My job, in those moments, wasn’t to squish it. It was to recognize that it was mission-aligned and figure out how to pull it into the fold without killing whatever conditions made it possible. That meant protecting the space where it came from. Protecting the messiness.

The magnifying-glass company would have measured that out of existence. The AI dashboard would have flagged an engineer spending unplanned time on unplanned work. The productivity score would have dipped. And no one would have known that they’d just erased the most valuable thing that happened all quarter.

That’s the risk I’m asking you to take seriously. Not that AI visibility tools are useless—some of them are genuinely useful for reducing ceremony and translation costs. But the confidence they create can outrun the accuracy they provide. You can end up running your organization on a map that looks beautiful and precise without realizing the map has drifted away from the territory.

Protect the tiger teams. And don’t let a beautiful map replace the ground.

[![](https://substackcdn.com/image/fetch/$s_!V1qV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cefb14b-6e41-4244-a7b8-482fd22b8147_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!V1qV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1cefb14b-6e41-4244-a7b8-482fd22b8147_1024x1024.png)
