---
title: "Beyond the Miracle: What Changes When AI Becomes Inevitable"
author: "Nate Jones"
published: 2025-11-23
url: https://natesnewsletter.substack.com/p/executive-briefing-daily-ai-adoption
audience: everyone
scraped_at: 2026-01-05 19:11:14
---

AI works but we aren’t using it. (Not most of us, anyway.)

That was the inescapable conclusion I came to after digging through Benedict Evans’ 90 slide deck on AI this week. Ben Evans is a silicon valley legend, and when he talks industry leaders listen. The focus of his keynote was simple: AI is going to eat the world, and we’re mostly not ready.

Ben Evans has lived through four massive software waves, and his job at this point is to think carefully and clearly about the overall strategic landscape. He isn’t selling you or me a model or running a research lab. He’s focused on explaining how platform shifts change power, margins, and industry structure. That makes him useful right now: a sanity anchor in a world saturated with AI pitches and breathless product announcements.

Five themes emerged from Evans’ talk that I think deserve serious consideration:

- **From miracle to infrastructure**: AI is becoming like electricity—not a tool you choose to use, but an ambient capability that changes what “differentiation” even means
- **Path dependence is real**: Where you deploy AI first shapes what becomes possible later. The wrong beachhead traps you in local problem sets while competitors run circles around you
- **Buyer leverage is growing—if you design for it**: Model convergence means you can arbitrage vendors, but only if you’ve built the architecture to do it
- **AI is eating the org chart**: This isn’t just a tech rollout. It’s an organizational restructuring that most leaders haven’t started planning for
- **Daily AI use is good, but not enough**: Evans calls out the adoption issue, and I go farther: I think the adoption gap is THE metric to obsess over to drive the rest

This piece walks through each one, with the specific framing questions and tradeoffs I’ve found useful with clients navigating these decisions right now.

By the end, you’ll have saved a ton of time vs. going through the 90 slide deck and you’ll also have my take over the top, where I see AI adoption efforts working vs. stalling out, and where I think you get the most leverage through focused work.

And yes, I’ve written a prompt for you to help you unlock all this and get into a serious discussion around where adoption starts and ends in your business, and what you can do about it.

> ***This is an Executive Circle briefing**, a Sunday newsletter exclusively for Founding Tier Members. You can learn more via this [60 second video](https://youtu.be/KC3GkEnHR-8) explaining what’s in each tier, and you can change your plan [here](https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack). Enjoy, and back to regular programming Monday!*


## [Grab the AI Adoption & Organizational Transformation Prompt](https://www.notion.so/product-templates/In-Depth-Prompt-AI-Adoption-Organizational-Transformation-2b45a2ccb52680e1835cf2bac0bea39c?source=copy_link)

I’ve built a prompt that lets you have a strategic conversation with Claude or Gemini or ChatGPT about where your organization actually stands on these issues. It’s loaded with the full context from this piece—the adoption gap mechanics, beachhead selection criteria, multi-model architecture tradeoffs, org chart implications—and it’s wired to work the way a good advisor does: one question at a time, pushing you toward specifics, and flagging the common traps when it sees them. If you’re serious about figuring out where AI fits in your business and where the real leverage points are, have a conversation and take the next step to figure out where this plugs in with your specific team situation.

---


# Beyond the Miracle: What Changes When AI Becomes Inevitable

*The strategic question has flipped. It’s no longer “Does AI work?” It’s “Where do we still matter when it’s everywhere?”*

---

Benedict Evans just delivered a 90-slide keynote called “[AI Eats the World](https://www.ben-evans.com/presentations/).” Evans isn’t selling you a model or pitching a product roadmap—he’s a 20-year veteran of watching platform shifts unfold, from PCs to the web to smartphones to social, and his job is to explain how these transitions change power, margins, and industry structure.

What makes his presentation useful isn’t any single insight. It’s what it marks: a psychological tipping point in how leaders should frame AI. The conversation has flipped from capability questions to structural ones. Not “Should we adopt AI?” but “If AI becomes as inevitable as spreadsheets, what parts of our value chain degrade into table stakes?”

I want to use his presentation as a baseline map and extend it into territory he didn’t cover—the organizational consequences, the architectural choices, the traps I keep watching companies fall into.

Four themes emerged that deserve serious attention: the shift from miracle to utility, why your first AI beachhead shapes everything that follows, how to build buyer leverage as models commoditize, and why AI is restructuring org charts faster than most leaders realize.

This piece walks through each one, with the specific framing questions and tradeoffs I’ve found useful with clients navigating these decisions right now.

---


## The Question Changed This Year

We’re no longer asking whether AI works. That question is settled. The work now is architectural: Where does AI fit in your business? Where do you gain leverage? Where do you lose it? What does this do to your workflows, your org chart, and your long-term differentiation?

Evans opened his keynote with a useful observation that frames this well. AI is a moving target—Larry Tesler’s theorem holds that “AI is whatever doesn’t work yet.” Once it works, we stop calling it AI. Databases were AI once. Search was AI. Classical machine learning was AI. Large language models wear the label today, but they too will fade into “just software” once we’re accustomed to what they can do.

His second argument: AI follows the historical pattern of platform shifts. Mainframes gave way to PCs, PCs to the web, the web to smartphones. Each wave attracts massive capital, reshapes winners and losers, but rarely deletes the previous layers. We didn’t lose the laptop when we gained the phone. We won’t lose chat models when we gain agents. The transitions stack.

On spending, Evans pointed to the obvious: Big Tech is pouring roughly $400 billion into capex this year—comparable to the entire global telecoms industry. At the same time, more labs can train “good enough” models, which means the model itself is starting to look like a commodity input rather than a defensible product.

Finally, he noted the adoption gap. Millions have tried AI; far fewer use it daily in core workflows. ChatGPT has 800 million weekly active users, but only about 5% pay. Enterprises have pilots everywhere, production deployments almost nowhere. The blockers aren’t technical—they’re motivation, skill gaps, governance, and integration.

That’s Evans’ map. But before I get to what it means strategically, it’s worth being precise about why this moment feels different—because your board needs a crisp answer to that question, and “AI is moving fast” isn’t one.

---


## **The Adoption Gap Is the Leading Indicator**

Evans flags the adoption gap—millions have tried AI, far fewer use it daily—but I think he undersells it. This isn’t one metric among many. It’s the metric that predicts whether everything else works.

Here’s the mechanism. AI fluency compounds. The person who uses Claude or ChatGPT daily for real work develops intuitions that occasional users never build. They learn what to delegate and what to keep. They develop prompting instincts. They notice when the model is confidently wrong. They spot use cases that wouldn’t occur to someone who only saw a demo. They get faster, and they teach colleagues.

The person who tried it once and went back to their old workflow learns nothing. They’re exactly where they were, except now they have an opinion about AI based on a single interaction.

I’ll go farther, because I think this is the real situation in most orgs: the person who uses it daily but mostly uses it for shallow use-cases like emails is just about as worse off as the person who never uses it. They have no real experience with AI. It’s just a magic email calculator for them. And so they never gain the new skill of learning about AI as a collaborator, as an intelligent partner. And they (and the organization) lose out on the leverage that skill gain can create.

Now multiply that across an organization. The company where 200 people use AI daily as deeply as they can has 200 people building intuitions, discovering edge cases, identifying workflows worth automating. The company where 2,000 people “have access” but 50 actually use it has 50 people learning and 1,950 people waiting for someone else to figure it out. Guess who is gonna win here.

This is why adoption is the leading indicator, but at the same time adoption is deceptive. You can’t execute on any of the other themes—beachhead selection, multi-model architecture, org redesign—without people who actually know how to work with AI. Strategy requires operators. If your people aren’t building the muscle, you’re not building capability. You’re running experiments that won’t scale.

Most companies measure AI success wrong—that’s the deceptive part. They track pilot completion, executive sponsorship, use case count, projected ROI. Sometimes they even track adoption rate as a surface metric. These are all activity metrics. They tell you things are happening. They don’t tell you whether capability is accumulating.

The companies I’ve seen actually build AI muscle measure something simpler: daily active usage ***in core workflows***. That last part is really critical. Not “logged in once.” Not “generated a summary for a meeting.” Not even “wrote emails daily.” I mean daily use, in the work that matters, by people whose judgment you trust. ***Daily deep work in AI***. That’s the signal. Of course, that requires defining core workflows and what AI involvement in those workflows looks like, and that’s often quite difficult.

What does good look like? A product manager who drafts every PRD with AI assistance and has learned exactly where the model helps versus where it hallucinates requirements—and has learned when he or she can dump the PRD entirely and use AI to craft a prototype instead. Or an engineer who uses AI for code review on every PR and can articulate which classes of bugs it catches and which it misses. Or a sales lead who runs every customer call through an AI summarizer and has built a personal prompt library for different call types. These people aren’t just using AI. They’re developing craft.

What does bad look like? An innovation team running sixteen pilots with no shared learning. An executive who demos ChatGPT in all-hands meetings but whose direct reports still do everything manually. A “center of excellence” that produces governance documents while actual workers ignore the tools entirely. Activity without accumulation.

The practical implication: stop celebrating pilots and start measuring deep, sticky usage adoption curves. For any AI initiative, ask: How many people are using this in deep ways daily? Is that number growing? What’s blocking the people who aren’t? Those questions will tell you more about your AI future than any ROI projection.

---


## Gaining Perspective: What Actually Shifted

Zooming back out, we know AI is moving fast. What we often struggle to articulate is why it feels categorically different now vs. even last year. Let me lay out what’s shifted this year: four shifts happened in rapid succession, and together they changed the strategic landscape.

The first was the collapse of hard problems. Visual reasoning, semantic video understanding, long-context audio processing, and complex long-running inference—these were “years away” in 2023. They’re solved now. Nano Banana Pro cracked visual reasoning benchmarks that seemed unsolvable eighteen months ago. Meta’s SAM 3 made semantic video search a plug-and-play utility. Gemini 3 demonstrated fluid multimodal processing across audio, video, and extended context windows. GPT 5.1 Pro does novel science and can be trusted with multi-hundred page legal inference problems. Problems that researchers assumed would require architectural breakthroughs turned out to require scale and iteration.

The second shift was OpenAI losing the undisputed crown. The top five or six models are now within striking distance of each other on core tasks, with Google indisputably ahead on most use-cases right now with Gemini 3. Assume more stiff competition to come. History tells us what happens next: when the base technology converges, value moves up the stack. The winners stop being the ones with the best engine and start being the ones with the best product, the best workflow integration, the best understanding of specific domains. This is the moment when building on top of models matters more than building the models themselves.

The third shift has been the miniaturization of intelligence. Useful small models—Nano-class, Mistral Small, Qwen variants—are now capable enough for a surprising range of production tasks. This is the “Intel Inside” moment for reasoning. Intelligence can run locally, on-device, at the edge, without calling home to expensive cloud APIs. The cost floor for useful AI dropped by an order of magnitude.

The fourth shift has been the emergence of agent interfaces. This one is easy to underestimate because it looks like a product feature or an experiment rather than a paradigm change. But there’s a fundamental difference between commanding a tool and delegating to an agent. Chat interfaces let you ask questions and get answers. Agent interfaces let you describe outcomes and have work done on your behalf. The move from “generate this email” to “handle my inbox according to these rules” changes the relationship between human and machine from conversation to coordination. It’s here today, but the future is distributed unequally and not everyone has seen it.

These four shifts together explain why the conversation has changed. It’s not that AI got incrementally better. Several previously-separate capabilities converged at once, and the combination unlocked use cases that weren’t possible six months ago.

---


## From Miracle to Infrastructure

I think Evans’ talk actually marks a psychological tipping point in how leaders frame AI.

AI is becoming like spreadsheets—inevitable, ambient, embedded into every serious workflow. Once you accept that framing, different questions emerge. Not “Should we adopt AI?” but “If AI is as inevitable as spreadsheets became, what parts of our value chain degrade into ‘just a feature’?”

That’s a new competitive lens and it poses serious questions with uncomfortable answers. The risk isn’t missing the AI moment. The risk is continuing to act as if this is tunable R&D, something you can wait out or treat as optional. It isn’t. This is infrastructure now. And more of the value chain than you think is at risk. (And yes, that piece at risk includes table stakes AI features that are now everywhere.)

A thought experiment will help clarify the stakes. Imagine AI as electricity—not as a tool you choose to use, but as an ambient capability that powers everything. Imagine it running through every nook and cranny of the business. In that world, which parts of your business get powered on immediately because the data is ready, the tools are there, the people have the capability? Which parts become undifferentiated table stakes that everyone has because there’s no real difference in data or toolsets? Which products face pricing pressure because the cost of producing them collapses to near zero? And which parts actually gain value—because human judgment, human creativity, human relationship-building become rarer and therefore more premium?

The answers vary by industry and company. But the exercise forces a useful reframe. You stop asking “Where should we use AI?” and start asking “What do we do when AI is everywhere, including in our competitors’ hands?”

Often the capabilities we thought were differentiators—sophisticated analysis, personalized recommendations, rapid content production—turn out to be features that AI commoditizes. The capabilities we undervalued because we could not define them—taste, judgment under ambiguity, the ability to hold context across a long relationship—turn out to be what’s left when the machines can do everything else cheap.

---


## Your First Beachhead Shapes Everything That Follows

Evans talked about the adoption gap, and he’s right that usage is lumpy. But there’s a deeper problem hiding underneath: where you choose to adopt AI first shapes what becomes possible later.

Think about what spreadsheets did. The first teams that adopted them weren’t just more efficient—they reorganized how information moved through the company. They could model scenarios, own numbers, self-serve. The tool changed not just individual productivity but the flow of decisions across the organization.

LLMs and agents are doing the same thing. You drop AI into one or two workflows. Those workflows change how information is produced and consumed. That changes which other workflows become possible or necessary. The sequence matters.

Let’s play this out. What if you have AI work with document summarization—a safe, visible, easy-to-demo use case. Leadership loves it. But six months in, you realized that summarization sits at the end of workflows, not the beginning. It does not generate data that improves other processes. It doesn’t create feedback loops. It is a dead end that happened to look impressive in steering committee meetings.

What if instead we try agent-assisted customer onboarding—a messier problem, harder to demo, but sitting at a junction where data flows into sales, compliance, and product. Every improvement compounds. By the time everyone around you notices what you’re working on, you’re probably a year ahead and gaining fast.

The wrong beachhead for AI traps you in a local maxima (stats language for a suboptimal space you can’t escape easily). If your first experiments are all “summarize this document,” you’ll never discover the compounding benefit of agent-assisted engineering triage, or multi-step coordination loops that weren’t possible before. You’ll have proven that AI can do something useful and simultaneously foreclosed the most valuable applications.

So how do you choose? The workflows worth starting with share several characteristics: high information density with lots of documents, tickets, or messages flowing through; high coordination cost with frequent handoffs and obvious bottlenecks; clear decision criteria but inconsistent execution—situations where you know what the right answer looks like, but humans struggle to apply it reliably; downstream dependencies where the output affects many other teams; data exhaust that improves the system over time; and measurable before-and-after impact on cost, latency, throughput, or error rate.

Workflows with most of these characteristics are good beachheads. Workflows with few of them are traps—they’ll generate impressive demos and underwhelming ROI.

The practical implication: don’t let your AI pilots be driven by which team has the most enthusiasm or the loudest executive sponsor. Drive them by which workflows sit at the junctions of your organization’s information flow.

---


## Build Buyer Leverage, Not Vendor Religion

Evans’ commoditization argument has a second-order effect almost no one is discussing. As models converge in quality, your power as a buyer increases—but only if you design for it.

Here’s what’s actually true today. Open-source and small models are “good enough” for a surprising range of tasks. Vision models in the Segment Anything lineage make once-exotic perception tasks feel like another API call. At the same time, frontier models from Anthropic, Google, and OpenAI remain decisively ahead on complex reasoning and general flexibility. Multiple studies suggest that Chinese open-source models, while proliferating rapidly, are often distilled from US foundation models and lack the same flexible general intelligence.

So two realities coexist: baseline intelligence is abundant, and cutting-edge intelligence is scarce.

If you take Evans seriously, the long-term equilibrium looks like a buyers’ paradise. You treat models as swappable components behind an internal interface. You route workloads to different models based on cost, latency, data sensitivity, jurisdiction, and task type. You benchmark frontier versus small models regularly. You invest in abstraction layers, evaluation infrastructure, and clear policies on what data can touch which provider.

I get a lot of nods these days around multi-model architecture but I’m not sure most folks have thought through what it actually enables. Here’s what model arbitrage looks like in practice: you use a small, fast model for initial retrieval and classification, route to a frontier model only when complex reasoning is required, then use a small model again for formatting and output. That’s a three-stage pipeline where the expensive model only handles the hard middle step.

Now, if you’re smart you don’t reach for the model lever as your first stop when you have a speed or quality problem. So often those challenges are solved by architecture, tools, or memory instead of models. Again, few leaders have the AI fluency to feel confident talking tradeoffs there and so model choice is often the scapegoat for a deeper problem.

No one model pattern is universally right. The point is that multi-model isn’t an architecture choice—it’s a margin strategy that allows you to do real work. It’s how you extract maximum value at minimum cost. And you can only execute it if you’ve built the abstraction layers and routing logic in advance, and in practice you only get it done right with a solid technical team and leadership that understands the point of the exercise.

Don’t ask “Which vendor should we pick?” Ask “Are we architecting a system that can arbitrage models over time?” The companies that do this will have negotiating leverage and cost flexibility. The companies that don’t will be locked into choices they may regret.

---


## AI Is Eating the Org Chart

Evans focused on tech cycles, but if you extend his logic, the organizational consequences are profound.

Spreadsheets didn’t just change software. They changed who needed to talk to whom, which roles became bottlenecks, which functions gained political power. Finance and operations rose because they owned the numbers. Cloud didn’t just move servers off premises. It shifted power from central IT to product and engineering, accelerating the pace at which teams could experiment.

AI will do the same for roles built around coordination, synthesis, and formatting—versus roles built around judgment, orchestration, and constraint-setting.

Andrej Karpathy made a useful observation recently: we’re in “first contact” with a non-human intelligence. Not in a science fiction sense, but in a workflow sense. A model capable of reading your email, Slack, tickets, dashboards, and documents—and proposing action—is effectively an informal chief of staff for every knowledge worker. That’s not just an individual productivity tool. It’s an organizational restructuring.

Three categories of roles will expand.

First: constraint-setters, the people who define the boundaries within which agents operate. What are the rules? What are the exceptions? When should the system escalate to a human? This is a design function that barely exists today.

Second: workflow architects, people who understand how information flows through the organization and can redesign those flows around AI capabilities.

Third: high-context decision-makers, people whose judgment depends on synthesizing information that can’t easily be written down. The harder it is to specify your decision criteria, the more valuable you become.

I can think of another three categories that will compress.

Coordinators—project managers, program managers, anyone whose primary job is tracking status and moving information between teams. Agents will do this.

Synthesis roles—analysts whose job is to compile information rather than interpret it. If your job is “pull data from these six sources and put it in a slide,” you’re competing with a machine that does it faster and cheaper.

And parallel managers—roles defined primarily by managing information flow between organizational units. When the information flows automatically, the manager of the flow becomes redundant.

One role will emerge that doesn’t really exist today: agent operations. Someone needs to monitor, validate, and optimize your “digital teammates.” Someone needs to notice when the agent is making systematic errors, when its instructions have drifted out of date, when a new capability means it should be handling a task it wasn’t handling before. Organizations that figure this out early will have a significant advantage.

If you only think of AI as a tool rollout, you’ll miss that you’re simultaneously doing an org design change. Your span of control assumptions, your management layers, your hiring plans for 2025 through 2027—all of it will need to adapt faster than in previous cycles.

This isn’t primarily about layoffs. It’s about restructuring decision flow inside the company. Leaders who get this wrong will wonder why their AI investments aren’t paying off. The answer will be that they changed the tools without changing the architecture of work.

---


## Filtering the Weekly Avalanche

The release cycle is relentless now. Every week brings new model announcements, new benchmarks, new agent frameworks, new claims of breakthrough capability. The risk is overreaction—or paralysis disguised as prudence.

Evans’ framework, combined with these themes, offers a filter. When something new drops, ask four questions:

1. Does this change our adoption path?
2. Does this shift our leverage with vendors?
3. Does this alter how information flows inside the organization?
4. Does this require a change to our org design or hiring plan?

Gemini 3’s release changed architecture decisions. Its long context windows and multimodal capabilities opened workflows that weren’t practical before—processing entire codebases, analyzing hours of video, reasoning across document sets. If you’re building AI infrastructure, that deserved attention.

When a mid-tier model announces a benchmark improvement from 78% to 81% on some leaderboard, that’s noise. It doesn’t change your vendor leverage. It doesn’t change your workflows. It doesn’t change your org. Ignore it.

When open-weight models achieve near-frontier performance on specific tasks, that changes your vendor leverage. You now have negotiating power you didn’t have before. You can credibly threaten to move workloads in-house. Worth tracking.

When a new capability emerges—like SAM 3’s semantic video understanding—ask whether it opens a workflow that was impossible before. Can you now do automated quality assurance on video content? Can you search hours of meeting recordings by topic? Can you extract structured data from video that previously required human review? If yes, that changes your workflow possibilities. If it’s just a faster or cheaper version of something you could already do, it’s less important.

Most announcements will fail all four tests. That’s fine. The goal isn’t to react to everything. It’s to build the judgment to know which developments actually change your operating reality.

---


## The Scarce Skill

The hardest leadership skill right now is deliberate synthesis. Everyone is reactive. Everyone is overwhelmed. The feed never stops.

Your job is to step back regularly and ask: What does this actually change? Where are we exposed? Where could we gain durable advantage? What needs to shift in our architecture—not just our tooling?

I’ve started recommending a quarterly rhythm to the executives I work with. Set aside real time—not ten minutes between meetings—to review what’s happened in AI over the past quarter. Not every announcement, but the ones that passed the four-question filter. What patterns do you see? What capabilities have converged? What’s now possible that wasn’t before?

Then decide. Based on that synthesis, what strategic decisions need to be made? Are you in the right beachheads? Is your multi-model architecture keeping up? Are your vendor relationships still serving you? If changes are needed, design them explicitly—both technical architecture and organizational architecture. You can’t do one without the other.

Pick one beachhead, one workflow, one pilot that tests your thesis. Make it concrete. After deployment, evaluate what you learned about your leverage, your architecture, your org design. Then repeat.

This loop matters because synthesis is the scarce skill. Not information—there’s plenty of that. Not speed—you can’t outrun the release cycle. The scarce skill is the ability to step back, see structure, and make decisions that don’t become obsolete in six months.

The companies that win this wave will be the ones whose leaders can ignore noise, see structure, and design for inevitability. Not the ones who reacted fastest to every announcement. The ones who understood what the announcements meant—and what they didn’t.

---


## [Read the full 90 slide deck](https://www.ben-evans.com/presentations/)

[![](https://substackcdn.com/image/fetch/$s_!Bm_7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dab5719-9185-4e3f-9888-b059093d696c_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!Bm_7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4dab5719-9185-4e3f-9888-b059093d696c_1024x1024.png)
