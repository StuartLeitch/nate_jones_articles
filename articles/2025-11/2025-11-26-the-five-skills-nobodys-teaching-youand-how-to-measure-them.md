---
title: "The Five Skills Nobody’s Teaching You—And How to Measure Them"
author: "Nate Jones"
published: 2025-11-26
url: https://natesnewsletter.substack.com/p/nobody-could-measure-ai-skills-so
audience: everyone
scraped_at: 2026-01-05 19:11:04
---

The bad news is you don’t really know how good you are at AI.

The good news is no one else really knows either.

I don’t know about you, but that won’t stop folks in my family from sharing their *opinions* on AI fluency around the Thanksgiving table.

[![](https://substackcdn.com/image/fetch/$s_!O1Xd!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536224d0-2e0f-4c06-bdac-14dd9f1329f8_1376x768.jpeg)](https://substackcdn.com/image/fetch/$s_!O1Xd!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F536224d0-2e0f-4c06-bdac-14dd9f1329f8_1376x768.jpeg)

The fact is we’re long on opinions and short on useful, objective AI skill assessments.

Until now.

I know everyone’s LinkedIn says “proficient in AI” or “AI certified in X tool.”

The phrase has become meaningless. Hiring managers can’t tell who’s competent and who’s just good at ChatGPT + buzzwords. Job seekers can’t prove what they actually know. And so you and I—even if we use AI every single day—have no reliable way to gauge where we’re at or show what we know.

And that’s a big problem, because this is the most monetizable skill of the decade, and it keeps changing fast. Someone who can keep up (even kinda) with the model drops is going to be 10x more useful in most organizations than someone who last checked their ChatGPT model number way back in the spring. But they both might be “ChatGPT certified” or whatever.

It’s a massive problem, and it translates into a huge career advantage that is MIA, because it’s impossible to train for, show, or measure.

Don’t get me wrong: the difference on the ground is real. The gap between a competent AI user and an advanced one isn’t subtle when you watch them work.

I am estimating *very* conservatively based on working with hundreds of folks over the last year or so: an advanced AI user gives you an extra ten to fifteen hours of productive output per week, minimum. I’ve seen cases where it’s much higher.

That’s not hype—that’s the gap between having documented workflows with specific prompt patterns versus winging it every time. False confidence is expensive. You think you’re leveraging AI effectively while leaving massive value on the table, and you don’t even know it because there’s never been a way to check.

Meanwhile, most AI courses are teaching you the wrong thing. OpenAI certifications. Gemini badges. Claude training modules. Every one of these organizations does good work teaching you the tool. And that’s the problem.

But tool certification is not AI competency—especially in a world where Gemini 3, Opus 4.5, and ChatGPT 5.1 all launched in the last two months. Betting your career on mastering one model is a losing strategy. The skills that matter are the ones that transfer across models and don’t expire when the next release drops.

Some readers may remember [I first wrote about this back in October](https://natesnewsletter.substack.com/p/the-ai-fluency-assessment-for-everyone).

So TLDR; I built a framework for measuring AI fluency across five dimensions that sit above any particular tool—strategy, prompting, workflow integration, critical evaluation, and ethics.

I published the assessment prompts, the scoring system, and a 90-day development plan generator (still there in the post if you want them!)

The response was overwhelming. Hundreds of people took the test. They wanted to know where they stood. They wanted to track their progress over time. They wanted something that felt real, not another certificate to staple to the wall.

One reader went further: Jonathan Edwards—a filmmaker out of northeastern Pennsylvania who got hooked on AI while building workflows for video editing—saw that post and thought: this should be an app.

So Jonathan reached out, and we spent a few weeks going back and forth asynchronously, refining the vision, pressure-testing the assessment, building something that could actually deliver on the promise.

So we built it. It’s called [AI Cred](https://www.aicred.ai/), and it’s live today.

Why AI Cred? Pretty simple:

- AI Cred lets you build an AI profile you can share with an externally validated leaderboard, score, and assessment of your AI competencies
- AI Cred goes farther than the original post in some important ways:

  - I took the time to curate and add hundreds of AI resources that the model can directly pull from when recommending custom learning for you on AI
  - AI Cred also has more power than a prompt: it can course-correct and adjust its custom recommendations over time as you grow and change in skills
  - And unlike the original, AI Cred can track your profile and AI skill growth over time, so you can show your growth to others
  - AI Cred also has shareable and searchable profiles, so you can share your own skill profile and search for others!
  - AI Cred generates FREE additional custom lesson plans for you as you grow and iterate your skills based on your initial test result
  - AI Cred also stops the bragging rights problem on LinkedIn—the AI-powered summary in your profile reflects your actual test results, and that means others can trust it’s real
  - And by popular request, AI Cred has a leaderboard, so you can actually compete and earn a place of pride right on the homepage

Pretty cool right?! But that’s not all, we have more in here…

- **My take on the five AI skills nobody’s teaching you**—strategy, prompting above the tool level, workflow integration, critical evaluation, and ethics as product design—and why they matter more than any certification
- **The story of how we built AI Cred**—yes, we built it with zero meetings and lots of AI-powered tools, so you’ll get some honest takes on Codex vs. Claude code in here for an actual AI-powered application
- **A deep dive on measurement philosophy**—why I crafted the assessment the way I did, how I’m thinking about keeping it up to date (including bringing in Opus 4.5 this week), and how I’ll continue to improve it over time
- **A chance to share your feedback**—AI Cred is a labor of love, and we’re opening up a work slack channel to work directly with us to give feedback and prioritize what matters for all of you

Plus (of course) there’s a deep discount for the Substack community—40% off for a test, and 60% off for a pack of 3 tests so you can track your learning over time.

AI Cred literally grew out of my Substack chat, and our first alpha testers were in the chat too, so we want you all to get a special welcome—scroll on down for that discount code!

I think you get the idea. This is the simplest way I can come up with to solve a major problem in the industry: how to tell how good you are at AI, and how to tell others about it. If you’re interested in knowing where you stand, jump in. There’s room on the leaderboard for you!


## [Get the AI Cred Assessment + Custom Learning Recommendation](https://www.aicred.ai/)

The assessment takes about 20-30 minutes and works like a real conversation—not a multiple-choice quiz you can game. You’ll get a quantified fluency score across all five competencies, a brutally honest breakdown of where you’re strong and where you’re fooling yourself, and a personalized learning path built specifically around your gaps. The evaluation costs one credit; all the training modules that follow are free.

You guys here on Substack get a steep discount:

- 40% off for 1 test: `BEAT8POINT9`
- 60% off for 3 tests: `341`
- 60% off for 12 tests: `STOCKUP`

---


## [See the Substack Post That Launched the App](https://natesnewsletter.substack.com/p/the-ai-fluency-assessment-for-everyone)

This is the post where I first introduced the AI Fluency Assessment and the concept of fluency velocity—the idea that how fast you’re climbing matters more than your current score. We kept that idea as core to [AI Cred](https://www.aicred.ai/), along with the idea of testing across a range of complex, tool-agnostic skills. I built a 10-point scale we’re still using today, and yes if you’re curious you can just hit the link, load the prompt and DIY.

The core insight: the gap between a 5 and a 7 alone is worth 10-15 hours of productive output per week, and most people have no idea where they actually stand.

Word to the wise: I do not grade on a curve, and this test is hard. You should be very proud of a 5 score!

---


# The Five Skills Nobody’s Teaching You—And How to Measure Them

Most AI courses are teaching you the wrong thing.

I want to start with a confession: I’ve spent the last year watching people get certified in AI, and most of them are learning skills that will expire in six months. They’re getting OpenAI certifications, Gemini badges, Claude training modules—and every one of these organizations does good work on teaching you the tool. But here’s the thing we need to stop pretending we don’t see: tool certification is not AI competency.

We’re living in a multi-model world now. In the last two months alone, we’ve seen Gemini 3 launch, Opus 4.5 drop, ChatGPT 5.1 go live, and Grok 4.1 appear out of nowhere. Every week brings a new model. If you’ve gathered anything from what I’ve been talking about lately, it’s that betting your career on mastering one interface is a losing strategy.

So the question becomes: what would it mean to develop AI fluency that actually scales? Fluency that grows with you as the models proliferate and evolve? That’s what I want to dig into today, because I don’t know very many courses that think this way. And the ones that do tend to be job-family specific—AI for engineering, AI for product management, AI for marketing—without recognizing that there are foundational skills above the level of any particular job that we all need regardless of what we do.

I’m going to give you five of them. And then I’m going to show you a tool that one of my Substack readers built—with my permission and using a prompt set from a very popular post I wrote—that actually measures where you stand on each of these dimensions. It’s called AI Cred, and today we’re launching it.

---


## The First Skill: Strategy Is Not Just For Executives Anymore

Here’s a belief I used to hold that I no longer think is true: that AI strategy belongs in the C-suite.

It doesn’t. AI puts artificial intelligence as a team member on every team in your organization. That means every person on every team needs the strategic insight to deploy that intelligence correctly. You need to know what the right product is that moves your workflow forward. If you’re in any of the building spaces—vibe coding, engineering, whatever it is—you have to have a strategic understanding of the market and how AI fits into it.

This is no longer something you can outsource to executives. Strategy is now a core individual competency, and if you don’t have it, you’re going to make expensive mistakes about where and how to deploy AI in your work.

---


## The Second Skill: Prompting Above The Level Of The Tool

Prompting is the skill everyone talks about, but almost nobody teaches it correctly.

Most prompting education focuses on task-specific patterns. Here’s how you prompt for a summary. Here’s how you prompt for code. And that’s fine as far as it goes, but it misses the deeper layer: how do you think about prompting as it shifts across different models and contexts?

How do you prompt Gemini differently from Claude? How do you prompt for a deck differently than a doc? These are skills that sit above any particular tool. You can use any prompting tool—I released one this week called Hey Presto—but you can use any of the hundred tools out there well or badly. What determines the outcome is your skill set, not which wrapper you’re using.

We haven’t developed a good pedagogy for this yet. We’re still so early that we’re treating prompting as a collection of recipes rather than a coherent skill with underlying principles. That needs to change.

---


## The Third Skill: Integration Into Workflows

I’ve said this so many times I’m almost tired of saying it: AI is useless if it lives off to the side of your work.

The question is what it means to have AI deeply connected and integrated into your workflows. How do you design workflows that are AI-native from the start? This is a learnable skill, but almost nobody teaches it explicitly. People teach you how to use tools. They don’t teach you how to redesign the way you work so that those tools are structurally embedded rather than bolt-on additions.

Integration design is its own discipline. You have to understand handoff points, feedback loops, where humans need to remain in control, and where automation should be seamless. This is workflow architecture, and most people are just winging it.

---


## The Fourth Skill: Critical Evaluation (Also Known As Taste)

Here’s something I was doing today as a fun test between Gemini 3, ChatGPT 5.1 Pro, and Claude. I asked them all to write me a creative story using the same prompt. Why a creative story? Because one of the interesting bars for LLMs, even in a business context, is how they form narrative.

I got three different stories back. And then I had to actually read them. I had to use my judgment to figure out which one was highest quality. I’m still digesting it—I haven’t landed on a conclusion yet—but the point is that this requires taste. It requires the ability to look at AI output and say with confidence: this is good, and this is garbage.

That skill extends well beyond creative writing. You need it for numeric data, for decks, for docs, for code, for everything. If you can only prompt but you can’t evaluate output, that’s dangerous. You’ll ship mediocre work because you can’t tell the difference.

---


## The Fifth Skill: Ethics As Product Design

People roll their eyes when I bring up ethics. They think it’s for ethics officers and academic conferences.

Let me give you a concrete example that shows why that’s wrong. Nano Banana Pro can now make passport pictures. The simplest ethical question is “don’t fake a passport”—that’s easy. But there’s a much harder question hiding underneath: how do you design systems with guardrails so that your product isn’t vulnerable to a model upgrade that suddenly enables something you didn’t anticipate?

That’s system design and ethics intertwined. You also have to think about security policies—maybe you need physical asset verification that can’t be spoofed by a generated photo. And you have to think about trust: how do you build trust in your product experiences so that people believe you’re one of the good guys when your underlying model can be used in a thousand different ways?

I’m firmly convinced that the question of how LLMs ought to act—which we traditionally call ethics—is really a question of product design to build trust. We’re all either designing products that use AI or using products in ways that either build trust with others or don’t. The way you use AI to build reports for sales or white papers can be trust-building or it can erode trust. That falls under the ethics banner whether you like the word or not.

---


## The Problem With Partial Competence

So I’ve painted a picture across five dimensions: strategy, prompting, integration into workflows, critical evaluation, and ethics.

Here’s why this matters: we constantly misjudge ourselves when we don’t understand the full skill set we need to develop. We tend to over-index on one dimension while ignoring the others. And I’ll tell you frankly, of these five, most of what I hear about is prompting. That’s an impoverished view of the rich LLM skill set we need to build.

Most AI-fluent users tend to be strong in only one or two pillars and weak in the rest. If you don’t measure across all five, you don’t get a real picture of how someone thinks, how they work, how they reason through ambiguity.

Real AI work blends all of these skills together. The judgment, the synthesis, the workflow design, the rapid learning, the ability to interrogate models—they all combine in application on a particular task to deliver value. If you’re great at strategy but can’t operationalize a workflow, it’s not going to work out. If you can evaluate output but you can’t integrate it into how work actually happens, you’re leaving value on the table.

---


## The Measurement Problem

Here’s a question that’s been nagging at me: how do you actually know where you stand?

You use AI daily. You’ve automated some things. You can prompt your way through most tasks. But are you measurably good at this? Or are you just average?

The brutal truth is that most people have no idea. And neither does anyone else, because until now there’s been no credible way to measure individual AI fluency. Everyone’s LinkedIn says “proficient in AI.” The phrase has become meaningless. Hiring managers can’t tell who’s competent and who’s just good at buzzwords.

The difference between a competent AI user and an advanced one isn’t subtle. It’s worth ten to fifteen hours of productive output per week. That’s measurable. But without objective measurement, most people overestimate their skills. And false confidence is expensive—you think you’re leveraging AI effectively while leaving massive value on the table, and you don’t even know it because there’s been no way to check.

---


## [AI Cred](https://www.aicred.ai/): What It Is And How It Works

This is where I want to point you to a tool that I did not build myself.

One of my Substack readers, Jonathan Edwards, is a filmmaker who runs a production company out of northeastern Pennsylvania. He got hooked on AI while building workflows for video editing, couldn’t stop talking about it, and eventually connected with me through the newsletter. When I published a post about evaluating AI fluency with a particular prompt set, he saw it and immediately thought: this could be an app.

He built it. It’s called AI Cred, and it’s live today.

What makes it different from everything else out there is fundamental to the design. It’s not a multiple-choice quiz. You can’t A/B/C/D your way through it. The assessment is a dynamic conversation with an AI evaluator that pushes back on your answers, challenges your assumptions, and asks you to demonstrate real-world problem-solving. It requires you to actually think.

When you complete the assessment, you get a quantified AI Fluency Score on a 0-10 scale, percentile-ranked against other users. You get a breakdown across the five competencies—you might score a 9 in prompting and a 3 in ethics. That asymmetry is exactly the point. It tells you where to focus.

There’s also something we call a Velocity Rating that measures how fast you learn during the assessment itself. Static knowledge expires. AI changes weekly. Your ability to adapt to new information in real-time matters more than what you already knew walking in.

---


## The Brutal Truth Feature

The assessment includes a section called The Brutal Truth. It’s exactly what it sounds like.

Jonathan showed me his own results during our conversation. His score was 8.9—the highest on the leaderboard at the time—but the Brutal Truth section wasn’t pulling punches. It told him that he’d mastered personal AI fluency and built impressive systems, but he’s still operating as a solo practitioner. His knowledge transfer has reached maybe twelve people through one-on-one work, and that’s not scale.

It told him his assessment app is a step in the right direction but it’s still MVP with no revenue validation. It told him his technical depth is a genuine asset but also a blind spot.

This is not designed to make you feel good. The goal is accurate feedback so you can actually improve. We don’t sugarcoat anything, because growth requires discomfort.

---


## The Personalized Learning Path

Once you see your results, AI Cred generates a custom training plan based on your specific gaps.

I want to be clear about why this matters to me. I’ve been asked for years to just point people at a course so they can skill up. But because the answer is genuinely custom—because it involves this multi-dimensional skill set—I can’t in good conscience point you at one tool and say “learn that tool.” I can’t just recommend one particular course and say that’ll do it.

What I want to do is point you at relevant resources that are tied to your particular gaps. That’s what the learning path does. So AI Cred looks at where you’re weak, it generates hands-on exercises tied to those weaknesses, and it pulls from a curated set of resources that I’ve spent considerable time selecting.

Each module builds on the previous one. For Jonathan, whose growth challenge was scaling his impact to others, the first module’s exercise was designing and executing a two-hour workshop for three to five non-technical users teaching one core AI workflow. If your weakness is something else, you get something else. The exercises are graded, and your responses feed back into the system.

When you finish the learning path, you take a reassessment that incorporates all the context from your first conversation and all your training modules. Then it regenerates a new plan for FREE based on where you’ve improved and where you still need work. The goal is iterative growth you can measure over time, not a certificate to staple to the wall.

---


## The Scoring Is Deliberately Hard

Let me give you a sense of the calibration.

Jonathan’s wife scored 5.5, and he was genuinely shocked. A casual ChatGPT user typically scores around 1.4 to 2. Someone who’s actively trying lands around 3. A 5.5 is a strong result for someone who uses these tools daily without having gone deep.

The top of the leaderboard right now is 8.9. Nobody has hit 9. Nobody might hit 9 for a while. This assessment is hard on purpose, because when I wrote the fluency algorithm, I didn’t care about grade inflation. I know it’s a thing. We’re not doing it here. If you earn your way to a score, it’s a real score.

The practical effect is that you have room to grow. You can retake the assessment in a month or two and see whether the resources you dug into actually changed the way you work and think. Because that’s the other piece I keep seeing: so often when we talk about AI fluency, it’s a piece of paper people staple to the wall. It doesn’t get into their heads and hands and onto their keyboards. This measures whether that outcome is actually happening.

---


## How We Built It: Claude, Codex, and the Limits of Each

I asked Jonathan to walk me through how he led the way on actually putting this together, because I think there’s a lesson in the process.

He started with a prototype in Lovable, which is one of the vibe-coding platforms. That got him to a working version fast. Then I challenged him—I wanted it in Next.js, which meant refactoring the whole thing out of Vue. So he did that, which meant the production version has no Lovable heritage left.

For the core development work, he used Claude Code primarily, with Codex for code review. His take on the tool comparison is worth repeating: Claude executes well and is pleasant to work with, but occasionally makes mistakes that need catching. Codex is phenomenal at reviewing what Claude did—it’ll go through the code, help him plan, find all the faults in the plans—but Codex is lazy about execution and straight up ignores you when you ask it to use tools unless you prompt for it explicitly every single time.

So his workflow became: plan with Codex, execute with Claude, review the execution with Codex again. Different tools for different strengths.

He tried Gemini 3 for a few days and said it blew him away initially. Then Opus 4.5 came out and he hasn’t thought about Gemini since. But he did have one observation that stuck with me: watching Gemini’s thought chain is unsettling. It starts off speaking in the first person, repeating your prompt back to you, and then it’ll say the wrong thing. And it’ll dig into saying the wrong thing. You’re watching it thinking oh no, oh no, and you’re about to hit cancel—and then right at that moment it self-corrects and says “no, I’m wrong, I should be doing this.”

I think that happens with AI more than we realize. Gemini made a bold choice to expose it. With Claude, I often see similar temporary misinterpretations in the streaming thought chain—it’ll say something like “the user’s under-described prompt” when I’ve given it fifty lines of context. It’s thinking before it’s actually looked at what you gave it. Eventually it gets there.

---


## The Social Layer: Leaderboards And Profiles

During the prototype phase, Jonathan ran a test with about a hundred people from my Substack community. And they were all dying to see how they compared to each other.

So the production version has a leaderboard. The top ten people show up on the home page. You can search for specific users. Each person gets a profile page where you can add your social links and a bio, and there’s an AI summary you can’t tamper with that lets the AI describe your capabilities so you don’t have to brag about yourself.

---


## The Build Philosophy: Fewer Meetings, More Shipping

One thing I want to highlight is that the first time Jonathan and I actually had a meeting—a real, synchronous conversation—was the day we recorded this launch video. Everything else happened asynchronously. Document after document, me saying “yeah this is fine, I hate this, that’s good,” him iterating, me eventually circling back with a response.

This is one of the cool things about building in the age of AI. You can ship actual products with collaborators you’ve barely spoken to in real-time, because the asynchronous workflows are so much more efficient now. The tools enable a kind of collaboration that would have required constant meetings five years ago.

---


## The Community Piece

Because [AI Cred](https://www.aicred.ai/) grew out of the Substack community, we want to give back to it.

If you’re a member of the Substack, you’re getting a fat discount because we love y’all.

As a reminder in case you forgot, it’s:

- 40% off for 1 test: `BEAT8POINT9`
- 60% off for 3 tests: `341`
- 60% off for 12 tests: `STOCKUP`

This is one of the products that got born out of the chat—from conversations with people who are genuinely obsessed with this stuff—and we want it to feel like part of the community, and we want more products like this to pop up!

We’re also opening a Slack channel where you can give feedback directly. If you sign up for AI Cred, you’ll be invited to join us in the work Slack and tell us what you think. Found a weird response? Want a feature on the leaderboard? Hit a bug? Just ping us. We want to model the reality that AI tooling evolves, and so does AI Cred. It’ll keep pace with how learning needs to grow.

---


## The Bigger Picture: Learning Speed Is The New Moat

Here’s what I keep coming back to when I think about why this matters.

The fear of AI is always worse than the reality of using it. That’s been true since the beginning, and it’s still true now. But the gap between people who are learning effectively and people who are stuck is widening fast. Not because of intelligence—because of intentionality.

AI fluency is a learnable skill. It’s not magic. It’s not reserved for engineers or people who went to the right schools. It’s a distinct competency that anyone can develop, if they know where they stand and what to work on.

That’s what measurement gives you. Clarity. Direction. A roadmap that isn’t one-size-fits-all but actually reflects your specific gaps and how to close them.

The era of “I use ChatGPT” is over. Everyone uses ChatGPT now. The question is whether you’re in the top 5% of practitioners or the bottom 50%, and the only way to know is to actually measure it.

So here’s my challenge: go find out.

Take the assessment at [aicred.ai](https://www.aicred.ai/). Beat Jonathan’s 8.9 if you can. See where you’re actually strong and where you’re fooling yourself. Then do the work to get better, and retake it in a month to see if anything changed.

That’s how fluency develops. Not by collecting certificates, but by measuring, learning, and measuring again.

---

*P.S. – Someone needs to beat Jonathan on that leaderboard (he’s at 8.9). I’m putting out the call. Go take the assessment and knock him off the top spot. I want to see what the leaderboard looks like tomorrow!*

[![](https://substackcdn.com/image/fetch/$s_!eFEw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597203b1-2755-483b-abd7-fab25c944b67_1024x1024.jpeg)](https://substackcdn.com/image/fetch/$s_!eFEw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F597203b1-2755-483b-abd7-fab25c944b67_1024x1024.jpeg)
