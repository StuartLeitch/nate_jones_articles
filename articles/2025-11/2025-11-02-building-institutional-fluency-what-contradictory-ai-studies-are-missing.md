---
title: "Building Institutional Fluency: What Contradictory AI Studies are Missing"
author: "Nate Jones"
published: 2025-11-02
url: https://natesnewsletter.substack.com/p/executive-briefing-wharton-says-75
audience: everyone
scraped_at: 2026-01-05 19:13:02
---

This week Wharton gave everyone whiplash.

We were just done processing MIT’s aggressive headline—95% of AI projects fail—when Wharton dropped their study saying 75% of enterprises succeed.

They can’t both be right.

If you’re exhausted by the headline whiplash, I get it. I am too.

Every week there’s a new study: *AI is revolutionary*. *AI is overhyped*. *Companies are winning.* *Companies are failing.*

The narrative keeps shifting and leaders keep asking me: **what is actually happening?**

I can tell you that, because I work with them.

Here’s what’s actually happening: while everyone argues about whether AI works, AI-native organizations put their heads down and focus on building institutional fluency in AI.

What does that mean? At it’s simplest: these companies are learning to solve problems across the business with intelligence. They’re not getting spun by contradictory headlines. They’re putting their heads down and building steady value.

I wrote about how [leaders can develop individual AI fluency](https://natesnewsletter.substack.com/p/executive-briefing-ai-usage-is-not) a couple of weeks ago—the difference between people who use AI and people with fluency that compounds.

This is the companion piece. This is about institutional AI fluency. How organizations build the capability to solve problems with intelligence at scale.

What is institutional fluency anyway?

It’s pretty simple: an organization can have lots of AI fluent people, and still not be seeing as much upside as it should simply because the institution itself remains stuck in old ways of working. All the structures of the business can reinforce the dead past while execs preach AI adoption at All Hands meetings—in fact, I see that a lot.

Looking across dozens of deployments I’ve been involved with, I’ve seen three consistent keys to developing institutional AI fluency:

1. Context fluency (a brand new skill even for expert teams)
2. The ownership-skills inversion (this breaks the way corporations have organized for centuries—and most haven’t figured it out yet)
3. Democratized taste (the Steve Jobs model is dead, but few know how to build what comes next)

I’ve seen over and over again that companies that focus on these three capabilities actually do win. They don’t need to care about whatever study comes out next week.

If you focus on these three, you’ll be learning the institutional skill of solving problems with intelligence, and you’ll be able to fully leverage the skills of your most AI fluent leaders and teams.

And yes, **of course there’s a conversational prompt** to help you assess your actual teams and figure out which of these you need to work on first. I want you to have everything you need to actually get focused so you don’t have to worry about the headlines.

We’re in the midst of the biggest change in corporate structures and ways of working in centuries. I want you to have the tools to build an AI-native organization, and to sustain your competitive advantage for the next decade.

Let’s get into it.

> ***This is an Executive Circle briefing**, a Sunday newsletter exclusively for Founding Tier Members. You can learn more via this [60 second video](https://youtu.be/KC3GkEnHR-8) explaining what’s in each tier, and you can change your plan [here](https://support.substack.com/hc/en-us/articles/360044105731-How-do-I-change-my-subscription-plan-on-Substack). Enjoy, and back to regular programming Monday!*


## [Grab the Institutional AI Fluency Prompt here](https://www.notion.so/product-templates/Institutional-AI-Fluency-Prompt-29f5a2ccb52680c1b6efd622185d6f4b?source=copy_link)

This conversational prompt helps you think through the three pillars of institutional fluency using your actual teams and projects. It walks you through context fluency, the ownership-skills inversion, and democratized taste—not as abstract concepts, but mapped to specific people and work you’re seeing right now. By the end, you’ll know which capability is actually blocking you and have a concrete first step to take with one team. It’s a structured conversation that turns “I think we have a problem but I’m not sure what it is” into “here’s exactly what’s broken and here’s what I’m doing about it next week.”

---


# Building Institutional Fluency: What Contradictory AI Studies are Missing

MIT and Wharton published research on the same Fortune 500 companies this week. MIT found 95% of AI projects fail. Wharton reported 75% positive ROI. Both cannot be correct.

The difference isn’t methodology. It’s measurement. And beneath that measurement gap sits something most organizations haven’t built yet: institutional fluency in solving problems with AI.


## Why MIT and Wharton Disagree

I’ll be blunt: I think MIT engineered a headline. They put an extremely tight screen on project success—tighter than almost any other internal software measure I’ve seen. In MIT’s view, every project is a failure by default unless you can measure dollar-and-cents impact on the bottom line within six months.

That’s not how we measure software ROI anywhere else. We track productivity gains, time saved, throughput improvements. Internal metrics that map to business value. Wharton took that approach. They asked executives how they measure ROI. Overwhelmingly, executives use those conventional metrics. By that standard, 75% report success.

MIT demands bottom-line dollars in six months. Wharton accepts productivity and throughput. Both approaches measure real things, and both have a point. MIT is correct that AI costs 10X more per employee than previous software. We need different ROI bars. Wharton is correct that this is how executives actually measure success on the ground.

Much more importantly, neither study explains how to build successfully. They measure different aspects of the same organizational capability gap.


## What Both Studies Miss

I’ve spent the last year working with Fortune 500 companies on AI transformation. Individual contributors tell me they don’t know how to solve problems with AI effectively. They’re guessing. Managers tell me their teams can’t decompose problems for AI. They don’t know how to help people get better. Executives tell me it’s their biggest capability gap. They see competitors moving but don’t have clear language for what fluency looks like.

The capability gap is universal. Every role is being redefined. For the first time in history, we’re solving problems using intelligence as a tool. Not intelligence as metaphor—actual reasoning, context, and judgment we can invoke programmatically. We’ve never done this before.

Organizations have built institutions around human intelligence as the scarce resource. Teams, hierarchies, decision rights, quality control—all designed to allocate scarce human intelligence carefully. That constraint broke. Intelligence is abundant now. The scarce resource is the capability to use it.

Most organizations don’t have that capability yet. The 95% failing in MIT’s measure haven’t figured out how to scale problem-solving with AI comprehensively enough to move the bottom line. The 75% succeeding in Wharton’s measure have pockets of capability but not systematic institutional fluency.

The organizations winning in both studies have built something specific. Not just tools. Not just pilots. Institutional fluency in solving problems with intelligence at scale.


## What Institutional Fluency Looks Like

This executive briefing is born from hard observation of many different AI deployments at scale. I’ve seen over and over that these are the three pillars that define whether your organization can systematically solve problems with intelligence.

Organizations demonstrating these capabilities show up in Wharton’s 75%. Organizations demonstrating them comprehensively—across enough teams that bottlenecks disappear—show up in MIT’s 5%.

What are these keys?


### 1. Context Fluency: Learning to Speak to AI

Teams with context fluency solve domain-specific problems with AI. Not generic problems. Their specific problems in their specific vertical with their specific constraints and workflows.

You can’t decompose problems AI can’t understand. If AI doesn’t know how your business works, how you drive value, what your processes are, what matters and what doesn’t—it can’t help you solve real problems. It solves toy problems. Generic problems any company might face.

Organizations with context fluency treat context as everybody’s job. Teams deliberately maintain domain knowledge in a form AI can work with. New team members don’t just learn the domain—they learn how to articulate it to AI systems. When teams solve novel problems, they encode solutions to build the team’s context library.

This operates at the team level. Teams are the atomic unit of institutional capability. Individuals come and go. Teams stay. Teams own verticals and domains. The value of the team is the context they inhabit and the context they can articulate to AI systems.

Teams lacking context fluency use generic prompts instead of domain-specific ones. AI outputs require heavy editing because they miss nuance. Knowledge lives in individual heads rather than shareable formats. New team members take months to get productive with AI tools. People say “AI doesn’t understand our business.”

Teams with context fluency produce the accountable acceleration Wharton measures. They decompose domain problems in ways AI can help solve. Executives measure productivity gains and count it as success.


### 2. The Ownership-Skills Inversion—What Most Teams Miss

This pillar is where most organizations break. Individual contributors are guessing at how to work effectively with AI. Managers don’t know how to help teams get better. Executives see the capability gap but their mental models for building capability don’t apply anymore.

Traditionally, we manipulated information to solve problems. Writing PRDs. Doing analysis. Pushing information fluency forward through individual skill. The skilled writer elevated the whole team. The brilliant analyst unlocked the problem space. Individual capabilities mattered enormously. Ownership resided at the team level with managers responsible for solving problems and driving around obstacles. Information moved slowly enough that this worked.

When you solve problems using intelligence as a tool, this inverts. Individual contributors must own quality and judgment in ways they never had to before. AI moves fast and has enormous capability. If individual contributors don’t have a strong sense of ownership—if they can’t assess whether AI work is good enough and insist when it isn’t—they add no value. You can’t set that quality bar at the manager level anymore. The manager can’t review every interaction. The manager can’t be the bottleneck.

At the same time, the skills needed to work with AI effectively—understanding how to decompose problems, how to structure context, how to prompt for reasoning—those can now live at the team level. Teams share prompts with each other. They share Claude Skills and custom GPTs. Someone who doesn’t fully understand transformer architectures can run a well-designed prompt and gain skill over time by socializing with the team. Technical capability is shareable in ways individual brilliance never was.

This inverts how corporations have been organized for hundreds of years. Managers have been accountable for domains and departments. They represent the business working with individual contributors who execute. AI-native organizations can’t be configured this way. The power you have with AI resides heavily with the individual. You have to devolve ownership down to individual contributors.

This causes real struggle. Individual contributors tell me they don’t know how to take that ownership effectively. They make judgment calls about AI outputs but aren’t confident those judgments are right. Managers tell me their entire role feels uncertain. If they’re not gatekeeping quality and making final calls, what are they doing? How do they add value? Executives see teams that should move faster but can’t figure out what’s blocking them.

The manager’s role inverts from controller to enabler. Managers in organizations with this pillar become skill sharers. They create and maintain the team’s prompt library and context documents. They build psychological safety so individual contributors can own judgment calls and push back on AI when it doesn’t meet the bar. They run retrospectives on what great looks like to socialize taste. They ensure domain knowledge gets encoded rather than held in people’s heads. They help individual contributors see how ownership connects to business outcomes.

The best managers I’ve seen do this through rituals. Weekly prompt reviews. Taste calibration sessions. Context documentation sprints. They teach rather than control. But most managers haven’t made this transition yet. Most organizations haven’t made this transition yet. That shows up in both studies as failure to scale AI problem-solving capability beyond small pockets of success.

Teams lacking this inversion have managers who bottleneck AI output approvals. Individual contributors wait for permission to iterate. Shared prompts exist but quality varies wildly because no one owns it. People defer to AI judgment instead of pushing back. Skill-sharing happens but no one feels accountable for results. Teams have tools but not capability.


### 3. Democratized Taste: Solving the Right Problems

This pillar separates organizations that are busy from organizations that are effective. Perfect context fluency and successful ownership-skills inversion can still produce comprehensive failure if you’re solving the wrong problems. With AI giving teams unprecedented power to execute, the cost of solving wrong problems has never been higher.

Traditionally, taste—judgment about what constitutes excellence, what problems are worth solving, what represents an extraordinary offer—could be delegated to a small group. The Steve Jobs of your company. The founders. A priesthood of maybe 10 or 15 people licensed to think differently and set the quality bar. Everyone else executed against that vision.

Organizations demonstrating institutional fluency have democratized taste to the team level. Teams are empowered to move autonomously and make their own judgments about quality and priority without sacrificing excellence. This isn’t optional with AI. The technology gives teams too much execution power for centralized taste to work. If teams wait for the priesthood to bless every decision, you lose all speed advantage AI provides. If teams move autonomously without taste, they build wrong things very efficiently.

Taste means several things. Taste in problems—which problems are worth solving versus which we should ignore or automate away. Taste in solution quality—what does great actually look like in our domain with our customers and our constraints. Taste in learning—when should we skill up versus when should we ship, when should we explore versus when should we optimize.

This is where the quality trade-off between MIT and Wharton shows up most clearly. MIT set an extraordinary quality bar: bottom-line impact in months. Wharton measured conventional software quality: productivity and throughput. Organizations with democratized taste can actually meet MIT’s bar because teams throughout the organization have internalized founder-level obsession with impact. They don’t just ask “did we ship something?” They ask “did we move the number that matters?”

Taste is not universal the way understanding LLMs is universal. Taste is specific to your vertical, your situation, your customers. It’s like context. It requires knowing your local domain well and having excellent judgment about what matters. That’s why it must be democratized. You can’t have a central priesthood with taste across 500 different domains. Taste has to live where problems live.

Teams lacking democratized taste ship fast but leadership keeps rejecting the work. Lots of activity happens but it’s unclear if it’s the right activity. People can’t distinguish important problems from urgent ones. The quality bar feels arbitrary or changes with whoever reviews. Teams need executive intervention to prioritize. They have problem-solving capability applied to wrong problems.

When IBM was at its height, they had a group of 10 or 15 tastemakers who could break all organizational norms to introduce creative thinking. That model doesn’t work anymore. We need institutions that socialize taste so deeply that teams can be trusted with the power AI gives them.


## How This Scales

Institutional fluency is fractal. The pattern looks the same whether you’re a 100-person company or a 10,000-person company. The dynamic doesn’t fundamentally change. The training surface area explodes.

A 100-person company needs this at maybe five to ten teams. A 10,000-person company needs it at 500-plus teams. Each team needs its own context because context is domain-specific. Each team needs to make the ownership-skills inversion because you can’t do it centrally. Each team needs democratized taste because taste is local judgment that can’t be standardized from the top.

The scale challenge isn’t “how do we make this work bigger?” It’s “how do we make 500 leaders capable of enabling this pattern in their local context?” That’s a leadership development problem, not an org design problem. You need to train leaders at every level to enable institutional fluency in their teams. You can’t cascade it hierarchically because the capability has to live locally.

This requires getting less hierarchical in how decisions get made. You can’t have seven approval layers when ownership lives with individual contributors and teams are empowered to move autonomously with taste. The hierarchy becomes about capability building, not decision rights. Leadership becomes about ensuring teams have what they need, not controlling what they do.


## What This Means

We’re living through the first moment in history when we can solve problems with intelligence as a tool. Every role is being redefined. Every organization is being tested on whether it can build the capability to participate.

The studies coming out—MIT’s 95% failure rate, Wharton’s 75% success rate—they measure different things but they both measure readiness for this shift. Organizations failing in MIT’s measure haven’t figured out how to solve problems with intelligence comprehensively enough to move the bottom line. Organizations succeeding in Wharton’s measure have pockets of teams demonstrating the capability. The 5% succeeding in MIT’s measure have built institutional fluency across enough teams that bottlenecks disappeared.

These three pillars—context fluency, ownership-skills inversion, democratized taste—this is what institutional AI fluency looks like in practice. This is what organizations demonstrating the capability are doing on the ground.

Right now, almost everyone is struggling with this. Individual contributors are guessing at how to solve problems with AI effectively. Managers’ mental models for building team capability are breaking. Executives can see the gap but don’t have clear language for what fluency looks like. That struggle is normal. This is unprecedented.

But the path forward is becoming clear. Build context fluency so teams can articulate their domain to AI and decompose their specific problems effectively. Make the ownership-skills inversion so individual contributors own quality while teams share capability. Democratize taste so teams can move autonomously without sacrificing excellence or solving wrong problems.

Organizations that build these capabilities won’t get spun by contradictory headlines. They won’t be confused about whether AI works or doesn’t work. They’ll be building steady value with intelligence as a tool, and that capability will define competitive advantage for the next decade.


## Where to Start

Most organizations reading this have zero or one pillar partially built. Maybe you have pockets of context fluency—teams that figured out domain-specific prompting through trial and error. Almost certainly you haven’t made the ownership-skills inversion. Your managers are still gatekeeping quality instead of enabling it. And democratized taste? That probably lives with founders or executives, not with teams throughout the organization.

This is normal. The capability is unprecedented. You’re not behind. But you do need to start building systematically.

Start with one team. Not an enterprise transformation. Pick a team that’s struggling in ways you can now name. Look at the diagnostic language after each pillar—the “Teams lacking X show Y” descriptions. Which pattern describes what you’re seeing? If managers are bottlenecking AI output approvals and individual contributors wait for permission to iterate, that’s the ownership-skills inversion. If teams ship fast but leadership keeps rejecting the work, that’s democratized taste. If AI outputs require heavy editing because they miss nuance, that’s context fluency.

One pillar is usually your primary constraint. Trying to build all three simultaneously will fail. You need to identify which capability gap is actually breaking your teams, then design specific interventions for that gap. This requires mapping the framework to actual teams, actual people, actual projects you’re seeing right now. Not general training programs. Specific capability building for specific blockers.

The path forward is systematic assessment of where capability exists and where it doesn’t, then deliberate experimentation with one receptive team. Get that team demonstrating the capability. Then socialize what you learned and expand from there.

I’m seeing this in real time working with Fortune 500 companies. The pattern is clear enough now that I can tell you this with confidence: if you want to participate in the intelligence revolution, these are the capabilities you need to build. Not just tools. Not just pilots. Institutional fluency in solving problems with intelligence at scale.

Good luck building, and now you can ignore the next overhyped study.

[![](https://substackcdn.com/image/fetch/$s_!lIAu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485257de-a2cb-4f00-b279-86904b9102a5_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!lIAu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F485257de-a2cb-4f00-b279-86904b9102a5_1024x1024.png)
