---
title: "Grab the 9 prompts I use to turn docs into practice reps + a 90-day pilot you can steal"
author: "Nate Jones"
published: 2025-12-11
url: https://natesnewsletter.substack.com/p/grab-the-9-prompts-i-use-to-turn
audience: everyone
scraped_at: 2026-01-05 19:10:13
---

*Your ten years of experience might be one year repeated ten times.*

*We tend to repeat the same habits, and that limits our career growth.*

*But what if AI could change that?*

*In 2019, David Perell wrote an essay called “Learn Like an Athlete.” His observation was sharp: athletes train, musicians train, performers train—but knowledge workers don’t. We just show up and do the work, hoping we get better through exposure.*

*Perell’s original thesis focused on “learning plans”—the idea that knowledge workers should structure their skill development the way athletes structure off-season training. Set clear objectives, break them into daily increments, measure progress. He used LeBron James as his central example: a player who treats his off-season with the same rigor he brings to games, systematically building specific capabilities rather than just playing more basketball.*

*That essay circulated widely, and Tyler Cowen picked it up on Marginal Revolution with a characteristically pointed addition. He didn’t disagree with Perell, but he sharpened the challenge into a question I haven’t been able to shake: What is it you do to train that is comparable to a pianist practicing scales?*

*The shift from “learning plan” to “scales” matters more than it might seem. A learning plan is still vague—you could satisfy it by reading books, attending conferences, collecting experiences. Scales are brutally specific. A pianist doesn’t just “work on their technique.” They practice a defined sequence, get immediate feedback from the sound, and repeat until the pattern becomes automatic. The skill is isolated, measured, drilled.*

*Most of us knowledge workers don’t have an answer to Cowen’s question. We read. We react. We accumulate years of experience. But experience **isn’t** the same as practice. Practice means isolating a skill, getting feedback, and repeating until patterns become automatic. Knowledge workers have never had a way to do that.*

*We didn’t until 2025. That’s changed now. AI makes useful feedback cheap enough to actually practice. For the first time, you can define what “good” looks like for fuzzy work like a decision doc or a spec, get consistent critique on every attempt, and see patterns in your weaknesses over time. You can get incredibly personalized feedback on everything you write, if you simply setup the system.*

*That’s what this post is for.*

*This piece makes the case for deliberate practice in knowledge work—where the athlete analogy holds, where it breaks, and how to build something like a practice gym for your team. It’s written for the skeptical operator who’s tired of theory and wants something concrete to try.*

***Here’s what’s inside:***

- ***The fundamental question.** Should knowledge workers even try to train like athletes, or does the analogy collapse on contact with reality?*
- ***What the research actually says.** Ericsson’s findings weren’t just about physical skills—plus the counterarguments you should know about.*
- ***Where the analogy breaks hard.** The honest limits of drilling your way to better thinking.*
- ***Five skills that keep showing up.** Judgment, orchestration, coordination, taste, and updating—and one popular “skill” I’d argue against.*
- ***What a practice gym looks like.** A concrete approach you can pilot with one team over 90 days.*
- ***The emotional reality.** What this looks like from inside the person doing the practicing.*
- ***The dark version.** How this goes wrong, so you can avoid building something people hate.*

***Plus nine prompts you can use immediately:***

- ***Decision Document Evaluator.** Score decision docs against a rubric with quoted evidence and suggested improvements.*
- ***Spec/Orchestration Evaluator.** Evaluate specs, PRDs, and process docs that turn strategy into executable work.*
- ***Executive Update Evaluator.** Critique communications to busy decision-makers against a focused rubric.*
- ***Rubric Builder.** Facilitate the really messy human conversation about “what good looks like” for any artifact your team produces.*
- ***Example Annotator.** Mark up real documents with explicit scoring rationale to calibrate your rubrics.*
- ***Messy-to-Drill Converter.** Turn real situations (Slack threads, vague requests) into focused 15-minute practice exercises.*
- ***Stronger Version Generator.** See what a better version looks like and understand specifically what you missed.*
- ***Weakness Pattern Analyzer.** Identify systematic gaps across multiple evaluated documents.*
- ***Interview Exercise Generator.** Create artifact-based hiring exercises aligned with how your team actually works.*

*Let’s go figure out how to actually level up our skills.*


## **[GRAB THE PROMPTS](https://www.notion.so/product-templates/The-Knowledge-Worker-Practice-Gym-Prompt-Library-2c65a2ccb5268028b24ec0dcbdaaa4a1?source=copy_link)**

I’ve built nine prompts that turn the ideas in this piece into something you can actually use. Three are evaluators that score your decision docs, specs, and executive updates against concrete rubrics. Two help you build and calibrate those rubrics with your team. Three create practice drills from the messy situations you’re already facing. And one generates artifact-based interview exercises so your hiring process tests the same skills you’re developing internally.

These aren’t generic templates. They’re designed to catch the failure modes I see constantly: fuzzy problem framing, missing risk sections, specs no one can execute, updates that bury the ask. If you want to move from reading about deliberate practice to actually doing it, start here.


## **How to Practice Like Lebron at Work**


### **Your Job Title Is Not a Skill**

We need to start talking about skills differently, and almost no one is ready for that conversation.

Our assumptions about skills have been so tightly coupled to job titles that the coupling is now embedded in our software. If you’ve ever used a hiring tool, a compensation benchmarking system, or a promotion rubric, you’ve seen it: the underlying model assumes skills must be layered into job descriptions. It can’t imagine skills existing independently of roles.

But that’s exactly where we’re headed. In a world where AI handles an increasing share of routine cognitive work, what matters is the specific capabilities you can deploy to produce outcomes—not which job title you happen to hold. Compensation should follow from what you can actually do, from the quality of the judgments you make and the clarity of the documents you produce.

In that world, knowing how to deliberately improve at specific skills becomes urgent rather than theoretical. And that brings us back to the question Cowen posed: what’s your equivalent of practicing scales?


### **The Provocation and Why It Sticks**

When Perell wrote his original essay, he focused on “learning plans”—the idea that knowledge workers should structure their skill development the way athletes structure their training. Set clear objectives, break them into daily increments, measure progress. His central example was LeBron James, who treats his off-season with the same rigor he brings to playoff games.

There’s nothing wrong with this framing, but it’s also not quite specific enough. You could satisfy a “learning plan” by reading more books, attending a few conferences, taking on stretch projects. That’s growth, maybe, but it’s not practice in the way a musician practices.

Cowen’s sharpening move was to make the challenge more uncomfortable. A pianist doesn’t just “learn” music—they practice scales, daily, with immediate feedback, until the patterns are so deep they don’t require conscious thought. What’s the knowledge work equivalent? What do you do repeatedly, with structured feedback, to build specific capabilities?

Most of us don’t have an answer. We attend meetings, write documents, make decisions—but we’re not isolating sub-skills and drilling them. We’re playing live games, every time, with our careers on the line. That’s experience, but it’s not practice.


### **What the Research Actually Says**

Before getting into how AI changes this, there’s a more fundamental question worth taking seriously. Is the mind trainable the way the body is, or does “practice like an athlete” break down when you try to apply it to writing strategy documents?

The skeptic’s objection deserves attention. Knowledge work is creative, contextual, judgment-based. Every situation differs. You can drill free throws because the hoop is always ten feet high, but how do you drill “making good decisions under ambiguity”? Isn’t that just wisdom? And doesn’t wisdom emerge from lived experience rather than structured exercises?

If the analogy is fundamentally wrong, everything that follows is theater. So it’s worth examining what the research actually suggests.

The deliberate practice research that Anders Ericsson made famous wasn’t primarily about physical skills. Yes, his landmark 1993 study examined violinists at a West Berlin music academy, finding that the “best” students had accumulated an average of over 10,000 hours of deliberate practice by age 20—about 2,500 hours more than the “good” violinists and 5,000 hours more than a group training to become teachers. But Ericsson also studied chess players, medical diagnosticians, and even typists. The finding wasn’t that muscles respond to reps. It was broader: skills with a defined performance standard seem to respond to structured practice with feedback, regardless of whether those skills are physical or cognitive.

The key variables Ericsson identified were: a clear target performance, immediate and accurate feedback on attempts, opportunities to repeat and adjust, and progressive difficulty as skill increases.

Chess players get all four. They study positions, attempt solutions, receive immediate feedback from stronger players or engines, and work progressively harder problems. The skill being trained is entirely cognitive—pattern recognition, calculation, positional judgment—but it improves through structured practice the same way a golf swing does. In a 2008 paper in Academic Emergency Medicine, Ericsson noted that standardized tests of chess positions, developed over decades, are highly correlated with tournament performance. The skill is measurable, and it responds to deliberate work.

Medical diagnosis follows a similar pattern, at least in training contexts. Case vignettes, immediate feedback on accuracy, increasing complexity. Ericsson proposed that medical educators could recreate patient encounters in time-constrained contexts and provide immediate feedback by comparing trainees’ decisions with those of expert physicians—essentially creating “drill” environments for clinical judgment.

But here’s where I need to be honest about the limits of this research.

Ericsson’s claims have been contested. In 2014, Brooke Macnamara, David Hambrick, and Frederick Oswald published a meta-analysis in *Psychological Science* covering 88 studies across multiple domains. They found that deliberate practice explained 26% of variance in performance for games, 21% for music, 18% for sports, 4% for education, and less than 1% for professions. Their conclusion: deliberate practice is important, but not as important as Ericsson argued.

The academic debate that followed has been ongoing and sometimes acrimonious. Ericsson and his colleagues have responded that many of the studies included in the meta-analyses didn’t actually measure deliberate practice as he defined it—they measured “practice” more broadly, including activities that lacked the key characteristics of individualized feedback and progressive challenge. Hambrick and collaborators have fired back that Ericsson’s definition shifts depending on which studies are being critiqued.

I’m not going to resolve this debate. But what seems clear to me is the following: deliberate practice explains *some* meaningful portion of skill development, especially for moving from novice to competent (rather than from good to elite). Cognitive ability, personality, and contextual factors also matter. And the domains where practice has the strongest effect tend to be those with clearer feedback signals—games, music, sports—rather than fuzzy professional contexts.

So I’m not importing the whole “10,000 hours” meme. I’m stealing the parts that seem to survive contact with messy work: clear target, feedback, reps. And I’m betting that AI changes the feedback part enough to make this more tractable than it’s ever been.


### **Why Knowledge Work Has Been Stuck**

Consider what a product manager or strategist or executive actually does. They write documents, make decisions, coordinate people, run meetings. The “skill” distributes across dozens of fuzzy sub-tasks, and outcomes are delayed by weeks or months.

Three structural problems have blocked deliberate practice.

The first is fuzzy outcomes. In basketball, the ball goes in or it doesn’t. In product work, “good” mixes across multiple dimensions simultaneously—speed, quality, political navigation, relationship management, risk tolerance. There’s no single bit that flips from zero to one, no clean signal separating success from failure.

The second is delayed and noisy feedback. You might make a significant decision in Q1 and learn whether it paid off sometime in Q3, if you’re lucky. Meanwhile the market shifted, a competitor launched something unexpected, a key hire left. You almost never get the clean counterfactual.

The third is low repetition. A serious musician plays scales hundreds of times per week. How many truly consequential decision docs does a PM write in a quarter? When each artifact is entangled with real money and real people, there are no low-stakes sandboxes where you can experiment and fail cheaply.

The result is that most knowledge workers spend the vast majority of their reps in live performance. We practice in front of the crowd. That might be better than nothing, but it’s a remarkably inefficient way to build skill.


### **What Aviation Training Suggests**

If you want a concrete example of how cognitive skills can be drilled despite real-world complexity, look at how pilots train.

Flight simulators don’t just exist because engines are expensive. They exist because certain failure modes—engine out on takeoff, instrument failures, severe crosswinds—are too dangerous to practice in actual aircraft but too important to encounter for the first time in a real emergency.

In simulator training, the structure mirrors Ericsson’s framework almost perfectly. There’s a defined scenario (engine failure at V1, the decision speed during takeoff). There’s immediate feedback from the aircraft’s behavior and the instructor. The scenario can be repeated with variations until the response becomes automatic. And the difficulty can be progressively increased by adding complications—crosswinds, degraded instruments, crew coordination challenges.

The United States Helicopter Safety Team has explicitly recommended using simulators for scenario-based training to avoid and recover from specific failure modes: loss of rotor RPM, loss of tail rotor effectiveness, spatial disorientation, unintended flight into instrument conditions, and low-altitude engine failure. These are cognitive and procedural skills—pattern recognition, checklist execution, decision-making under stress—and they respond to structured drill.

What makes this relevant to knowledge work is the underlying structure, not the specific domain. Pilots aren’t drilling muscle memory in the way a pianist drills scales. They’re drilling judgment: how to recognize a situation, what options to consider, which checklists to execute, when to deviate from standard procedure. The simulator provides a sandbox where failure is cheap and feedback is immediate.

We don’t have simulators for decision documents. But we might be able to build something structurally similar.


### **Where the Analogy Breaks Hard**

Being honest about limits matters, because it shapes what you should and shouldn’t expect from this approach.

**Context-dependence is high.** A free throw is a free throw. But “framing a decision well” looks different in a startup than at a Fortune 500, different in a crisis than during steady-state operations, different with a technical audience than with executives. How much of the skill transfers across contexts remains genuinely uncertain.

**You can’t drill your way to wisdom.** Judgment has a component that comes only from lived experience—from watching your own decisions play out over years, from learning which mental models apply in which situations, from developing calibrated confidence about what you know and don’t know. Deliberate practice might build components of good judgment—clear framing, explicit reasoning, systematic risk identification—but those components don’t substitute for accumulated context.

**Relationship-first skills resist this approach.** Managing a board, navigating complex negotiations, building trust with a skeptical stakeholder—these depend heavily on reading people, adapting in real time, and drawing on relational capital built over months or years. You can practice communication patterns, but the skill lives primarily in the relationship, not the artifact.

**Data sensitivity makes some domains impractical.** In defense, certain healthcare contexts, or anywhere with strict information controls, running documents through AI review may simply not be possible. The feedback loop that makes this approach work requires that you can actually show your work to the system providing feedback.

**Early-stage creativity doesn’t want rubrics.** When you’re doing zero-to-one work—exploring a genuinely new problem space, generating options no one has considered—patterns are intentionally unstable. You *want* variance. Optimizing for a rubric too early can kill ideas before they’ve had a chance to develop.

So what can you actually train? Pattern recognition, structured reasoning, clear communication, the habit of surfacing trade-offs explicitly. These are components of good judgment, even if they don’t constitute the whole thing. And they show up in artifacts.


### **Five Skills That Keep Showing Up (Plus One I’d Argue Against)**

I’ve been trying to get specific about which skills matter—not generic “AI skills” but the particular capabilities I see teams break on repeatedly.

**Judgment** is how you frame decisions, define the real options, and choose under uncertainty. It shows up in decision documents, prioritization calls, and experiment designs. The failure mode is fuzzy problem framing—documents that describe a situation without ever specifying what’s actually being decided.

**Orchestration** is how you turn fuzzy goals into concrete workflows that humans and AI can execute together. It lives in specs, prompts, process documentation, and handoff protocols. The failure mode is specs that are simultaneously too vague to execute and too detailed to understand.

**Coordination** is how you move groups of humans through ambiguity without creating additional chaos. As agents become more capable, this may expand to coordinating agents and humans together. The skill surfaces in executive emails, one-pagers, and meeting notes. The failure mode is updates that bury the ask three paragraphs in or require two follow-up meetings to interpret.

**Taste** is whether you have a meaningful quality bar for your product, your writing, your design. Can you articulate what “good” looks like in your domain? This shows up in product choices, design reviews, and narrative quality. The failure mode is accepting whatever comes out without a clear sense of what would make it better.

**Updating** is how you revise your beliefs as evidence shifts, without getting whipped around by noise. It becomes visible in how your positions evolve over time, not in how you defend them once. The failure mode is either refusing to change your mind in the face of evidence or changing your mind constantly based on whoever talked to you last.

None of these skills live in a LinkedIn tagline. They live in artifacts—the documents you leave behind.

**One skill I’d argue against: “prompting” as a top-level capability.** I see people listing “prompt engineering” or “AI literacy” as the key skill to develop. I think this gets the abstraction wrong. Prompting is a thin layer on top of orchestration and specification literacy. If you can write a clear spec for a human, you can write a clear prompt for a model. If you can’t, no amount of prompt tricks will save you. Train the deeper skills, and “prompting” emerges as a side effect.


### **What AI Changes**

AI is not a magic brain. It’s a tool that can read text, follow instructions, and apply a rubric with reasonable consistency. But that consistency matters, because it gives us something like a wall to practice against—a source of feedback that was previously too expensive to scale.

The process starts without AI. You pick one artifact that matters for your team—a decision doc, a spec, a positioning brief—and sit down with the people whose judgment you trust. You ask them: when you say a document of this type is good, what specifically do you mean?

Then you push. Ask gently, ask clearly, ask persistently, until you’ve extracted a concrete list. For a decision doc, that might include: Is the decision itself stated in a single sentence? Are there at least two genuine options? Are the stakes explicit? Is there a clear recommendation? Are risks surfaced?

You turn that list into a rubric—one to five on each dimension. You gather three to five real examples and mark them up with annotations. This one excels at clarity. That one surfaces risks well but has weak option generation.

Notice that nothing so far involves AI. Defining what good looks like is work only humans can do.

Once you have the rubric and annotated examples, you bring them to a model. You instruct it to score new documents against your rubric, quote the specific passages it’s reacting to, explain briefly why it gave each score, and suggest edits that would improve weak dimensions.

What changes? Instead of a manager skimming a document and approving it because they have fifteen minutes before their next meeting, you get structured critique applied consistently to every document of that type. Over a quarter, patterns emerge. Where are you systematically weak? What failure modes keep recurring?

Is this perfect? No. The scores are noisy. But directional signal is enormously better than no signal. For the first time, knowledge workers have something resembling film review.


### **What This Looks Like for a Team**

Few team leads do this yet, but the mechanics are straightforward.

You decide that next quarter you’re going to focus on one specific artifact—perhaps specs, perhaps decision docs. You and your team define the rubric together. You pull example documents that represent quality work. You annotate them as a group, building shared understanding of what each score means.

Then you configure a model so that whenever someone marks a document as ready for review, AI runs a rubric pass before any human looks at it. Engineers already do this with automated code review on pull requests; now you’re applying the same idea to documents.

Two new habits follow. First, let the AI critique land before human review, so the human reviewer can focus on judgment calls rather than catching obvious gaps. Second, once or twice a week, each team member spends ten minutes practicing on whatever dimension the AI keeps flagging as their growth area.

By the end of the quarter, you should be able to answer: Did scores improve? Are documents getting approved with fewer revision cycles? Are decisions happening faster?

If those answers are yes, you’ve learned something. If they’re no, you’ve also learned something.


### **Using the Same Scales for Hiring**

Most companies evaluate skills through remarkably indirect methods. We ask candidates to tell us about a time they influenced a stakeholder, listen to the story, and try to infer whether they can do the work.

If you’ve defined a pattern for a particular artifact, you have a more grounded option: give candidates the same exercise your team practices with.

A short take-home where the candidate writes or repairs a decision document. A live session where you work through it together, introducing a changed constraint partway through. A critique exercise where you show them a deliberately mediocre AI-generated document and ask them to identify what’s wrong.

You can use the same rubric you developed internally. The goal isn’t to let AI make hiring decisions. It’s to create a shared lens for evaluating what good looks like on actual work.

A useful side effect: hiring and development now point at the same skills. The capabilities you test for in candidates are the same capabilities you help them practice once they join.


### **AI Use Is the Point, Not the Problem**

Roughly two-thirds of AI usage in organizations is shadow usage—people not reporting it. This approach doesn’t require anyone to hide their AI. You can be open about using it and still improve, because the goal is the outcome.

If a candidate uses AI to produce their take-home, you’ll learn quickly whether they understand what they produced. When you change a constraint live and they can’t adapt, when they struggle to explain the trade-offs without a screen in front of them, you see where their actual skill ends.

The evaluation happens through live conversation. The goal isn’t catching people cheating. The goal is observing whether someone has a pattern of thought that remains visible even when they can’t tab-complete their way through.


### **Emotions Can Be Our Worst Enemy Here**

Most of what I’ve written so far is structural and organizational. But let me say something about how this feels from inside the person doing the practicing.

There’s a specific fear that runs through a lot of knowledge workers right now: “AI is making me feel obsolete. I’m not sure how to get better anymore. The things I used to be proud of—my writing, my analysis, my judgment—now feel like they might not matter.”

This is real, and I don’t think it helps to pretend it isn’t.

But here’s the reframe I’d offer: you now have the opportunity to design your own personal training loop instead of drifting through “more experience.”

For most of the history of knowledge work, getting better meant hoping that you happened to encounter the right challenges at the right time, with managers who gave useful feedback. That was always arbitrary. Some people got lucky with mentors. Most didn’t.

Now you can construct your own reps. You can build your own rubrics. You can get feedback on demand, consistently, without waiting for someone to have time for you.

That’s not a threat. That’s agency.

A practical version of this: if you’re a PM, commit to two decision-doc drills per week, one spec drill, one coordination/communication drill. Use artifacts from your own week—the messy Slack thread, the vague request from your manager, the meeting that went sideways. Turn them into practice exercises. Run them through your rubric. See what patterns emerge.

This won’t make you feel less obsolete overnight. But it will give you something to actually *do* about it, which is more than most people have.


### **This Could Be Misapplied (Like Any Tech, Honestly)**

I’d be doing you a disservice if I didn’t describe how this goes wrong.

Here’s the bad version: AI scoring gets used directly in performance calibration. Leaderboards appear showing who has the highest “doc scores.” Rubrics get written by consultants who’ve never done the actual work, never updated, and deployed as weapons in political battles. Managers use low scores to justify decisions they’ve already made. People learn to game the rubrics rather than to actually improve.

This is what happens when you take a coaching tool and turn it into a surveillance tool.

The difference between the good version and the bad version isn’t complicated:

**Bad version**: Surveillance + metric worship + centralized control.

**Good version**: Cheap feedback + local ownership + optional practice.

In the good version, individuals own their scores. Managers see team-level patterns rather than individual rankings. Rubrics are defined by the people who do the work and updated when they stop being useful. Practice is something you choose to do because it helps, not something that gets checked in your annual review.

The moment you tie scores to performance calibration, people will game them instead of learning from them. Keep the tool in the coaching lane.


### **What This Looks Like Three Years Out**

I’ll keep this short because I don’t want to oversell.

If organizations actually adopt this approach, a few things change. Performance reviews stop being entirely vibes-based and start anchoring on artifact patterns. Interviews revolve around actual work simulations rather than behavioral storytelling. People talk about their “quarterly skills block” the way athletes talk about their off-season focus—which specific capability they’re drilling, what their weaknesses are, how they’re trying to address them.

The teams that do this well will produce cleaner documents, make faster decisions, and onboard new people more quickly. Basically, they’ll have an improvement layer that’s recursive that lets them get better. Something intentional to layer over the osmotic process of career development today.


### **Don’t Over-Use Rubrics**

Rubric scores will be noisy. They’re useful as coaching signal, not as inputs to compensation decisions. The moment you tie scores to performance reviews, people will game them instead of learning from them.

There’s a real risk of creating a surveillance feeling. Feedback should be private by default; managers should see team-level patterns rather than every individual score.

And program fatigue is a genuine concern. If you try to roll this out for every artifact type simultaneously, it will collapse. Starting small—one artifact, one rubric, one quarter—is a requirement, not just a suggestion.


### **So What Are Your Scales?**

Perell’s original essay was about ambition and intentionality—treating your career development with the seriousness athletes bring to their training. Cowen’s sharpening question made it more concrete: what do you actually practice, repeatedly, with structured feedback?

For most of knowledge work history, we haven’t had a good answer. Feedback was too expensive, outcomes too fuzzy, repetitions too rare. AI changes the economics enough to make practice feasible. Whether it actually makes people better remains to be proven, but the structural barriers that blocked us seem addressable now.

I don’t know yet whether this will matter as much as I think. The research on deliberate practice is more contested than the pop-science version suggests. The transfer of drilled skills to novel contexts is genuinely uncertain. But my strong hunch is that if we practice this way, we’ll get better.

And if anything in this AI wave has a shot at making us better, not just faster, I think it looks like this: defined patterns, cheap feedback, actual reps. Treating knowledge work with the same rigor we’d bring to any other skill we wanted to develop.

Give it a shot this quarter: One artifact. One rubric. One team. Ninety days. See if the documents improve, if decisions happen faster, if people actually get better.

Tell me what you get!

[![](https://substackcdn.com/image/fetch/$s_!pAXe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba70c327-d856-4bf3-bb85-58164cf46cd6_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!pAXe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba70c327-d856-4bf3-bb85-58164cf46cd6_1024x1024.png)
