---
title: "Grab the Prompts"
author: "Nate Jones"
published: 2025-12-26
url: https://natesnewsletter.substack.com/p/grab-my-ai-gap-finder-kit-for-operators
subtitle: "Watch now | Faster than a speeding...nano banana lol. But seriously, things are about to move faster than we've ever seen--here's how to not only keep up, but get ahead."
audience: everyone
scraped_at: 2026-01-05 19:09:22
---

I’ve been watching this space for years. I have never seen it move this fast.

One month ago, Google shipped Nano Banana Pro with professional-grade text rendering inside images. Today, I’m tracking startups using tools built on tools built on that release—three generations of businesses stacked in thirty days. Not research projects. Funded companies. Real products.

Here’s the mechanism most people miss: you don’t need to smooth the whole AI capability surface to unlock massive value. You need to fill in the *right* gap. Text inside images was that gap for business collateral. Every previous model could generate gorgeous visuals, but the moment you needed legible typography—a slide title, a marketing tagline—the output was unusable.

Now it isn’t. Fix one gap, unlock entire categories.

And this particular gap matters more than most—because we’re visual creatures first, text creatures second. Seeing is faster than reading. When AI moves from chat windows to images you can actually use, it taps into something more fundamental than text interfaces ever could.

If we’re three layers deep in a month on image generation, how many more gaps are about to close in 2026—and how many opportunities are you going to miss watching from the sidelines?

Here’s what’s inside:

- The jagged frontier framework—and why the 30% gains crowd will never catch the 300% gains crowd
- What Nano Banana Pro actually fixed, and the three failure modes that still break
- Five areas where we’re one unlock away from the next cascade
- How to map jaggedness in your own work before someone else does
- 4 prompts designed to help you successfully navigate the visual frontier

Understanding the pattern—where the gaps are, which ones are closing, and how to position before the cascade—is going to separate the builders from the spectators this year.

So let’s get building.


# **[Grab the Prompts](https://www.notion.so/product-templates/The-2026-Jagged-Unlock-Kit-2d35a2ccb52680b4b49ae1ae4014ce5c?source=copy_link)**

I’ve packaged everything from this article into four prompts you can run *today*.

The Visual Frontier Map tells you exactly which elements to generate vs. create manually for your specific deliverable type—decks, infographics, UI mocks, whatever you ship regularly.

The QA Harness catches the three failure modes that still break: typography drift (the “MARKETTING STRAETGY” problem), semantic labeling errors (professional-looking images with confidently wrong labels), and translation instability (ask for Spanish, get a different layout).

The Workflow Router turns your map and test results into a production process with gates that prevent broken visuals from shipping.

And the Cascade Scanner helps you spot the next Nano Banana Pro—tiny gaps about to close with massive downstream unlocks.

These aren’t generic “AI productivity” templates. They’re built around the specific jaggedness of visual artifact generation in late 2025: what’s finally working, what still breaks, and how to route around the valleys while exploiting the peaks. If you ship any kind of visual collateral, this kit will change how you work.


# The Speed of Unlocking: What Nano Banana Pro Tells Us About 2026

We need to talk about velocity.

Not the velocity of model improvements—you’ve heard that story. I want to talk about the velocity of *entrepreneurship* that a single breakthrough enables. Because what’s happening right now with Nano Banana Pro isn’t just a better model. It’s a case study in how fast value compounds once you fill in one of the jagged gaps in AI intelligence. And understanding that pattern—really understanding it—is going to separate the people who build successfully in 2026 from the people who watch from the sidelines wondering what happened.

Here’s the mental model that makes everything else make sense.


## The Jagged Frontier

LLMs are jagged intelligence surfaces. Breathtaking on some tasks, brittle on adjacent ones. But here’s the thing: humans are jagged too. We just don’t think about it that way because we’ve spent our entire careers jamming our particular shapes into predefined holes.

My brain can solve differential equations in real time to catch a baseball without conscious thought. The physics of trajectory, wind resistance, the timing of my hand closing—all computed instantly, automatically, as long as I’ve practiced. My brain can identify faces with shocking accuracy because we evolved to be face-to-face creatures. But ask me to do that same differential equation math on paper, and I need training, time, and probably a calculator. The intelligence surface has peaks and valleys. We all have different shapes to our jaggedness.

Historically, careers have been about matching human-shaped capability bundles to job-shaped holes. Hiring is basically: find a person who can do 80% of this bundle reliably. The jagged frontier flips that entirely. AI isn’t a person-shaped bundle. It’s a weird, spiky surface with a completely different topology than human intelligence. And the real shift—the one most people haven’t internalized—is that work stops being “roles filled by humans” and becomes “pipelines assembled from jagged components,” where the human’s job is often to route around brittleness and lock the AI into the parts it’s structurally advantaged at.

This is the practical meaning of the “jagged technological frontier” concept from the Dell’Acqua, Mollick, and Lakhani field experiment with BCG consultants. They found something crucial: AI boosts output when you’re working inside the frontier, and can actively *degrade* performance when you push it outside the frontier. Not just “doesn’t help”—actually makes things worse. The jaggedness isn’t a metaphor. It’s an operational reality with measurable consequences.

And this connects directly to what I call the fluency versus activity distinction—the difference between 30% gains and 300% gains.

The 30% crowd treats AI like a power tool. They use it more. They generate more output. But they don’t change the shape of their work. They’re still doing the same tasks, just with an electric assist. The 300% crowd treats AI like a routing layer. They build a task map and learn where the frontier is jagged, then they redesign workflows so AI handles what it’s structurally advantaged at while humans cover the failure modes. In the BCG-style framing, this is the difference between “Centaurs” (clean handoffs between human and AI phases) and “Cyborgs” (deeply integrated, task-by-task collaboration). Fluency is knowing how to navigate the jaggedness intentionally, not just generating more AI output and hoping for the best.

When I reviewed Nano Banana Pro, I found it could accurately summarize an entire earnings report. Complex financial documents, dense with numbers and context, distilled into clear summaries. But if you asked it to make a children’s alphabet—26 symbols, in order, no deviations—it couldn’t do it reliably. It would “helpfully” improvise. That’s not a bug in the traditional sense; it’s how generative priors behave. But it means the intelligence surface has a peak at “synthesize complex business documents” and a valley at “follow exact sequential constraints.” Knowing where those peaks and valleys are is the whole game.


## What Nano Banana Pro Actually Solved

So what did Nano Banana Pro specifically unlock? Why is this the moment things accelerated?

The answer is deceptively simple: professional-grade text rendering inside images.

Not “kinda readable” text. Actually usable text for posters, diagrams, mockups, and slides. Google positioned it explicitly around advanced text rendering, higher-fidelity output, and more precise editing controls—camera angle, lighting, aspect ratio—plus higher resolution output tiers. That sounds incremental if you’re not paying attention. It’s not.

Text inside images was a hard blocker for real business collateral. Every previous image model could generate gorgeous visuals, but the moment you needed legible typography—a slide title, an infographic label, a marketing tagline—the whole thing fell apart. You’d get almost-readable text, or text that drifted into gibberish, or text that looked fine until you zoomed in and realized it said “MARKETTING STRAETGY.” The last mile of business-usable imagery was gated by typography, and typography was broken.

Once you can reliably generate legible text inside images, you unlock entire categories. Slide visuals. Infographics. UI mockups. Localized marketing assets. Design-in-the-loop iterations that previously fell apart at the typography step. The capability threshold crossed wasn’t “better images”—it was “images that can go directly into a deck without a designer cleaning them up.”

Google also shipped it across high-distribution surfaces simultaneously: Gemini app, Workspace, Ads, AI Studio, and Vertex. This is why it showed up as a sudden ecosystem event rather than a niche model release. The pipes were already laid. Product surfaces were ready to ingest the model the moment it cleared the threshold.

The other piece is control. Nano Banana Pro isn’t just higher quality—it’s more controllable. Better instruction-following for visual composition. Local edits and multi-step refinement. Character consistency across multiple generations, which is one of the key requirements for branded systems and iterative creative work. Google explicitly splits the line: Nano Banana is “Fast” for casual creativity; Nano Banana Pro is the “Thinking” model for advanced outputs and precision. That control layer matters because business use cases require predictability, not just capability.


## What Still Breaks

But here’s the thing about jagged surfaces: knowing the peaks isn’t enough. You need to know the valleys too, because misuse actively degrades outcomes.

Three failure modes matter in practice with Nano Banana Pro.

First, exactness under tight constraints. If the requirement is “these 26 symbols, in this order, no deviations,” image models still drift. The children’s alphabet is the canonical case, but it shows up anywhere you need precise sequential or structural compliance. The model will “helpfully” improvise because that’s how generative priors work. It’s not trying to follow your exact specification—it’s trying to generate something that looks like it should belong to the category you described. Those are different objectives, and they diverge exactly when precision matters most.

Second, “do the same thing, but translated” without changing layout. There’s evidence that in-place translation requests can cause unwanted layout and style changes. You ask for the same infographic in Spanish, and you get a different composition. Higher-resolution settings can improve outcomes, but it’s not solved. For global marketing teams, this is a significant constraint.

Third, semantic grounding and labeling errors. Even in strong demos, models can misidentify or mislabel elements in a scene. For explanatory infographics and “here’s what this diagram shows” workflows, that’s deadly. The image looks professional; the labels are confidently wrong.

There’s also a softer, product-relevant failure: a realism bias. Some users report the model “snaps” surreal prompts back toward photorealism. Great for ads. Terrible for weird, stylized concepts. If your creative direction is deliberately non-photorealistic, you may find yourself fighting the model’s priors.

None of this means Nano Banana Pro isn’t a breakthrough. It means the breakthrough is *specific*. It filled in one part of the jagged surface—the part that was blocking business collateral—while leaving other valleys intact. Knowing exactly what’s solved and what isn’t is how you build successfully on top of it.


## The Cascade Mechanics

Now we get to the part that matters for 2026: how fast the cascade moves once a threshold is crossed.

Consider Capsules, a startup I stumbled across that didn’t ask me to cover them and doesn’t know I’m writing this. Capsules is building a new storytelling medium that combines text with generative images reflecting the mood of what you’re reading, all in a scrollable format. It feels like unraveling a parchment that shows you moving pictures as you scan. Light, easy, thoughtful. Works for travel logs, reflective pitches, personal narratives—the kind of stories that previously lived in long Twitter threads or niche Reddit communities.

What makes Capsules interesting isn’t whether it succeeds. What makes it interesting is that it *couldn’t exist* a month ago. The image generation wasn’t good enough. The text rendering wasn’t there. The consistency wasn’t at the level where you could ship something without hand-correcting every frame.

Now it is. And Capsules built on top of that. And already I’m seeing startups using Capsules to pitch *their* ideas—a company building a “canary trigger” for journalists, telling their origin story through generated imagery that traces from Cold War East Germany to blockchain-enabled dead man’s switches. Three generations deep in roughly a month. Nano Banana Pro begat Capsules begat startups using Capsules to fundraise.

But Capsules is just one instance of a broader pattern. The cascade is happening across multiple lineages simultaneously.

Google Workspace baked Nano Banana Pro into Slides, Vids, and NotebookLM—including features like “Help me visualize” and slide beautification that turn image generation into a workflow primitive. It’s not a separate tool anymore; it’s a capability embedded in the surface where work happens.

Vertex AI and enterprise availability make it a selectable backend for anyone building vertical creative tooling inside companies. If you’re building an internal tool for your marketing team, you can now plug in Nano Banana Pro as infrastructure.

Adobe integrated it into Firefly and Photoshop, making it a first-class option inside existing professional creative pipelines. The model doesn’t require users to change their tools; it meets them where they already work.

Third-party presentation and creative products are explicitly marketing Nano Banana Pro as the differentiator—studio-quality slides with readable in-image text as the headline feature.

What you’re seeing is three lineages lighting up at once: consumer creation, knowledge-work collateral, and professional creative tooling. They all share the same missing piece—reliable text-in-image plus controllability—and they all became possible the moment that piece was filled in.


## Why This Is Different

How should you think about the “three lineages in a month” observation? Is this unprecedented, or have we seen similar velocity before?

I’d frame it as a distribution shock plus capability threshold combination.

The capability threshold is what I’ve been describing: once text rendering and control get “good enough,” you can generate assets that slot into real workflows. Slides, ads, UI comps, marketing collateral. The output crosses from “impressive demo” to “actually usable without cleanup.”

The distribution shock is that it didn’t ship into a single lab demo. It shipped into Gemini, Workspace, Ads, AI Studio, and Vertex almost simultaneously. Adoption isn’t gated by new user behavior. The surfaces where people already work are ready to ingest the capability the moment it clears the threshold.

We’ve seen “model to ecosystem” waves before. But this is faster because the pipes are already laid. The infrastructure for distributing model capabilities into products is mature now. When a model crosses a usability threshold, the ecosystem responds in weeks, not years.

That’s the pattern to internalize. The next breakthrough—wherever it comes—will propagate even faster because the distribution infrastructure keeps improving. The gap between “capability exists” and “capability is embedded in products people use” is shrinking toward zero.


## Where We’re Close

So where else are we close to a breakthrough? Where are the building blocks mostly in place, waiting for something to put them together?

I’m watching five areas. For each one, the question isn’t “is progress happening?” The question is: what exists now, what’s missing, and what would a real breakthrough look like?

**Robotic grasping and embodied manipulation.** The building blocks are further along than most people realize. Vision-Language-Action models and robotics foundation models are accelerating, with major players explicitly targeting generality and dexterity. DeepMind’s Gemini Robotics work, Nvidia’s Isaac GR00T—these aren’t research curiosities anymore. What’s missing is robustness under messy real-world variation, safety guarantees, cost-effective training data, and reliable generalization across different hands, tools, and objects. “Lab impressive” is still different from “warehouse reliable.” The real world remains undefeated. But we’re closer than the popular narrative suggests, and when this crosses, the cascade into logistics, manufacturing, and household robotics will be enormous.

**Always-on agents.** Agent frameworks are real. You can build agents today that chain tools, maintain context, and execute multi-step workflows. The hard part is long-horizon reliability plus tool ecosystems that don’t crumble under edge cases. The story here is less “agents exist” and more “agents don’t stay sane for weeks.” An agent that works brilliantly for an hour and then confidently corrupts your data on hour six isn’t a product. What’s missing is the combination of reliability, recovery, and trust that lets you leave an agent running without supervision. When someone cracks this—when you can deploy an agent and trust it to run for days without degrading—the implications for operations, monitoring, and background automation are massive.

**Continual learning.** Active research is happening on training and update schemes that let models learn without catastrophic forgetting. Non-parametric memory approaches are part of the picture. What’s missing is safe, predictable updating without silent corruption, plus evaluation methods that catch regressions early. The current paradigm—train once, deploy, retrain from scratch—is expensive and limiting. When continual learning works, models stop being static artifacts and start being systems that improve with use. That changes the economics of deployment entirely.

**Memory.** The industry is explicitly pushing structured retrieval and memory mechanisms for agents. Microsoft’s public positioning acknowledges that memory is hard and expensive, and points toward structured retrieval augmentation approaches. What’s missing has four parts: deciding what to remember, keeping memories truthful, preventing manipulation, and aligning memory with user intent over time. Memory isn’t just a technical problem—it’s a product design problem about what an AI should know about you and how that knowledge should evolve. But the building blocks are mostly there. This is largely a systems engineering and product design challenge now, not a fundamental research blocker.

**Proactivity.** Systems can suggest. They struggle to intervene correctly without becoming annoying or risky. What’s missing is high-precision preference models, low false-positive rates, and clear permissioning so proactive actions don’t feel like overreach. The difference between “helpful assistant that anticipates my needs” and “annoying system that keeps interrupting me” is entirely about calibration. When someone nails the calibration—when proactive AI feels like a good executive assistant rather than an overeager intern—the value creation will be significant.

Of these, memory plus bounded proactivity in software is closest. Not because it’s fully solved, but because it’s mostly a product and systems engineering problem at this point. Retrieval, summarization, permissions, evaluation—these are hard, but they’re not waiting for fundamental breakthroughs. Robotics is improving fast, but the real world is still undefeated.


## What You Should Actually Do

Three concrete actions.

First, map jaggedness in your own work. Make a simple frontier map of tasks where AI is reliably better than you, reliably worse, and high variance. Then redesign workflows so AI lives in the “reliably better” zone and humans cover the edges. This sounds obvious, but almost no one does it systematically. The BCG research implies this is where the real advantage comes from—not from using AI more, but from knowing what’s inside versus outside the frontier. Misuse can degrade outcomes. The map protects you.

Second, treat multimodal artifact generation as a new primitive. Nano Banana Pro isn’t “another image model.” It’s a step toward generating finished business artifacts—slides, ads, diagrams—because text-in-image and controls are crossing usability thresholds. If your organization ships collateral, you need capability here. Build it or buy it, but don’t ignore it. The teams that treat this as infrastructure rather than novelty will have structural advantages in production speed.

Third, invest in evaluation, not vibes. Jaggedness means you don’t get to assume smooth improvement. You need harnesses, regression tests, and “known failure mode” checklists per workflow. Every time you deploy AI in a new context, you need to know where it breaks for that context. The children’s alphabet failure doesn’t show up in an earnings summary workflow. The semantic labeling failure doesn’t show up in a creative brainstorm workflow. Each deployment surface has its own failure modes, and you need to find them before they find you.


## The Pattern Recognition Skill

Here’s what I want you to take away.

The entrepreneurs who win in this environment aren’t necessarily the ones with the best ideas. They’re the ones who recognize when a jagged gap closes and move before everyone else catches up. They’re the ones watching the building blocks, understanding what’s missing, and ready to build the moment the foundation is solid enough to hold weight.

With Nano Banana Pro, you could see it coming if you were paying attention. Realistic images were there. Text rendering was improving. Control mechanisms were advancing. All we needed was something that put it together with sufficient consistency. The pieces were on the board; we were waiting for someone to assemble them.

Look for that pattern elsewhere. Look at the spaces where we have almost everything but not quite. Robotic grasping: the VLA models exist, the foundation models exist, we’re waiting for real-world robustness. Always-on agents: the frameworks exist, the tools exist, we’re waiting for long-horizon reliability. Memory: the retrieval mechanisms exist, the summarization exists, we’re waiting for the product design to mature.

When those gaps close—and some of them will close in 2026—the cascade will be fast. If we’re three lineages deep in a month on image generation, how many businesses will we unlock when robotic manipulation crosses the threshold? When always-on agents become trustworthy? When memory systems actually work?

The growth numbers tell the story. Gemini, powered by Nano Banana Pro, is on track to reach a billion users faster than ChatGPT did. Faster than the fastest-growing app in history. And I think the image capabilities are the primary driver—because we are visual creatures first, text creatures second. Seeing is easier than reading. Always has been. When you make AI visual, you tap into something more fundamental than chat interfaces ever could.

That’s the lesson of Capsules. That’s the lesson of Nano Banana Pro. That’s the lesson of this absurdly fast moment we’re living through.

The gaps are closing. The cascades are accelerating. The question is whether you’re watching the building blocks closely enough to know when the next one crosses—and whether you’re ready to build when it does.

[![](https://substackcdn.com/image/fetch/$s_!aDDH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ed5c262-d4ba-4d33-b1a8-f49e5335484f_1024x1024.png)](https://substackcdn.com/image/fetch/$s_!aDDH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7ed5c262-d4ba-4d33-b1a8-f49e5335484f_1024x1024.png)
